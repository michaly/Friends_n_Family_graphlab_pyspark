{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the RDDs\n",
      "Finished joinning RDDs\n",
      "Finished normalizing\n",
      "END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015-07-02 04:48:36,477 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=2068485, maxMem=278302556\n",
      "2015-07-02 04:48:36,478 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_37 stored as values in memory (estimated size 241.2 KB, free 263.2 MB)\n",
      "2015-07-02 04:48:36,497 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=2315435, maxMem=278302556\n",
      "2015-07-02 04:48:36,497 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_37_piece0 stored as bytes in memory (estimated size 19.0 KB, free 263.2 MB)\n",
      "2015-07-02 04:48:36,498 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_37_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,498 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_37_piece0\n",
      "2015-07-02 04:48:36,499 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 37 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 04:48:36,508 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=2334900, maxMem=278302556\n",
      "2015-07-02 04:48:36,509 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_38 stored as values in memory (estimated size 241.2 KB, free 262.9 MB)\n",
      "2015-07-02 04:48:36,527 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=2581850, maxMem=278302556\n",
      "2015-07-02 04:48:36,527 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_38_piece0 stored as bytes in memory (estimated size 19.0 KB, free 262.9 MB)\n",
      "2015-07-02 04:48:36,528 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_38_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,528 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_38_piece0\n",
      "2015-07-02 04:48:36,528 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 38 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 04:48:36,536 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=2601315, maxMem=278302556\n",
      "2015-07-02 04:48:36,536 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_39 stored as values in memory (estimated size 241.2 KB, free 262.7 MB)\n",
      "2015-07-02 04:48:36,555 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=2848265, maxMem=278302556\n",
      "2015-07-02 04:48:36,556 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_39_piece0 stored as bytes in memory (estimated size 19.0 KB, free 262.7 MB)\n",
      "2015-07-02 04:48:36,556 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_39_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,556 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_39_piece0\n",
      "2015-07-02 04:48:36,557 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 39 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 04:48:36,564 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=2867730, maxMem=278302556\n",
      "2015-07-02 04:48:36,564 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_40 stored as values in memory (estimated size 241.2 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:36,571 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 36\n",
      "2015-07-02 04:48:36,572 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_36_piece0\n",
      "2015-07-02 04:48:36,572 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_36_piece0 of size 2958 dropped from memory (free 275190834)\n",
      "2015-07-02 04:48:36,572 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_36_piece0 on localhost:40918 in memory (size: 2.9 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,573 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_36_piece0\n",
      "2015-07-02 04:48:36,573 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_36\n",
      "2015-07-02 04:48:36,573 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_36 of size 4736 dropped from memory (free 275195570)\n",
      "2015-07-02 04:48:36,573 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 36\n",
      "2015-07-02 04:48:36,574 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 35\n",
      "2015-07-02 04:48:36,574 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_35\n",
      "2015-07-02 04:48:36,574 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_35 of size 8792 dropped from memory (free 275204362)\n",
      "2015-07-02 04:48:36,574 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_35_piece0\n",
      "2015-07-02 04:48:36,574 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_35_piece0 of size 5670 dropped from memory (free 275210032)\n",
      "2015-07-02 04:48:36,575 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_35_piece0 on localhost:40918 in memory (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,575 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_35_piece0\n",
      "2015-07-02 04:48:36,576 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 35\n",
      "2015-07-02 04:48:36,576 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 23\n",
      "2015-07-02 04:48:36,587 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=3092524, maxMem=278302556\n",
      "2015-07-02 04:48:36,588 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_40_piece0 stored as bytes in memory (estimated size 19.0 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:36,588 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_40_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,589 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_40_piece0\n",
      "2015-07-02 04:48:36,589 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 40 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 04:48:36,603 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 04:48:36,627 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 04:48:36,650 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 04:48:36,877 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-3-4ba038715317>:105\n",
      "2015-07-02 04:48:36,877 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 269 (reduceByKey at <ipython-input-3-4ba038715317>:105)\n",
      "2015-07-02 04:48:36,878 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 5 (collect at <ipython-input-3-4ba038715317>:105) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 04:48:36,878 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 70(collect at <ipython-input-3-4ba038715317>:105)\n",
      "2015-07-02 04:48:36,878 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 69)\n",
      "2015-07-02 04:48:36,879 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 69)\n",
      "2015-07-02 04:48:36,880 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 69 (PairwiseRDD[269] at reduceByKey at <ipython-input-3-4ba038715317>:105), which has no missing parents\n",
      "2015-07-02 04:48:36,882 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8792) called with curMem=3111989, maxMem=278302556\n",
      "2015-07-02 04:48:36,883 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_41 stored as values in memory (estimated size 8.6 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:36,883 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5669) called with curMem=3120781, maxMem=278302556\n",
      "2015-07-02 04:48:36,884 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_41_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:36,884 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_41_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:36,885 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_41_piece0\n",
      "2015-07-02 04:48:36,886 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 41 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:36,886 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 69 (PairwiseRDD[269] at reduceByKey at <ipython-input-3-4ba038715317>:105)\n",
      "2015-07-02 04:48:36,886 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 69.0 with 2 tasks\n",
      "2015-07-02 04:48:36,887 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 69.0 (TID 638, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:48:36,887 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 69.0 (TID 639, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:48:36,888 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 69.0 (TID 638)\n",
      "2015-07-02 04:48:36,889 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 69.0 (TID 639)\n",
      "2015-07-02 04:48:36,895 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:0+7742886\n",
      "2015-07-02 04:48:36,898 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:7742886+7742887\n",
      "2015-07-02 04:48:37,407 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 516, boot = 4, init = 6, finish = 506\n",
      "2015-07-02 04:48:37,408 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 69.0 (TID 638). 2121 bytes result sent to driver\n",
      "2015-07-02 04:48:37,409 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 69.0 (TID 638) in 522 ms on localhost (1/2)\n",
      "2015-07-02 04:48:37,667 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 775, boot = 6, init = 3, finish = 766\n",
      "2015-07-02 04:48:37,669 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 69.0 (TID 639). 2121 bytes result sent to driver\n",
      "2015-07-02 04:48:37,672 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 69.0 (TID 639) in 785 ms on localhost (2/2)\n",
      "2015-07-02 04:48:37,673 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:48:37,674 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 69 (reduceByKey at <ipython-input-3-4ba038715317>:105) finished in 0.786 s\n",
      "2015-07-02 04:48:37,674 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:48:37,674 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:48:37,674 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 70)\n",
      "2015-07-02 04:48:37,674 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:48:37,675 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 70: List()\n",
      "2015-07-02 04:48:37,675 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 70 (PythonRDD[272] at collect at <ipython-input-3-4ba038715317>:105), which is now runnable\n",
      "2015-07-02 04:48:37,676 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4736) called with curMem=3126450, maxMem=278302556\n",
      "2015-07-02 04:48:37,676 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_42 stored as values in memory (estimated size 4.6 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:37,677 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(2959) called with curMem=3131186, maxMem=278302556\n",
      "2015-07-02 04:48:37,677 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.9 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:37,678 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_42_piece0 in memory on localhost:40918 (size: 2.9 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:37,679 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_42_piece0\n",
      "2015-07-02 04:48:37,679 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 42 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:37,679 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 70 (PythonRDD[272] at collect at <ipython-input-3-4ba038715317>:105)\n",
      "2015-07-02 04:48:37,680 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 70.0 with 2 tasks\n",
      "2015-07-02 04:48:37,680 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 70.0 (TID 640, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:48:37,681 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 70.0 (TID 641, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:48:37,681 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 70.0 (TID 641)\n",
      "2015-07-02 04:48:37,681 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 70.0 (TID 640)\n",
      "2015-07-02 04:48:37,683 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 1 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:48:37,683 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:48:37,686 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 1 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:48:37,686 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:48:37,819 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 136, boot = -241, init = 376, finish = 1\n",
      "2015-07-02 04:48:37,820 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 70.0 (TID 641). 928 bytes result sent to driver\n",
      "2015-07-02 04:48:37,820 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 70.0 (TID 641) in 140 ms on localhost (1/2)\n",
      "2015-07-02 04:48:37,831 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 149, boot = 149, init = 0, finish = 0\n",
      "2015-07-02 04:48:37,832 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 70.0 (TID 640). 870 bytes result sent to driver\n",
      "2015-07-02 04:48:37,833 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 70.0 (TID 640) in 153 ms on localhost (2/2)\n",
      "2015-07-02 04:48:37,833 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:48:37,833 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 70 (collect at <ipython-input-3-4ba038715317>:105) finished in 0.153 s\n",
      "2015-07-02 04:48:37,834 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 5 finished: collect at <ipython-input-3-4ba038715317>:105, took 0.956701 s\n",
      "2015-07-02 04:48:38,226 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 04:48:38,274 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-3-4ba038715317>:139\n",
      "2015-07-02 04:48:38,275 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 219 (reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,276 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 224 (reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,276 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 282 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,277 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 229 (reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,277 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 289 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,277 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 239 (reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,278 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 296 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,278 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 234 (reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,278 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 303 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,279 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 244 (reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,279 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 310 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,279 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 249 (reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,279 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 254 (reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,280 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 317 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,280 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 259 (reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,280 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 264 (reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,280 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 324 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,281 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 275 (reduceByKey at <ipython-input-3-4ba038715317>:108)\n",
      "2015-07-02 04:48:38,281 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 331 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,281 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 338 (join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:38,281 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 343 (reduceByKey at <ipython-input-3-4ba038715317>:139)\n",
      "2015-07-02 04:48:38,282 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 6 (collect at <ipython-input-3-4ba038715317>:139) with 52 output partitions (allowLocal=false)\n",
      "2015-07-02 04:48:38,282 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 92(collect at <ipython-input-3-4ba038715317>:139)\n",
      "2015-07-02 04:48:38,282 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 91)\n",
      "2015-07-02 04:48:38,284 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 91)\n",
      "2015-07-02 04:48:38,311 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 71 (PairwiseRDD[219] at reduceByKey at <ipython-input-3-4ba038715317>:34), which has no missing parents\n",
      "2015-07-02 04:48:38,312 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8608) called with curMem=3134145, maxMem=278302556\n",
      "2015-07-02 04:48:38,313 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_43 stored as values in memory (estimated size 8.4 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,313 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5593) called with curMem=3142753, maxMem=278302556\n",
      "2015-07-02 04:48:38,314 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_43_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,314 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_43_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:38,314 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_43_piece0\n",
      "2015-07-02 04:48:38,316 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 43 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,316 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 71 (PairwiseRDD[219] at reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,316 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 71.0 with 2 tasks\n",
      "2015-07-02 04:48:38,318 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 71.0 (TID 642, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:48:38,318 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 71.0 (TID 643, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:48:38,318 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 71.0 (TID 642)\n",
      "2015-07-02 04:48:38,318 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 71.0 (TID 643)\n",
      "2015-07-02 04:48:38,319 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 72 (PairwiseRDD[224] at reduceByKey at <ipython-input-3-4ba038715317>:34), which has no missing parents\n",
      "2015-07-02 04:48:38,320 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8608) called with curMem=3148346, maxMem=278302556\n",
      "2015-07-02 04:48:38,321 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_44 stored as values in memory (estimated size 8.4 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,322 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5591) called with curMem=3156954, maxMem=278302556\n",
      "2015-07-02 04:48:38,322 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_44_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,322 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:0+4095974\n",
      "2015-07-02 04:48:38,323 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_44_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:38,325 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:4095974+4095975\n",
      "2015-07-02 04:48:38,325 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_44_piece0\n",
      "2015-07-02 04:48:38,327 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 44 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,328 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 72 (PairwiseRDD[224] at reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,328 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 72.0 with 2 tasks\n",
      "2015-07-02 04:48:38,329 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 72.0 (TID 644, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:48:38,330 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 72.0 (TID 645, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:48:38,332 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 72.0 (TID 644)\n",
      "2015-07-02 04:48:38,336 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 74 (PairwiseRDD[229] at reduceByKey at <ipython-input-3-4ba038715317>:34), which has no missing parents\n",
      "2015-07-02 04:48:38,338 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 72.0 (TID 645)\n",
      "2015-07-02 04:48:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8616) called with curMem=3162545, maxMem=278302556\n",
      "2015-07-02 04:48:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_45 stored as values in memory (estimated size 8.4 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,342 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5609) called with curMem=3171161, maxMem=278302556\n",
      "2015-07-02 04:48:38,342 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,343 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_45_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:38,343 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_45_piece0\n",
      "2015-07-02 04:48:38,344 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 45 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,346 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 18 missing tasks from Stage 74 (PairwiseRDD[229] at reduceByKey at <ipython-input-3-4ba038715317>:34)\n",
      "2015-07-02 04:48:38,346 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 74.0 with 18 tasks\n",
      "2015-07-02 04:48:38,349 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:0+7742886\n",
      "2015-07-02 04:48:38,350 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:7742886+7742887\n",
      "2015-07-02 04:48:38,350 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 74.0 (TID 646, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:38,351 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 74.0 (TID 647, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:38,351 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 76 (PairwiseRDD[239] at reduceByKey at <ipython-input-3-4ba038715317>:46), which has no missing parents\n",
      "2015-07-02 04:48:38,351 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 74.0 (TID 648, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:38,352 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8664) called with curMem=3176770, maxMem=278302556\n",
      "2015-07-02 04:48:38,352 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_46 stored as values in memory (estimated size 8.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,353 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 74.0 (TID 647)\n",
      "2015-07-02 04:48:38,353 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5617) called with curMem=3185434, maxMem=278302556\n",
      "2015-07-02 04:48:38,353 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 74.0 (TID 646)\n",
      "2015-07-02 04:48:38,353 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_46_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,354 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_46_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.1 MB)\n",
      "2015-07-02 04:48:38,355 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 74.0 (TID 648)\n",
      "2015-07-02 04:48:38,355 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_46_piece0\n",
      "2015-07-02 04:48:38,357 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 46 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,358 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 76 (PairwiseRDD[239] at reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,358 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 76.0 with 2 tasks\n",
      "2015-07-02 04:48:38,360 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 78 (PairwiseRDD[234] at reduceByKey at <ipython-input-3-4ba038715317>:46), which has no missing parents\n",
      "2015-07-02 04:48:38,364 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8664) called with curMem=3191051, maxMem=278302556\n",
      "2015-07-02 04:48:38,365 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_47 stored as values in memory (estimated size 8.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,366 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5616) called with curMem=3199715, maxMem=278302556\n",
      "2015-07-02 04:48:38,367 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.4 MB)\n",
      "2015-07-02 04:48:38,367 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_47_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,368 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_47_piece0\n",
      "2015-07-02 04:48:38,375 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 47 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,375 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 78 (PairwiseRDD[234] at reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,375 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 78.0 with 2 tasks\n",
      "2015-07-02 04:48:38,378 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 80 (PairwiseRDD[244] at reduceByKey at <ipython-input-3-4ba038715317>:46), which has no missing parents\n",
      "2015-07-02 04:48:38,385 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8672) called with curMem=3205331, maxMem=278302556\n",
      "2015-07-02 04:48:38,386 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_48 stored as values in memory (estimated size 8.5 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,387 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5628) called with curMem=3214003, maxMem=278302556\n",
      "2015-07-02 04:48:38,387 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.5 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,388 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_48_piece0 in memory on localhost:40918 (size: 5.5 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,389 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_48_piece0\n",
      "2015-07-02 04:48:38,390 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 48 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,391 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 18 missing tasks from Stage 80 (PairwiseRDD[244] at reduceByKey at <ipython-input-3-4ba038715317>:46)\n",
      "2015-07-02 04:48:38,391 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 80.0 with 18 tasks\n",
      "2015-07-02 04:48:38,398 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:67108864+33554432\n",
      "2015-07-02 04:48:38,401 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:0+33554432\n",
      "2015-07-02 04:48:38,402 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:33554432+33554432\n",
      "2015-07-02 04:48:38,404 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 82 (PairwiseRDD[249] at reduceByKey at <ipython-input-3-4ba038715317>:57), which has no missing parents\n",
      "2015-07-02 04:48:38,405 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8384) called with curMem=3219631, maxMem=278302556\n",
      "2015-07-02 04:48:38,406 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_49 stored as values in memory (estimated size 8.2 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,407 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5420) called with curMem=3228015, maxMem=278302556\n",
      "2015-07-02 04:48:38,409 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_49_piece0 stored as bytes in memory (estimated size 5.3 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,410 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_49_piece0 in memory on localhost:40918 (size: 5.3 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,411 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_49_piece0\n",
      "2015-07-02 04:48:38,412 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 49 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,413 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 82 (PairwiseRDD[249] at reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,413 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 82.0 with 2 tasks\n",
      "2015-07-02 04:48:38,419 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 85 (PairwiseRDD[259] at reduceByKey at <ipython-input-3-4ba038715317>:57), which has no missing parents\n",
      "2015-07-02 04:48:38,428 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8384) called with curMem=3233435, maxMem=278302556\n",
      "2015-07-02 04:48:38,428 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_50 stored as values in memory (estimated size 8.2 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,449 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 42\n",
      "2015-07-02 04:48:38,450 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_42\n",
      "2015-07-02 04:48:38,450 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_42 of size 4736 dropped from memory (free 275065473)\n",
      "2015-07-02 04:48:38,450 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_42_piece0\n",
      "2015-07-02 04:48:38,450 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_42_piece0 of size 2959 dropped from memory (free 275068432)\n",
      "2015-07-02 04:48:38,451 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_42_piece0 on localhost:40918 in memory (size: 2.9 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,451 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_42_piece0\n",
      "2015-07-02 04:48:38,451 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 42\n",
      "2015-07-02 04:48:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5421) called with curMem=3234124, maxMem=278302556\n",
      "2015-07-02 04:48:38,455 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_50_piece0 stored as bytes in memory (estimated size 5.3 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,456 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_50_piece0 in memory on localhost:40918 (size: 5.3 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,457 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_50_piece0\n",
      "2015-07-02 04:48:38,458 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 50 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,459 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 85 (PairwiseRDD[259] at reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:48:38,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 85.0 with 2 tasks\n",
      "2015-07-02 04:48:38,463 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 88 (PairwiseRDD[275] at reduceByKey at <ipython-input-3-4ba038715317>:108), which has no missing parents\n",
      "2015-07-02 04:48:38,465 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8512) called with curMem=3239545, maxMem=278302556\n",
      "2015-07-02 04:48:38,472 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_51 stored as values in memory (estimated size 8.3 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,473 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5506) called with curMem=3248057, maxMem=278302556\n",
      "2015-07-02 04:48:38,474 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_51_piece0 stored as bytes in memory (estimated size 5.4 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:38,475 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_51_piece0 in memory on localhost:40918 (size: 5.4 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:38,476 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_51_piece0\n",
      "2015-07-02 04:48:38,476 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 51 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:38,477 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 88 (PairwiseRDD[275] at reduceByKey at <ipython-input-3-4ba038715317>:108)\n",
      "2015-07-02 04:48:38,478 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 88.0 with 2 tasks\n",
      "2015-07-02 04:48:41,833 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 3512, boot = -495, init = 503, finish = 3504\n",
      "2015-07-02 04:48:41,836 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 71.0 (TID 642). 1958 bytes result sent to driver\n",
      "2015-07-02 04:48:41,837 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 74.0 (TID 649, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:41,838 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 71.0 (TID 642) in 3521 ms on localhost (1/2)\n",
      "2015-07-02 04:48:41,839 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 74.0 (TID 649)\n",
      "2015-07-02 04:48:41,852 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:100663296+33554432\n",
      "2015-07-02 04:48:42,461 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 4141, boot = -483, init = 493, finish = 4131\n",
      "2015-07-02 04:48:42,464 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 71.0 (TID 643). 1958 bytes result sent to driver\n",
      "2015-07-02 04:48:42,466 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 74.0 (TID 650, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:42,467 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 74.0 (TID 650)\n",
      "2015-07-02 04:48:42,467 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 71.0 (TID 643) in 4149 ms on localhost (2/2)\n",
      "2015-07-02 04:48:42,467 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:48:42,469 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 71 (reduceByKey at <ipython-input-3-4ba038715317>:34) finished in 4.152 s\n",
      "2015-07-02 04:48:42,469 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:48:42,469 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 78, Stage 88, Stage 85, Stage 82, Stage 74, Stage 72, Stage 76, Stage 80)\n",
      "2015-07-02 04:48:42,469 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 73, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:48:42,469 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:48:42,478 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:134217728+33554432\n",
      "2015-07-02 04:48:42,482 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:48:42,495 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:48:42,499 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:48:42,509 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:48:42,511 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:48:42,514 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List(Stage 74, Stage 73)\n",
      "2015-07-02 04:48:42,518 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:48:42,522 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:48:42,527 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 78, Stage 77)\n",
      "2015-07-02 04:48:42,532 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 73: List(Stage 72)\n",
      "2015-07-02 04:48:42,536 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:48:42,546 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75, Stage 76)\n",
      "2015-07-02 04:48:42,547 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:48:43,846 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 5505, boot = 4, init = 13, finish = 5488\n",
      "2015-07-02 04:48:43,847 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 72.0 (TID 644). 1958 bytes result sent to driver\n",
      "2015-07-02 04:48:43,849 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 74.0 (TID 651, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:43,849 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 74.0 (TID 651)\n",
      "2015-07-02 04:48:43,851 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 72.0 (TID 644) in 5521 ms on localhost (1/2)\n",
      "2015-07-02 04:48:43,860 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:167772160+33554432\n",
      "2015-07-02 04:48:44,322 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 5982, boot = 8, init = 7, finish = 5967\n",
      "2015-07-02 04:48:44,325 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 72.0 (TID 645). 1958 bytes result sent to driver\n",
      "2015-07-02 04:48:44,327 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 74.0 (TID 652, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:48:44,327 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 74.0 (TID 652)\n",
      "2015-07-02 04:48:44,328 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 72.0 (TID 645) in 5999 ms on localhost (2/2)\n",
      "2015-07-02 04:48:44,328 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:48:44,329 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 72 (reduceByKey at <ipython-input-3-4ba038715317>:34) finished in 6.001 s\n",
      "2015-07-02 04:48:44,330 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:48:44,330 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 78, Stage 88, Stage 85, Stage 82, Stage 74, Stage 76, Stage 80)\n",
      "2015-07-02 04:48:44,330 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 73, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:48:44,330 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:48:44,334 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:48:44,344 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:48:44,345 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:201326592+33554432\n",
      "2015-07-02 04:48:44,348 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:48:44,355 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:48:44,357 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:48:44,360 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List(Stage 74, Stage 73)\n",
      "2015-07-02 04:48:44,373 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:48:44,376 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:48:44,380 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 78, Stage 77)\n",
      "2015-07-02 04:48:44,383 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 73: List()\n",
      "2015-07-02 04:48:44,385 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:48:44,388 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75, Stage 76)\n",
      "2015-07-02 04:48:44,390 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:48:44,390 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 73 (PairwiseRDD[282] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:48:44,392 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(11720) called with curMem=3253563, maxMem=278302556\n",
      "2015-07-02 04:48:44,395 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_52 stored as values in memory (estimated size 11.4 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:44,396 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5755) called with curMem=3265283, maxMem=278302556\n",
      "2015-07-02 04:48:44,398 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_52_piece0 stored as bytes in memory (estimated size 5.6 KB, free 262.3 MB)\n",
      "2015-07-02 04:48:44,407 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_52_piece0 in memory on localhost:40918 (size: 5.6 KB, free: 265.0 MB)\n",
      "2015-07-02 04:48:44,408 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_52_piece0\n",
      "2015-07-02 04:48:44,408 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 52 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:48:44,409 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 4 missing tasks from Stage 73 (PairwiseRDD[282] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:48:44,410 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 73.0 with 4 tasks\n",
      "2015-07-02 04:49:28,859 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45005, boot = 8, init = 4, finish = 44993\n",
      "2015-07-02 04:49:28,862 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 74.0 (TID 651). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:28,864 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 73.0 (TID 653, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:49:28,864 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 73.0 (TID 653)\n",
      "2015-07-02 04:49:28,864 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 74.0 (TID 651) in 45016 ms on localhost (1/18)\n",
      "2015-07-02 04:49:28,881 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:49:28,882 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:49:28,895 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 21, boot = 5, init = 5, finish = 11\n",
      "2015-07-02 04:49:28,900 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 31, boot = 7, init = 23, finish = 1\n",
      "2015-07-02 04:49:28,903 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 73.0 (TID 653). 1065 bytes result sent to driver\n",
      "2015-07-02 04:49:28,904 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 73.0 (TID 654, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:49:28,904 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 73.0 (TID 653) in 41 ms on localhost (1/4)\n",
      "2015-07-02 04:49:28,906 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 73.0 (TID 654)\n",
      "2015-07-02 04:49:28,935 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:49:28,936 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:49:28,950 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 18, boot = -8, init = 21, finish = 5\n",
      "2015-07-02 04:49:28,975 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 64, boot = 62, init = 1, finish = 1\n",
      "2015-07-02 04:49:28,977 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 73.0 (TID 654). 1065 bytes result sent to driver\n",
      "2015-07-02 04:49:28,979 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 73.0 (TID 655, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:49:28,980 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 73.0 (TID 655)\n",
      "2015-07-02 04:49:28,981 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 73.0 (TID 654) in 78 ms on localhost (2/4)\n",
      "2015-07-02 04:49:28,984 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:49:28,984 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:49:28,996 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 11, boot = -3, init = 5, finish = 9\n",
      "2015-07-02 04:49:29,028 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -33, init = 75, finish = 0\n",
      "2015-07-02 04:49:29,030 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 73.0 (TID 655). 1065 bytes result sent to driver\n",
      "2015-07-02 04:49:29,031 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 73.0 (TID 656, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:49:29,032 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 73.0 (TID 656)\n",
      "2015-07-02 04:49:29,032 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 73.0 (TID 655) in 54 ms on localhost (3/4)\n",
      "2015-07-02 04:49:29,038 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:49:29,038 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:49:29,053 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 17, boot = 0, init = 4, finish = 13\n",
      "2015-07-02 04:49:29,082 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -29, init = 74, finish = 0\n",
      "2015-07-02 04:49:29,085 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 73.0 (TID 656). 1065 bytes result sent to driver\n",
      "2015-07-02 04:49:29,086 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 74.0 (TID 657, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:29,087 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 74.0 (TID 657)\n",
      "2015-07-02 04:49:29,087 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 73.0 (TID 656) in 57 ms on localhost (4/4)\n",
      "2015-07-02 04:49:29,087 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:49:29,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 73 (join at <ipython-input-3-4ba038715317>:131) finished in 44.677 s\n",
      "2015-07-02 04:49:29,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:49:29,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 78, Stage 88, Stage 85, Stage 82, Stage 74, Stage 76, Stage 80)\n",
      "2015-07-02 04:49:29,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:49:29,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:49:29,092 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:49:29,095 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:49:29,097 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:234881024+33554432\n",
      "2015-07-02 04:49:29,099 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:49:29,101 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:49:29,102 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:49:29,105 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List(Stage 74)\n",
      "2015-07-02 04:49:29,109 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:49:29,113 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:49:29,116 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 78, Stage 77)\n",
      "2015-07-02 04:49:29,119 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:49:29,122 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75, Stage 76)\n",
      "2015-07-02 04:49:29,125 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:49:32,930 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54548, boot = 6, init = 48, finish = 54494\n",
      "2015-07-02 04:49:32,935 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 74.0 (TID 647). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:32,937 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 74.0 (TID 658, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:32,938 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 74.0 (TID 647) in 54587 ms on localhost (2/18)\n",
      "2015-07-02 04:49:32,940 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 74.0 (TID 658)\n",
      "2015-07-02 04:49:32,954 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:268435456+33554432\n",
      "2015-07-02 04:49:33,806 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51961, boot = 21, init = 0, finish = 51940\n",
      "2015-07-02 04:49:33,812 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 74.0 (TID 649). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:33,819 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 74.0 (TID 659, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:33,820 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 74.0 (TID 649) in 51982 ms on localhost (3/18)\n",
      "2015-07-02 04:49:33,820 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 74.0 (TID 659)\n",
      "2015-07-02 04:49:33,824 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:301989888+33554432\n",
      "2015-07-02 04:49:34,128 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55760, boot = 7, init = 34, finish = 55719\n",
      "2015-07-02 04:49:34,134 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 74.0 (TID 648). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:34,135 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 74.0 (TID 660, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:34,137 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 74.0 (TID 648) in 55786 ms on localhost (4/18)\n",
      "2015-07-02 04:49:34,140 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 74.0 (TID 660)\n",
      "2015-07-02 04:49:34,155 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:335544320+33554432\n",
      "2015-07-02 04:49:34,583 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50249, boot = 9, init = 11, finish = 50229\n",
      "2015-07-02 04:49:34,590 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 74.0 (TID 652). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:34,591 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 74.0 (TID 661, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:34,592 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 74.0 (TID 661)\n",
      "2015-07-02 04:49:34,592 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 74.0 (TID 652) in 50266 ms on localhost (5/18)\n",
      "2015-07-02 04:49:34,596 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:369098752+33554432\n",
      "2015-07-02 04:49:35,763 INFO  [Executor task launch worker-14] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57379, boot = 8, init = 33, finish = 57338\n",
      "2015-07-02 04:49:35,768 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 74.0 (TID 646). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:35,778 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 74.0 (TID 662, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:35,779 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 74.0 (TID 662)\n",
      "2015-07-02 04:49:35,779 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 74.0 (TID 646) in 57432 ms on localhost (6/18)\n",
      "2015-07-02 04:49:35,790 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:402653184+33554432\n",
      "2015-07-02 04:49:36,551 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54077, boot = 10, init = 9, finish = 54058\n",
      "2015-07-02 04:49:36,556 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 74.0 (TID 650). 2137 bytes result sent to driver\n",
      "2015-07-02 04:49:36,558 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 74.0 (TID 663, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:49:36,558 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 74.0 (TID 650) in 54093 ms on localhost (7/18)\n",
      "2015-07-02 04:49:36,564 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 74.0 (TID 663)\n",
      "2015-07-02 04:49:36,572 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:436207616+33554432\n",
      "2015-07-02 04:50:18,916 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45090, boot = -838, init = 844, finish = 45084\n",
      "2015-07-02 04:50:18,921 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 74.0 (TID 659). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:18,922 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 74.0 (TID 664, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:18,924 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 74.0 (TID 659) in 45106 ms on localhost (8/18)\n",
      "2015-07-02 04:50:18,926 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 74.0 (TID 664)\n",
      "2015-07-02 04:50:18,934 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:469762048+33554432\n",
      "2015-07-02 04:50:21,621 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52529, boot = -30, init = 42, finish = 52517\n",
      "2015-07-02 04:50:21,635 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 74.0 (TID 657). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:21,637 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 74.0 (TID 665, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:21,638 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 74.0 (TID 657) in 52551 ms on localhost (9/18)\n",
      "2015-07-02 04:50:21,638 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 74.0 (TID 665)\n",
      "2015-07-02 04:50:21,643 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:503316480+33554432\n",
      "2015-07-02 04:50:25,549 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51404, boot = -320, init = 335, finish = 51389\n",
      "2015-07-02 04:50:25,554 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 74.0 (TID 660). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:25,556 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 74.0 (TID 666, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:25,557 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 74.0 (TID 660) in 51422 ms on localhost (10/18)\n",
      "2015-07-02 04:50:25,557 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 74.0 (TID 666)\n",
      "2015-07-02 04:50:25,568 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:536870912+33554432\n",
      "2015-07-02 04:50:25,829 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51230, boot = -408, init = 412, finish = 51226\n",
      "2015-07-02 04:50:25,835 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 74.0 (TID 661). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:25,844 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 74.0 (TID 667, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:25,845 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 74.0 (TID 667)\n",
      "2015-07-02 04:50:25,845 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 74.0 (TID 661) in 51254 ms on localhost (11/18)\n",
      "2015-07-02 04:50:25,848 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:570425344+3731893\n",
      "2015-07-02 04:50:25,957 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53012, boot = -3844, init = 3862, finish = 52994\n",
      "2015-07-02 04:50:25,963 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 74.0 (TID 658). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:25,967 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 76.0 (TID 668, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:50:25,968 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 74.0 (TID 658) in 53032 ms on localhost (12/18)\n",
      "2015-07-02 04:50:25,969 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 76.0 (TID 668)\n",
      "2015-07-02 04:50:25,981 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:0+4095974\n",
      "2015-07-02 04:50:26,836 INFO  [Executor task launch worker-14] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51052, boot = -1191, init = 1223, finish = 51020\n",
      "2015-07-02 04:50:26,844 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 74.0 (TID 662). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:26,847 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 76.0 (TID 669, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:50:26,847 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 76.0 (TID 669)\n",
      "2015-07-02 04:50:26,847 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 74.0 (TID 662) in 51069 ms on localhost (13/18)\n",
      "2015-07-02 04:50:26,853 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:4095974+4095975\n",
      "2015-07-02 04:50:29,032 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52461, boot = -751, init = 760, finish = 52452\n",
      "2015-07-02 04:50:29,038 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 74.0 (TID 663). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:29,040 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 78.0 (TID 670, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:50:29,041 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 74.0 (TID 663) in 52484 ms on localhost (14/18)\n",
      "2015-07-02 04:50:29,047 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 78.0 (TID 670)\n",
      "2015-07-02 04:50:29,051 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:0+7742886\n",
      "2015-07-02 04:50:29,330 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 3358, boot = -126, init = 138, finish = 3346\n",
      "2015-07-02 04:50:29,334 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 76.0 (TID 668). 2121 bytes result sent to driver\n",
      "2015-07-02 04:50:29,335 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 78.0 (TID 671, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:50:29,336 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 76.0 (TID 668) in 3369 ms on localhost (1/2)\n",
      "2015-07-02 04:50:29,336 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 78.0 (TID 671)\n",
      "2015-07-02 04:50:29,346 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:7742886+7742887\n",
      "2015-07-02 04:50:30,617 INFO  [Executor task launch worker-14] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 3765, boot = -872, init = 877, finish = 3760\n",
      "2015-07-02 04:50:30,620 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 76.0 (TID 669). 2121 bytes result sent to driver\n",
      "2015-07-02 04:50:30,623 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 80.0 (TID 672, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:30,624 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 80.0 (TID 672)\n",
      "2015-07-02 04:50:30,624 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 76.0 (TID 669) in 3778 ms on localhost (2/2)\n",
      "2015-07-02 04:50:30,624 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:50:30,625 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 76 (reduceByKey at <ipython-input-3-4ba038715317>:46) finished in 112.266 s\n",
      "2015-07-02 04:50:30,625 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:50:30,625 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 78, Stage 88, Stage 85, Stage 82, Stage 74, Stage 80)\n",
      "2015-07-02 04:50:30,625 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:50:30,626 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:50:30,629 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:50:30,632 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:0+33554432\n",
      "2015-07-02 04:50:30,633 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:50:30,642 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:50:30,644 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:50:30,645 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:50:30,649 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List(Stage 74)\n",
      "2015-07-02 04:50:30,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:50:30,657 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:50:30,661 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 78, Stage 77)\n",
      "2015-07-02 04:50:30,663 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:50:30,666 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75)\n",
      "2015-07-02 04:50:30,668 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:50:33,038 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 7187, boot = -280, init = 285, finish = 7182\n",
      "2015-07-02 04:50:33,043 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 74.0 (TID 667). 2137 bytes result sent to driver\n",
      "2015-07-02 04:50:33,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 80.0 (TID 673, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:33,045 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 74.0 (TID 667) in 7201 ms on localhost (15/18)\n",
      "2015-07-02 04:50:33,045 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 80.0 (TID 673)\n",
      "2015-07-02 04:50:33,049 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:33554432+33554432\n",
      "2015-07-02 04:50:36,021 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 6972, boot = -2198, init = 2204, finish = 6966\n",
      "2015-07-02 04:50:36,026 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 78.0 (TID 670). 1958 bytes result sent to driver\n",
      "2015-07-02 04:50:36,028 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 80.0 (TID 674, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:36,030 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 78.0 (TID 670) in 6991 ms on localhost (1/2)\n",
      "2015-07-02 04:50:36,031 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 80.0 (TID 674)\n",
      "2015-07-02 04:50:36,041 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:67108864+33554432\n",
      "2015-07-02 04:50:36,227 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 6887, boot = -290, init = 299, finish = 6878\n",
      "2015-07-02 04:50:36,232 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 78.0 (TID 671). 1958 bytes result sent to driver\n",
      "2015-07-02 04:50:36,235 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 80.0 (TID 675, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:50:36,236 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 80.0 (TID 675)\n",
      "2015-07-02 04:50:36,237 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 78.0 (TID 671) in 6901 ms on localhost (2/2)\n",
      "2015-07-02 04:50:36,237 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:50:36,238 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 78 (reduceByKey at <ipython-input-3-4ba038715317>:46) finished in 117.862 s\n",
      "2015-07-02 04:50:36,238 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:50:36,238 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 82, Stage 74, Stage 80)\n",
      "2015-07-02 04:50:36,238 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:50:36,238 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:50:36,242 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:50:36,245 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:100663296+33554432\n",
      "2015-07-02 04:50:36,251 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:50:36,253 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:50:36,254 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:50:36,254 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:50:36,257 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List(Stage 74)\n",
      "2015-07-02 04:50:36,268 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:50:36,272 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:50:36,275 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 77)\n",
      "2015-07-02 04:50:36,277 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:50:36,280 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75)\n",
      "2015-07-02 04:50:36,288 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:51:11,074 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52142, boot = -42357, init = 42365, finish = 52134\n",
      "2015-07-02 04:51:11,079 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 74.0 (TID 664). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:11,083 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 80.0 (TID 676, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:11,083 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 74.0 (TID 664) in 52161 ms on localhost (16/18)\n",
      "2015-07-02 04:51:11,084 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 80.0 (TID 676)\n",
      "2015-07-02 04:51:11,092 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:134217728+33554432\n",
      "2015-07-02 04:51:14,212 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52568, boot = -2707, init = 2714, finish = 52561\n",
      "2015-07-02 04:51:14,219 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 74.0 (TID 665). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:14,220 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 80.0 (TID 677, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:14,221 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 80.0 (TID 677)\n",
      "2015-07-02 04:51:14,221 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 74.0 (TID 665) in 52585 ms on localhost (17/18)\n",
      "2015-07-02 04:51:14,232 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:167772160+33554432\n",
      "2015-07-02 04:51:22,048 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56485, boot = -3924, init = 3937, finish = 56472\n",
      "2015-07-02 04:51:22,053 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 74.0 (TID 666). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:22,054 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 80.0 (TID 678, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:22,055 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 74.0 (TID 666) in 56500 ms on localhost (18/18)\n",
      "2015-07-02 04:51:22,055 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:51:22,056 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 80.0 (TID 678)\n",
      "2015-07-02 04:51:22,056 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 74 (reduceByKey at <ipython-input-3-4ba038715317>:34) finished in 163.710 s\n",
      "2015-07-02 04:51:22,056 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:51:22,056 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 82, Stage 80)\n",
      "2015-07-02 04:51:22,056 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 75, Stage 90, Stage 87, Stage 79, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:51:22,057 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:51:22,061 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:51:22,065 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:51:22,066 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:201326592+33554432\n",
      "2015-07-02 04:51:22,072 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:51:22,073 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:51:22,075 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:51:22,082 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 75: List()\n",
      "2015-07-02 04:51:22,086 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:51:22,090 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:51:22,101 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 77)\n",
      "2015-07-02 04:51:22,104 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:51:22,108 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List(Stage 75)\n",
      "2015-07-02 04:51:22,109 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:51:22,110 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 75 (PairwiseRDD[289] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:51:22,114 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13200) called with curMem=3271038, maxMem=278302556\n",
      "2015-07-02 04:51:22,118 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_53 stored as values in memory (estimated size 12.9 KB, free 262.3 MB)\n",
      "2015-07-02 04:51:22,119 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7263) called with curMem=3284238, maxMem=278302556\n",
      "2015-07-02 04:51:22,120 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_53_piece0 stored as bytes in memory (estimated size 7.1 KB, free 262.3 MB)\n",
      "2015-07-02 04:51:22,127 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_53_piece0 in memory on localhost:40918 (size: 7.1 KB, free: 265.0 MB)\n",
      "2015-07-02 04:51:22,128 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_53_piece0\n",
      "2015-07-02 04:51:22,130 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 53 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:51:22,133 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 22 missing tasks from Stage 75 (PairwiseRDD[289] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:51:22,133 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 75.0 with 22 tasks\n",
      "2015-07-02 04:51:43,379 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67140, boot = -198, init = 211, finish = 67127\n",
      "2015-07-02 04:51:43,401 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 80.0 (TID 675). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:43,403 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 75.0 (TID 679, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,403 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 75.0 (TID 679)\n",
      "2015-07-02 04:51:43,407 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 80.0 (TID 675) in 67172 ms on localhost (1/18)\n",
      "2015-07-02 04:51:43,409 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 4 non-empty blocks out of 4 blocks\n",
      "2015-07-02 04:51:43,410 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,416 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 4, boot = -1, init = 4, finish = 1\n",
      "2015-07-02 04:51:43,431 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -21338, init = 21350, finish = 1\n",
      "2015-07-02 04:51:43,437 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 75.0 (TID 679). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,439 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 75.0 (TID 680, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,440 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 75.0 (TID 679) in 38 ms on localhost (1/22)\n",
      "2015-07-02 04:51:43,440 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 75.0 (TID 680)\n",
      "2015-07-02 04:51:43,444 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 4 non-empty blocks out of 4 blocks\n",
      "2015-07-02 04:51:43,445 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,489 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -4, init = 48, finish = 2\n",
      "2015-07-02 04:51:43,494 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = 1, init = 47, finish = 0\n",
      "2015-07-02 04:51:43,505 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 75.0 (TID 680). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,507 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 75.0 (TID 681, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,507 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 75.0 (TID 680) in 68 ms on localhost (2/22)\n",
      "2015-07-02 04:51:43,508 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 75.0 (TID 681)\n",
      "2015-07-02 04:51:43,512 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 4 non-empty blocks out of 4 blocks\n",
      "2015-07-02 04:51:43,512 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:43,560 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -12, init = 54, finish = 1\n",
      "2015-07-02 04:51:43,568 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -8, init = 59, finish = 2\n",
      "2015-07-02 04:51:43,592 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 75.0 (TID 681). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,593 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 75.0 (TID 682, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,593 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 75.0 (TID 681) in 87 ms on localhost (3/22)\n",
      "2015-07-02 04:51:43,593 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 75.0 (TID 682)\n",
      "2015-07-02 04:51:43,598 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 4 non-empty blocks out of 4 blocks\n",
      "2015-07-02 04:51:43,599 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,642 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 47, finish = 0\n",
      "2015-07-02 04:51:43,652 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -9, init = 56, finish = 1\n",
      "2015-07-02 04:51:43,668 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 75.0 (TID 682). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,676 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 75.0 (TID 683, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,676 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 75.0 (TID 683)\n",
      "2015-07-02 04:51:43,677 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 75.0 (TID 682) in 85 ms on localhost (4/22)\n",
      "2015-07-02 04:51:43,681 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:43,682 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,693 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -1, init = 7, finish = 7\n",
      "2015-07-02 04:51:43,729 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -7, init = 48, finish = 1\n",
      "2015-07-02 04:51:43,734 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 75.0 (TID 683). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,743 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 75.0 (TID 684, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,743 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 75.0 (TID 683) in 68 ms on localhost (5/22)\n",
      "2015-07-02 04:51:43,743 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 75.0 (TID 684)\n",
      "2015-07-02 04:51:43,748 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:43,749 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,759 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -7, init = 12, finish = 5\n",
      "2015-07-02 04:51:43,792 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -41, init = 82, finish = 0\n",
      "2015-07-02 04:51:43,798 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 75.0 (TID 684). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,800 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 75.0 (TID 685, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,801 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 75.0 (TID 684) in 59 ms on localhost (6/22)\n",
      "2015-07-02 04:51:43,802 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 75.0 (TID 685)\n",
      "2015-07-02 04:51:43,808 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:43,809 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,818 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -1, init = 8, finish = 3\n",
      "2015-07-02 04:51:43,854 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -35, init = 77, finish = 1\n",
      "2015-07-02 04:51:43,866 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 75.0 (TID 685). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,868 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 75.0 (TID 686, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,868 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 75.0 (TID 685) in 68 ms on localhost (7/22)\n",
      "2015-07-02 04:51:43,870 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 75.0 (TID 686)\n",
      "2015-07-02 04:51:43,876 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:43,877 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:43,885 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 12, boot = -6, init = 14, finish = 4\n",
      "2015-07-02 04:51:43,921 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -49, init = 89, finish = 0\n",
      "2015-07-02 04:51:43,933 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 75.0 (TID 686). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:43,934 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 75.0 (TID 687, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:43,935 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 75.0 (TID 687)\n",
      "2015-07-02 04:51:43,936 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 75.0 (TID 686) in 69 ms on localhost (8/22)\n",
      "2015-07-02 04:51:43,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:43,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:43,949 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -8, init = 15, finish = 3\n",
      "2015-07-02 04:51:43,985 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -40, init = 81, finish = 1\n",
      "2015-07-02 04:51:43,998 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 75.0 (TID 687). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,000 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 75.0 (TID 688, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,001 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 75.0 (TID 687) in 67 ms on localhost (9/22)\n",
      "2015-07-02 04:51:44,002 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 75.0 (TID 688)\n",
      "2015-07-02 04:51:44,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,016 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 9, boot = -7, init = 13, finish = 3\n",
      "2015-07-02 04:51:44,051 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -47, init = 89, finish = 0\n",
      "2015-07-02 04:51:44,058 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 75.0 (TID 688). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,059 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 75.0 (TID 689, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,060 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 75.0 (TID 688) in 60 ms on localhost (10/22)\n",
      "2015-07-02 04:51:44,061 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 75.0 (TID 689)\n",
      "2015-07-02 04:51:44,065 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,075 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 11, boot = -5, init = 12, finish = 4\n",
      "2015-07-02 04:51:44,110 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:51:44,120 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 75.0 (TID 689). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,123 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 75.0 (TID 690, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,123 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 75.0 (TID 690)\n",
      "2015-07-02 04:51:44,123 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 75.0 (TID 689) in 64 ms on localhost (11/22)\n",
      "2015-07-02 04:51:44,133 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,134 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,144 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = 0, init = 9, finish = 4\n",
      "2015-07-02 04:51:44,178 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -40, init = 87, finish = 0\n",
      "2015-07-02 04:51:44,192 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 75.0 (TID 690). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,195 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 75.0 (TID 691, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,195 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 75.0 (TID 691)\n",
      "2015-07-02 04:51:44,196 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 75.0 (TID 690) in 73 ms on localhost (12/22)\n",
      "2015-07-02 04:51:44,201 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,213 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 9, boot = -7, init = 14, finish = 2\n",
      "2015-07-02 04:51:44,246 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -43, init = 85, finish = 0\n",
      "2015-07-02 04:51:44,262 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 75.0 (TID 691). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,263 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 75.0 (TID 692, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,264 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 75.0 (TID 691) in 70 ms on localhost (13/22)\n",
      "2015-07-02 04:51:44,264 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 75.0 (TID 692)\n",
      "2015-07-02 04:51:44,269 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,270 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,283 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -8, init = 17, finish = 4\n",
      "2015-07-02 04:51:44,316 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -47, init = 88, finish = 1\n",
      "2015-07-02 04:51:44,327 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 75.0 (TID 692). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,329 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 75.0 (TID 693, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,329 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 75.0 (TID 692) in 66 ms on localhost (14/22)\n",
      "2015-07-02 04:51:44,329 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 75.0 (TID 693)\n",
      "2015-07-02 04:51:44,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,336 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,347 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 14, boot = -9, init = 18, finish = 5\n",
      "2015-07-02 04:51:44,380 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -40, init = 82, finish = 0\n",
      "2015-07-02 04:51:44,397 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 75.0 (TID 693). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,399 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 75.0 (TID 694, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,399 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 75.0 (TID 693) in 70 ms on localhost (15/22)\n",
      "2015-07-02 04:51:44,403 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 75.0 (TID 694)\n",
      "2015-07-02 04:51:44,409 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,409 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,419 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 11, boot = -12, init = 18, finish = 5\n",
      "2015-07-02 04:51:44,457 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -49, init = 90, finish = 1\n",
      "2015-07-02 04:51:44,470 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 75.0 (TID 694). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,472 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 75.0 (TID 695, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,472 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 75.0 (TID 695)\n",
      "2015-07-02 04:51:44,472 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 75.0 (TID 694) in 74 ms on localhost (16/22)\n",
      "2015-07-02 04:51:44,478 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,478 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,488 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -6, init = 12, finish = 4\n",
      "2015-07-02 04:51:44,524 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -47, init = 89, finish = 1\n",
      "2015-07-02 04:51:44,534 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 75.0 (TID 695). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,538 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 75.0 (TID 696, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,539 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 75.0 (TID 695) in 68 ms on localhost (17/22)\n",
      "2015-07-02 04:51:44,540 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 75.0 (TID 696)\n",
      "2015-07-02 04:51:44,546 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,547 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,557 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -1, init = 8, finish = 3\n",
      "2015-07-02 04:51:44,590 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -43, init = 85, finish = 1\n",
      "2015-07-02 04:51:44,600 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 75.0 (TID 696). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,603 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 75.0 (TID 697, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,604 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 75.0 (TID 696) in 67 ms on localhost (18/22)\n",
      "2015-07-02 04:51:44,605 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 75.0 (TID 697)\n",
      "2015-07-02 04:51:44,611 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,612 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:44,623 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -2, init = 9, finish = 6\n",
      "2015-07-02 04:51:44,658 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:51:44,670 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 75.0 (TID 697). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,672 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 75.0 (TID 698, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,673 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 75.0 (TID 697) in 69 ms on localhost (19/22)\n",
      "2015-07-02 04:51:44,673 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 75.0 (TID 698)\n",
      "2015-07-02 04:51:44,679 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,679 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,691 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -8, init = 15, finish = 6\n",
      "2015-07-02 04:51:44,726 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -41, init = 84, finish = 0\n",
      "2015-07-02 04:51:44,741 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 75.0 (TID 698). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,743 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 75.0 (TID 699, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,744 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 75.0 (TID 698) in 72 ms on localhost (20/22)\n",
      "2015-07-02 04:51:44,744 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 75.0 (TID 699)\n",
      "2015-07-02 04:51:44,748 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,748 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,756 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 6, boot = -6, init = 10, finish = 2\n",
      "2015-07-02 04:51:44,794 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -44, init = 85, finish = 0\n",
      "2015-07-02 04:51:44,810 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 75.0 (TID 699). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,812 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 75.0 (TID 700, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:44,813 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 75.0 (TID 699) in 71 ms on localhost (21/22)\n",
      "2015-07-02 04:51:44,814 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 75.0 (TID 700)\n",
      "2015-07-02 04:51:44,819 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:51:44,819 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:44,831 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 10, boot = -7, init = 13, finish = 4\n",
      "2015-07-02 04:51:44,862 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -56, init = 98, finish = 0\n",
      "2015-07-02 04:51:44,867 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 75.0 (TID 700). 1083 bytes result sent to driver\n",
      "2015-07-02 04:51:44,869 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 80.0 (TID 701, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:44,869 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 75.0 (TID 700) in 57 ms on localhost (22/22)\n",
      "2015-07-02 04:51:44,869 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:51:44,870 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 80.0 (TID 701)\n",
      "2015-07-02 04:51:44,871 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 75 (join at <ipython-input-3-4ba038715317>:131) finished in 22.735 s\n",
      "2015-07-02 04:51:44,871 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:51:44,871 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 82, Stage 80)\n",
      "2015-07-02 04:51:44,871 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 90, Stage 87, Stage 79, Stage 91, Stage 77, Stage 92)\n",
      "2015-07-02 04:51:44,871 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:51:44,876 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:51:44,880 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:51:44,884 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:51:44,885 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:51:44,887 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:51:44,891 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:234881024+33554432\n",
      "2015-07-02 04:51:44,892 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:51:44,896 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:51:44,900 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List(Stage 77)\n",
      "2015-07-02 04:51:44,902 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:51:44,906 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 77: List()\n",
      "2015-07-02 04:51:44,907 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:51:44,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 77 (PairwiseRDD[296] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:51:44,911 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13224) called with curMem=3291501, maxMem=278302556\n",
      "2015-07-02 04:51:44,912 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_54 stored as values in memory (estimated size 12.9 KB, free 262.3 MB)\n",
      "2015-07-02 04:51:44,913 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7274) called with curMem=3304725, maxMem=278302556\n",
      "2015-07-02 04:51:44,913 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_54_piece0 stored as bytes in memory (estimated size 7.1 KB, free 262.3 MB)\n",
      "2015-07-02 04:51:44,914 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_54_piece0 in memory on localhost:40918 (size: 7.1 KB, free: 265.0 MB)\n",
      "2015-07-02 04:51:44,915 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_54_piece0\n",
      "2015-07-02 04:51:44,916 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 54 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:51:44,919 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 24 missing tasks from Stage 77 (PairwiseRDD[296] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:51:44,920 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 77.0 with 24 tasks\n",
      "2015-07-02 04:51:47,059 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71025, boot = -2972, init = 2984, finish = 71013\n",
      "2015-07-02 04:51:47,069 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 80.0 (TID 674). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:47,071 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 77.0 (TID 702, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,072 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 80.0 (TID 674) in 71044 ms on localhost (2/18)\n",
      "2015-07-02 04:51:47,079 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 77.0 (TID 702)\n",
      "2015-07-02 04:51:47,083 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,084 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,094 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 6, boot = 0, init = 6, finish = 0\n",
      "2015-07-02 04:51:47,134 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -2212, init = 2253, finish = 1\n",
      "2015-07-02 04:51:47,139 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 77.0 (TID 702). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,140 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 77.0 (TID 703, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,141 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 77.0 (TID 702) in 70 ms on localhost (1/24)\n",
      "2015-07-02 04:51:47,141 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 77.0 (TID 703)\n",
      "2015-07-02 04:51:47,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,147 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,188 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -3, init = 45, finish = 0\n",
      "2015-07-02 04:51:47,197 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -47, init = 91, finish = 0\n",
      "2015-07-02 04:51:47,211 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 77.0 (TID 703). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,213 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 77.0 (TID 704, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,214 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 77.0 (TID 704)\n",
      "2015-07-02 04:51:47,215 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 77.0 (TID 703) in 74 ms on localhost (2/24)\n",
      "2015-07-02 04:51:47,219 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,220 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,262 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 49, finish = 1\n",
      "2015-07-02 04:51:47,272 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -17, init = 63, finish = 1\n",
      "2015-07-02 04:51:47,283 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 77.0 (TID 704). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,290 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 77.0 (TID 705, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,291 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 77.0 (TID 704) in 78 ms on localhost (3/24)\n",
      "2015-07-02 04:51:47,292 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 77.0 (TID 705)\n",
      "2015-07-02 04:51:47,297 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,297 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:47,337 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 47, finish = 1\n",
      "2015-07-02 04:51:47,346 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -19, init = 65, finish = 0\n",
      "2015-07-02 04:51:47,362 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 77.0 (TID 705). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,364 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 77.0 (TID 706, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,365 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 77.0 (TID 705) in 74 ms on localhost (4/24)\n",
      "2015-07-02 04:51:47,365 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 77.0 (TID 706)\n",
      "2015-07-02 04:51:47,370 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,371 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,415 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -8, init = 50, finish = 1\n",
      "2015-07-02 04:51:47,425 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -8, init = 56, finish = 0\n",
      "2015-07-02 04:51:47,432 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 77.0 (TID 706). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,434 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 77.0 (TID 707, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,434 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 77.0 (TID 707)\n",
      "2015-07-02 04:51:47,434 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 77.0 (TID 706) in 71 ms on localhost (5/24)\n",
      "2015-07-02 04:51:47,438 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,438 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,480 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 49, finish = 1\n",
      "2015-07-02 04:51:47,485 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -13, init = 58, finish = 0\n",
      "2015-07-02 04:51:47,497 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 77.0 (TID 707). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,498 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 77.0 (TID 708, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,499 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 77.0 (TID 707) in 66 ms on localhost (6/24)\n",
      "2015-07-02 04:51:47,499 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 77.0 (TID 708)\n",
      "2015-07-02 04:51:47,503 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,503 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:47,546 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 46, finish = 1\n",
      "2015-07-02 04:51:47,553 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -9, init = 56, finish = 0\n",
      "2015-07-02 04:51:47,560 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 77.0 (TID 708). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,561 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 77.0 (TID 709, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,562 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 77.0 (TID 708) in 64 ms on localhost (7/24)\n",
      "2015-07-02 04:51:47,564 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 77.0 (TID 709)\n",
      "2015-07-02 04:51:47,571 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,572 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,615 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -3, init = 45, finish = 0\n",
      "2015-07-02 04:51:47,624 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -11, init = 60, finish = 0\n",
      "2015-07-02 04:51:47,651 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 77.0 (TID 709). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,653 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 77.0 (TID 710, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,654 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 77.0 (TID 709) in 93 ms on localhost (8/24)\n",
      "2015-07-02 04:51:47,654 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 77.0 (TID 710)\n",
      "2015-07-02 04:51:47,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:47,701 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 46, finish = 1\n",
      "2015-07-02 04:51:47,716 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -6, init = 51, finish = 1\n",
      "2015-07-02 04:51:47,728 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 77.0 (TID 710). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,733 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 77.0 (TID 711, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,734 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 77.0 (TID 711)\n",
      "2015-07-02 04:51:47,739 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 77.0 (TID 710) in 86 ms on localhost (9/24)\n",
      "2015-07-02 04:51:47,748 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,748 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:47,788 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -9, init = 55, finish = 1\n",
      "2015-07-02 04:51:47,795 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -9, init = 62, finish = 1\n",
      "2015-07-02 04:51:47,809 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 77.0 (TID 711). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,810 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 77.0 (TID 712, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,812 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 77.0 (TID 712)\n",
      "2015-07-02 04:51:47,812 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 77.0 (TID 711) in 79 ms on localhost (10/24)\n",
      "2015-07-02 04:51:47,816 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,816 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:47,860 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -12, init = 54, finish = 1\n",
      "2015-07-02 04:51:47,869 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -19, init = 67, finish = 1\n",
      "2015-07-02 04:51:47,875 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 77.0 (TID 712). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,876 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 77.0 (TID 713, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,877 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 77.0 (TID 712) in 67 ms on localhost (11/24)\n",
      "2015-07-02 04:51:47,880 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 77.0 (TID 713)\n",
      "2015-07-02 04:51:47,886 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,887 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:47,927 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -5, init = 47, finish = 1\n",
      "2015-07-02 04:51:47,932 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -11, init = 56, finish = 0\n",
      "2015-07-02 04:51:47,940 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 77.0 (TID 713). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:47,943 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 77.0 (TID 714, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:47,944 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 77.0 (TID 713) in 68 ms on localhost (12/24)\n",
      "2015-07-02 04:51:47,948 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 77.0 (TID 714)\n",
      "2015-07-02 04:51:47,952 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:47,956 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 4 ms\n",
      "2015-07-02 04:51:47,996 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 50, finish = 1\n",
      "2015-07-02 04:51:48,010 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -15, init = 61, finish = 1\n",
      "2015-07-02 04:51:48,017 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 77.0 (TID 714). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,022 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 77.0 (TID 715, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,023 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 77.0 (TID 714) in 80 ms on localhost (13/24)\n",
      "2015-07-02 04:51:48,026 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 77.0 (TID 715)\n",
      "2015-07-02 04:51:48,030 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,030 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,070 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -8, init = 48, finish = 1\n",
      "2015-07-02 04:51:48,077 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -9, init = 55, finish = 0\n",
      "2015-07-02 04:51:48,086 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 77.0 (TID 715). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,087 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 77.0 (TID 716, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,089 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 77.0 (TID 715) in 67 ms on localhost (14/24)\n",
      "2015-07-02 04:51:48,090 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 77.0 (TID 716)\n",
      "2015-07-02 04:51:48,103 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,104 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:48,147 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -5, init = 45, finish = 1\n",
      "2015-07-02 04:51:48,154 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = 4, init = 49, finish = 0\n",
      "2015-07-02 04:51:48,164 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 77.0 (TID 716). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,166 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 77.0 (TID 717, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,167 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 77.0 (TID 717)\n",
      "2015-07-02 04:51:48,170 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 77.0 (TID 716) in 83 ms on localhost (15/24)\n",
      "2015-07-02 04:51:48,173 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,173 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,216 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -7, init = 47, finish = 0\n",
      "2015-07-02 04:51:48,226 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -14, init = 62, finish = 0\n",
      "2015-07-02 04:51:48,241 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 77.0 (TID 717). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,250 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 77.0 (TID 718, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,251 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 77.0 (TID 717) in 86 ms on localhost (16/24)\n",
      "2015-07-02 04:51:48,252 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 77.0 (TID 718)\n",
      "2015-07-02 04:51:48,257 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,257 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,301 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 47, finish = 1\n",
      "2015-07-02 04:51:48,317 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -6, init = 53, finish = 1\n",
      "2015-07-02 04:51:48,327 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 77.0 (TID 718). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,329 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 77.0 (TID 719, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,330 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 77.0 (TID 719)\n",
      "2015-07-02 04:51:48,330 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 77.0 (TID 718) in 80 ms on localhost (17/24)\n",
      "2015-07-02 04:51:48,342 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,342 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,386 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -1, init = 42, finish = 1\n",
      "2015-07-02 04:51:48,400 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = 0, init = 48, finish = 0\n",
      "2015-07-02 04:51:48,419 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 77.0 (TID 719). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,422 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 77.0 (TID 720, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,423 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 77.0 (TID 719) in 95 ms on localhost (18/24)\n",
      "2015-07-02 04:51:48,423 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 77.0 (TID 720)\n",
      "2015-07-02 04:51:48,426 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,426 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,470 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -1, init = 42, finish = 0\n",
      "2015-07-02 04:51:48,479 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -8, init = 55, finish = 1\n",
      "2015-07-02 04:51:48,498 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 77.0 (TID 720). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,500 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 77.0 (TID 721, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,501 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 77.0 (TID 720) in 80 ms on localhost (19/24)\n",
      "2015-07-02 04:51:48,503 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 77.0 (TID 721)\n",
      "2015-07-02 04:51:48,507 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,507 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,552 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 50, finish = 1\n",
      "2015-07-02 04:51:48,561 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -13, init = 61, finish = 0\n",
      "2015-07-02 04:51:48,580 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 77.0 (TID 721). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,581 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 77.0 (TID 722, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,582 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 77.0 (TID 722)\n",
      "2015-07-02 04:51:48,582 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 77.0 (TID 721) in 83 ms on localhost (20/24)\n",
      "2015-07-02 04:51:48,589 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,590 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:48,633 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 04:51:48,637 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -5, init = 55, finish = 0\n",
      "2015-07-02 04:51:48,660 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 77.0 (TID 722). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,665 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 77.0 (TID 723, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,666 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 77.0 (TID 722) in 85 ms on localhost (21/24)\n",
      "2015-07-02 04:51:48,669 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 77.0 (TID 723)\n",
      "2015-07-02 04:51:48,674 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 22 non-empty blocks out of 22 blocks\n",
      "2015-07-02 04:51:48,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:48,715 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 04:51:48,724 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -7, init = 53, finish = 0\n",
      "2015-07-02 04:51:48,749 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 77.0 (TID 723). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,750 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 77.0 (TID 724, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,751 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 77.0 (TID 723) in 86 ms on localhost (22/24)\n",
      "2015-07-02 04:51:48,752 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 77.0 (TID 724)\n",
      "2015-07-02 04:51:48,756 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:51:48,757 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:48,768 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -4, init = 7, finish = 10\n",
      "2015-07-02 04:51:48,806 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -8, init = 49, finish = 0\n",
      "2015-07-02 04:51:48,818 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 77.0 (TID 724). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,819 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 77.0 (TID 725, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:48,820 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 77.0 (TID 724) in 70 ms on localhost (23/24)\n",
      "2015-07-02 04:51:48,824 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 77.0 (TID 725)\n",
      "2015-07-02 04:51:48,829 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:51:48,829 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:48,842 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 15, boot = -10, init = 13, finish = 12\n",
      "2015-07-02 04:51:48,878 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -48, init = 90, finish = 1\n",
      "2015-07-02 04:51:48,889 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 77.0 (TID 725). 1085 bytes result sent to driver\n",
      "2015-07-02 04:51:48,894 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 80.0 (TID 726, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:48,894 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 77.0 (TID 725) in 75 ms on localhost (24/24)\n",
      "2015-07-02 04:51:48,895 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:51:48,896 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 77 (join at <ipython-input-3-4ba038715317>:131) finished in 3.973 s\n",
      "2015-07-02 04:51:48,896 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:51:48,896 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 82, Stage 80)\n",
      "2015-07-02 04:51:48,896 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 90, Stage 87, Stage 79, Stage 91, Stage 92)\n",
      "2015-07-02 04:51:48,896 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:51:48,897 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 80.0 (TID 726)\n",
      "2015-07-02 04:51:48,901 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:51:48,902 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:268435456+33554432\n",
      "2015-07-02 04:51:48,904 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 79, Stage 80)\n",
      "2015-07-02 04:51:48,916 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:51:48,917 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:51:48,919 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:51:48,922 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:51:48,926 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:51:48,929 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 79: List()\n",
      "2015-07-02 04:51:48,931 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:51:48,941 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:51:48,942 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 79 (PairwiseRDD[303] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:51:48,944 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13224) called with curMem=3311999, maxMem=278302556\n",
      "2015-07-02 04:51:48,944 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_55 stored as values in memory (estimated size 12.9 KB, free 262.2 MB)\n",
      "2015-07-02 04:51:48,946 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7275) called with curMem=3325223, maxMem=278302556\n",
      "2015-07-02 04:51:48,946 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_55_piece0 stored as bytes in memory (estimated size 7.1 KB, free 262.2 MB)\n",
      "2015-07-02 04:51:48,947 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_55_piece0 in memory on localhost:40918 (size: 7.1 KB, free: 265.0 MB)\n",
      "2015-07-02 04:51:48,948 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_55_piece0\n",
      "2015-07-02 04:51:48,949 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 55 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:51:48,952 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 26 missing tasks from Stage 79 (PairwiseRDD[303] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:51:48,959 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 79.0 with 26 tasks\n",
      "2015-07-02 04:51:57,182 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 84133, boot = -2408, init = 2412, finish = 84129\n",
      "2015-07-02 04:51:57,189 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 80.0 (TID 673). 2137 bytes result sent to driver\n",
      "2015-07-02 04:51:57,191 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 79.0 (TID 727, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,192 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 80.0 (TID 673) in 84148 ms on localhost (3/18)\n",
      "2015-07-02 04:51:57,198 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 79.0 (TID 727)\n",
      "2015-07-02 04:51:57,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,203 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,211 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 6, boot = -4, init = 10, finish = 0\n",
      "2015-07-02 04:51:57,250 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8307, init = 8349, finish = 0\n",
      "2015-07-02 04:51:57,254 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 79.0 (TID 727). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,261 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 79.0 (TID 728, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,262 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 79.0 (TID 728)\n",
      "2015-07-02 04:51:57,262 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 79.0 (TID 727) in 72 ms on localhost (1/26)\n",
      "2015-07-02 04:51:57,267 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,268 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,311 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -2, init = 46, finish = 0\n",
      "2015-07-02 04:51:57,319 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -47, init = 95, finish = 0\n",
      "2015-07-02 04:51:57,335 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 79.0 (TID 728). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,337 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 79.0 (TID 729, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,338 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 79.0 (TID 728) in 77 ms on localhost (2/26)\n",
      "2015-07-02 04:51:57,338 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 79.0 (TID 729)\n",
      "2015-07-02 04:51:57,343 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,343 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,386 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -8, init = 49, finish = 0\n",
      "2015-07-02 04:51:57,393 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -20, init = 66, finish = 0\n",
      "2015-07-02 04:51:57,403 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 79.0 (TID 729). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,404 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 79.0 (TID 730, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,405 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 79.0 (TID 730)\n",
      "2015-07-02 04:51:57,406 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 79.0 (TID 729) in 69 ms on localhost (3/26)\n",
      "2015-07-02 04:51:57,413 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,413 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,457 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = 0, init = 42, finish = 0\n",
      "2015-07-02 04:51:57,465 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -11, init = 62, finish = 0\n",
      "2015-07-02 04:51:57,480 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 79.0 (TID 730). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,482 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 79.0 (TID 731, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,483 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 79.0 (TID 730) in 78 ms on localhost (4/26)\n",
      "2015-07-02 04:51:57,483 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 79.0 (TID 731)\n",
      "2015-07-02 04:51:57,487 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,487 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:57,528 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -5, init = 46, finish = 0\n",
      "2015-07-02 04:51:57,536 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -17, init = 64, finish = 0\n",
      "2015-07-02 04:51:57,543 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 79.0 (TID 731). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,552 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 79.0 (TID 732, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,553 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 79.0 (TID 731) in 72 ms on localhost (5/26)\n",
      "2015-07-02 04:51:57,554 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 79.0 (TID 732)\n",
      "2015-07-02 04:51:57,562 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,562 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:57,606 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 04:51:57,616 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -6, init = 57, finish = 0\n",
      "2015-07-02 04:51:57,639 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 79.0 (TID 732). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,643 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 79.0 (TID 733, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,644 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 79.0 (TID 733)\n",
      "2015-07-02 04:51:57,645 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 79.0 (TID 732) in 93 ms on localhost (6/26)\n",
      "2015-07-02 04:51:57,650 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,650 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,691 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 47, finish = 1\n",
      "2015-07-02 04:51:57,700 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -14, init = 60, finish = 0\n",
      "2015-07-02 04:51:57,718 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 79.0 (TID 733). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,720 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 79.0 (TID 734, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,721 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 79.0 (TID 733) in 78 ms on localhost (7/26)\n",
      "2015-07-02 04:51:57,722 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 79.0 (TID 734)\n",
      "2015-07-02 04:51:57,725 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,725 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:57,769 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -7, init = 48, finish = 1\n",
      "2015-07-02 04:51:57,778 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -15, init = 61, finish = 0\n",
      "2015-07-02 04:51:57,788 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 79.0 (TID 734). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,790 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 79.0 (TID 735, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,791 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 79.0 (TID 734) in 72 ms on localhost (8/26)\n",
      "2015-07-02 04:51:57,792 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 79.0 (TID 735)\n",
      "2015-07-02 04:51:57,796 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,797 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,837 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -1, init = 42, finish = 1\n",
      "2015-07-02 04:51:57,849 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -12, init = 57, finish = 0\n",
      "2015-07-02 04:51:57,861 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 79.0 (TID 735). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,863 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 79.0 (TID 736, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,864 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 79.0 (TID 735) in 75 ms on localhost (9/26)\n",
      "2015-07-02 04:51:57,864 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 79.0 (TID 736)\n",
      "2015-07-02 04:51:57,868 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,869 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,910 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 47, finish = 0\n",
      "2015-07-02 04:51:57,917 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -20, init = 64, finish = 0\n",
      "2015-07-02 04:51:57,921 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 79.0 (TID 736). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,923 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 79.0 (TID 737, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,924 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 79.0 (TID 737)\n",
      "2015-07-02 04:51:57,925 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 79.0 (TID 736) in 62 ms on localhost (10/26)\n",
      "2015-07-02 04:51:57,931 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:57,932 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:57,977 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -1, init = 44, finish = 1\n",
      "2015-07-02 04:51:57,984 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = 0, init = 52, finish = 0\n",
      "2015-07-02 04:51:57,995 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 79.0 (TID 737). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:57,997 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 79.0 (TID 738, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:57,998 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 79.0 (TID 737) in 76 ms on localhost (11/26)\n",
      "2015-07-02 04:51:57,998 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 79.0 (TID 738)\n",
      "2015-07-02 04:51:58,005 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,006 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,049 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -10, init = 51, finish = 1\n",
      "2015-07-02 04:51:58,064 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -14, init = 64, finish = 0\n",
      "2015-07-02 04:51:58,084 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 79.0 (TID 738). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,085 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 79.0 (TID 739, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,087 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 79.0 (TID 738) in 90 ms on localhost (12/26)\n",
      "2015-07-02 04:51:58,087 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 79.0 (TID 739)\n",
      "2015-07-02 04:51:58,094 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,095 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,137 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -9, init = 52, finish = 0\n",
      "2015-07-02 04:51:58,144 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -6, init = 54, finish = 0\n",
      "2015-07-02 04:51:58,161 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 79.0 (TID 739). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,163 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 79.0 (TID 740, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,163 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 79.0 (TID 739) in 78 ms on localhost (13/26)\n",
      "2015-07-02 04:51:58,165 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 79.0 (TID 740)\n",
      "2015-07-02 04:51:58,170 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,216 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -1, init = 43, finish = 0\n",
      "2015-07-02 04:51:58,225 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -1, init = 50, finish = 1\n",
      "2015-07-02 04:51:58,237 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 79.0 (TID 740). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,239 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 79.0 (TID 741, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,240 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 79.0 (TID 740) in 77 ms on localhost (14/26)\n",
      "2015-07-02 04:51:58,242 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 79.0 (TID 741)\n",
      "2015-07-02 04:51:58,247 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,247 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:58,291 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 04:51:58,299 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -9, init = 56, finish = 1\n",
      "2015-07-02 04:51:58,312 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 79.0 (TID 741). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,314 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 79.0 (TID 742, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,315 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 79.0 (TID 742)\n",
      "2015-07-02 04:51:58,315 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 79.0 (TID 741) in 76 ms on localhost (15/26)\n",
      "2015-07-02 04:51:58,320 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,320 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,361 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -12, init = 52, finish = 1\n",
      "2015-07-02 04:51:58,369 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -11, init = 58, finish = 0\n",
      "2015-07-02 04:51:58,390 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 79.0 (TID 742). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,392 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 79.0 (TID 743, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,393 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 79.0 (TID 742) in 79 ms on localhost (16/26)\n",
      "2015-07-02 04:51:58,394 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 79.0 (TID 743)\n",
      "2015-07-02 04:51:58,398 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,399 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,444 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 50, finish = 0\n",
      "2015-07-02 04:51:58,454 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -24, init = 73, finish = 0\n",
      "2015-07-02 04:51:58,464 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 79.0 (TID 743). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,465 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 79.0 (TID 744, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,466 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 79.0 (TID 743) in 74 ms on localhost (17/26)\n",
      "2015-07-02 04:51:58,467 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 79.0 (TID 744)\n",
      "2015-07-02 04:51:58,470 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,471 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,516 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -7, init = 48, finish = 1\n",
      "2015-07-02 04:51:58,525 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -18, init = 67, finish = 0\n",
      "2015-07-02 04:51:58,532 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 79.0 (TID 744). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,534 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 79.0 (TID 745, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,535 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 79.0 (TID 744) in 70 ms on localhost (18/26)\n",
      "2015-07-02 04:51:58,536 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 79.0 (TID 745)\n",
      "2015-07-02 04:51:58,543 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,543 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:58,587 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 50, finish = 1\n",
      "2015-07-02 04:51:58,598 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -17, init = 64, finish = 0\n",
      "2015-07-02 04:51:58,608 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 79.0 (TID 745). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,609 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 79.0 (TID 746, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,610 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 79.0 (TID 746)\n",
      "2015-07-02 04:51:58,610 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 79.0 (TID 745) in 76 ms on localhost (19/26)\n",
      "2015-07-02 04:51:58,620 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,620 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:58,665 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -16, init = 57, finish = 1\n",
      "2015-07-02 04:51:58,670 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -17, init = 70, finish = 1\n",
      "2015-07-02 04:51:58,677 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 79.0 (TID 746). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,679 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 79.0 (TID 747, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,679 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 79.0 (TID 746) in 70 ms on localhost (20/26)\n",
      "2015-07-02 04:51:58,680 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 79.0 (TID 747)\n",
      "2015-07-02 04:51:58,687 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,688 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,731 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = 0, init = 43, finish = 0\n",
      "2015-07-02 04:51:58,740 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -8, init = 58, finish = 0\n",
      "2015-07-02 04:51:58,746 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 79.0 (TID 747). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,748 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 79.0 (TID 748, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,749 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 79.0 (TID 748)\n",
      "2015-07-02 04:51:58,749 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 79.0 (TID 747) in 70 ms on localhost (21/26)\n",
      "2015-07-02 04:51:58,755 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,756 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,799 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -3, init = 45, finish = 0\n",
      "2015-07-02 04:51:58,813 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -12, init = 59, finish = 1\n",
      "2015-07-02 04:51:58,836 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 79.0 (TID 748). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,837 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 79.0 (TID 749, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,838 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 79.0 (TID 749)\n",
      "2015-07-02 04:51:58,839 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 79.0 (TID 748) in 90 ms on localhost (22/26)\n",
      "2015-07-02 04:51:58,844 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,844 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,886 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -9, init = 52, finish = 0\n",
      "2015-07-02 04:51:58,894 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -6, init = 54, finish = 0\n",
      "2015-07-02 04:51:58,905 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 79.0 (TID 749). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,907 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 79.0 (TID 750, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,908 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 79.0 (TID 750)\n",
      "2015-07-02 04:51:58,908 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 79.0 (TID 749) in 71 ms on localhost (23/26)\n",
      "2015-07-02 04:51:58,914 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 24 non-empty blocks out of 24 blocks\n",
      "2015-07-02 04:51:58,915 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:58,958 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -2, init = 43, finish = 0\n",
      "2015-07-02 04:51:58,975 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -7, init = 54, finish = 0\n",
      "2015-07-02 04:51:58,990 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 79.0 (TID 750). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:58,997 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 79.0 (TID 751, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:58,997 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 79.0 (TID 750) in 90 ms on localhost (24/26)\n",
      "2015-07-02 04:51:58,998 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 79.0 (TID 751)\n",
      "2015-07-02 04:51:59,003 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:51:59,003 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:51:59,033 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 30, boot = -5, init = 9, finish = 26\n",
      "2015-07-02 04:51:59,048 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = 0, init = 41, finish = 1\n",
      "2015-07-02 04:51:59,066 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 79.0 (TID 751). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:59,069 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 79.0 (TID 752, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:51:59,069 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 79.0 (TID 751) in 73 ms on localhost (25/26)\n",
      "2015-07-02 04:51:59,070 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 79.0 (TID 752)\n",
      "2015-07-02 04:51:59,076 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:51:59,077 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:51:59,108 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 33, boot = -6, init = 10, finish = 29\n",
      "2015-07-02 04:51:59,121 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 50, finish = 0\n",
      "2015-07-02 04:51:59,143 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 79.0 (TID 752). 1087 bytes result sent to driver\n",
      "2015-07-02 04:51:59,147 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 80.0 (TID 753, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:51:59,149 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 80.0 (TID 753)\n",
      "2015-07-02 04:51:59,149 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 79.0 (TID 752) in 81 ms on localhost (26/26)\n",
      "2015-07-02 04:51:59,149 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:51:59,150 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 79 (join at <ipython-input-3-4ba038715317>:131) finished in 10.189 s\n",
      "2015-07-02 04:51:59,150 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:51:59,150 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 82, Stage 80)\n",
      "2015-07-02 04:51:59,150 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:51:59,150 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:51:59,154 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:301989888+33554432\n",
      "2015-07-02 04:51:59,154 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:51:59,158 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:51:59,162 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:51:59,171 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:51:59,173 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List(Stage 82)\n",
      "2015-07-02 04:51:59,176 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:51:59,180 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:51:59,182 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:51:59,184 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:52:19,056 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 108427, boot = -1280, init = 1290, finish = 108417\n",
      "2015-07-02 04:52:19,077 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 80.0 (TID 672). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:19,079 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 80.0 (TID 754, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:19,079 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 80.0 (TID 672) in 108457 ms on localhost (4/18)\n",
      "2015-07-02 04:52:19,080 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 80.0 (TID 754)\n",
      "2015-07-02 04:52:19,083 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:335544320+33554432\n",
      "2015-07-02 04:52:20,274 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 66043, boot = -3136, init = 3141, finish = 66038\n",
      "2015-07-02 04:52:20,294 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 80.0 (TID 677). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:20,295 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 80.0 (TID 755, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:20,295 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 80.0 (TID 755)\n",
      "2015-07-02 04:52:20,296 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 80.0 (TID 677) in 66075 ms on localhost (5/18)\n",
      "2015-07-02 04:52:20,299 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:369098752+33554432\n",
      "2015-07-02 04:52:21,376 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70290, boot = -34838, init = 34848, finish = 70280\n",
      "2015-07-02 04:52:21,393 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 80.0 (TID 676). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:21,394 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 80.0 (TID 756, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:21,395 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 80.0 (TID 676) in 70312 ms on localhost (6/18)\n",
      "2015-07-02 04:52:21,397 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 80.0 (TID 756)\n",
      "2015-07-02 04:52:21,403 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:402653184+33554432\n",
      "2015-07-02 04:52:41,049 INFO  [Executor task launch worker-14] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 78990, boot = -7829, init = 7842, finish = 78977\n",
      "2015-07-02 04:52:41,075 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 80.0 (TID 678). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:41,076 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 80.0 (TID 757, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:41,077 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 80.0 (TID 757)\n",
      "2015-07-02 04:52:41,077 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 80.0 (TID 678) in 79023 ms on localhost (7/18)\n",
      "2015-07-02 04:52:41,080 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:436207616+33554432\n",
      "2015-07-02 04:52:50,642 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65764, boot = -40, init = 56, finish = 65748\n",
      "2015-07-02 04:52:50,649 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 80.0 (TID 701). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:50,650 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 80.0 (TID 758, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:50,651 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 80.0 (TID 758)\n",
      "2015-07-02 04:52:50,652 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 80.0 (TID 701) in 65784 ms on localhost (8/18)\n",
      "2015-07-02 04:52:50,658 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:469762048+33554432\n",
      "2015-07-02 04:52:59,234 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70333, boot = -49, init = 55, finish = 70327\n",
      "2015-07-02 04:52:59,247 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 80.0 (TID 726). 2137 bytes result sent to driver\n",
      "2015-07-02 04:52:59,249 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 80.0 (TID 759, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:52:59,250 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 80.0 (TID 726) in 70356 ms on localhost (9/18)\n",
      "2015-07-02 04:52:59,256 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 80.0 (TID 759)\n",
      "2015-07-02 04:52:59,262 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:503316480+33554432\n",
      "2015-07-02 04:53:07,646 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 68492, boot = -7, init = 12, finish = 68487\n",
      "2015-07-02 04:53:07,665 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 80.0 (TID 753). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:07,666 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 80.0 (TID 760, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:53:07,667 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 80.0 (TID 753) in 68520 ms on localhost (10/18)\n",
      "2015-07-02 04:53:07,667 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 80.0 (TID 760)\n",
      "2015-07-02 04:53:07,670 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:536870912+33554432\n",
      "2015-07-02 04:53:22,203 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63120, boot = -19938, init = 19942, finish = 63116\n",
      "2015-07-02 04:53:22,220 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 80.0 (TID 754). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:22,221 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 80.0 (TID 761, localhost, PROCESS_LOCAL, 1298 bytes)\n",
      "2015-07-02 04:53:22,221 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 80.0 (TID 761)\n",
      "2015-07-02 04:53:22,221 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 80.0 (TID 754) in 63143 ms on localhost (11/18)\n",
      "2015-07-02 04:53:22,236 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/BluetoothProximity.csv:570425344+3731893\n",
      "2015-07-02 04:53:29,384 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69085, boot = -1218, init = 1222, finish = 69081\n",
      "2015-07-02 04:53:29,390 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 80.0 (TID 755). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:29,391 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 82.0 (TID 762, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:53:29,392 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 82.0 (TID 762)\n",
      "2015-07-02 04:53:29,393 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 80.0 (TID 755) in 69098 ms on localhost (12/18)\n",
      "2015-07-02 04:53:29,401 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:0+7742886\n",
      "2015-07-02 04:53:30,503 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 1105, boot = -7170, init = 7178, finish = 1097\n",
      "2015-07-02 04:53:30,505 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 82.0 (TID 762). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:30,506 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 82.0 (TID 763, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:53:30,507 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 82.0 (TID 763)\n",
      "2015-07-02 04:53:30,507 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 82.0 (TID 762) in 1116 ms on localhost (1/2)\n",
      "2015-07-02 04:53:30,516 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:7742886+7742887\n",
      "2015-07-02 04:53:31,651 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 1141, boot = -1109, init = 1120, finish = 1130\n",
      "2015-07-02 04:53:31,653 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 82.0 (TID 763). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:31,654 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 85.0 (TID 764, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:53:31,655 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 85.0 (TID 764)\n",
      "2015-07-02 04:53:31,660 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 82.0 (TID 763) in 1150 ms on localhost (2/2)\n",
      "2015-07-02 04:53:31,661 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:53:31,662 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 82 (reduceByKey at <ipython-input-3-4ba038715317>:57) finished in 293.249 s\n",
      "2015-07-02 04:53:31,662 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:53:31,662 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 80)\n",
      "2015-07-02 04:53:31,662 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 83, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:53:31,662 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:53:31,667 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:0+4095974\n",
      "2015-07-02 04:53:31,668 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81, Stage 83)\n",
      "2015-07-02 04:53:31,671 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:53:31,679 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:53:31,680 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:53:31,681 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 83: List()\n",
      "2015-07-02 04:53:31,685 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:53:31,692 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:53:31,695 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:53:31,698 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:53:31,699 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 83 (PairwiseRDD[254] at reduceByKey at <ipython-input-3-4ba038715317>:57), which is now runnable\n",
      "2015-07-02 04:53:31,700 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7280) called with curMem=3332498, maxMem=278302556\n",
      "2015-07-02 04:53:31,705 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_56 stored as values in memory (estimated size 7.1 KB, free 262.2 MB)\n",
      "2015-07-02 04:53:31,706 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4670) called with curMem=3339778, maxMem=278302556\n",
      "2015-07-02 04:53:31,707 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_56_piece0 stored as bytes in memory (estimated size 4.6 KB, free 262.2 MB)\n",
      "2015-07-02 04:53:31,708 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_56_piece0 in memory on localhost:40918 (size: 4.6 KB, free: 265.0 MB)\n",
      "2015-07-02 04:53:31,709 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_56_piece0\n",
      "2015-07-02 04:53:31,709 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 56 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:53:31,710 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 83 (PairwiseRDD[254] at reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:53:31,716 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 83.0 with 2 tasks\n",
      "2015-07-02 04:53:32,241 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 577, boot = -1143, init = 1152, finish = 568\n",
      "2015-07-02 04:53:32,242 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 85.0 (TID 764). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:32,243 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 83.0 (TID 765, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:53:32,243 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 83.0 (TID 765)\n",
      "2015-07-02 04:53:32,244 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 85.0 (TID 764) in 589 ms on localhost (1/2)\n",
      "2015-07-02 04:53:32,252 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:53:32,253 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:53:32,258 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 11, boot = -582, init = 590, finish = 3\n",
      "2015-07-02 04:53:32,264 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 83.0 (TID 765). 1063 bytes result sent to driver\n",
      "2015-07-02 04:53:32,266 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 83.0 (TID 766, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:53:32,266 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 83.0 (TID 765) in 23 ms on localhost (1/2)\n",
      "2015-07-02 04:53:32,267 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 83.0 (TID 766)\n",
      "2015-07-02 04:53:32,269 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:53:32,269 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:53:32,277 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 4, boot = -17, init = 19, finish = 2\n",
      "2015-07-02 04:53:32,278 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 83.0 (TID 766). 1063 bytes result sent to driver\n",
      "2015-07-02 04:53:32,281 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 85.0 (TID 767, localhost, PROCESS_LOCAL, 1286 bytes)\n",
      "2015-07-02 04:53:32,281 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 85.0 (TID 767)\n",
      "2015-07-02 04:53:32,281 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 83.0 (TID 766) in 16 ms on localhost (2/2)\n",
      "2015-07-02 04:53:32,281 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 83.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:53:32,282 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 83 (reduceByKey at <ipython-input-3-4ba038715317>:57) finished in 0.565 s\n",
      "2015-07-02 04:53:32,282 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:53:32,282 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 85, Stage 80)\n",
      "2015-07-02 04:53:32,283 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:53:32,283 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:53:32,288 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81)\n",
      "2015-07-02 04:53:32,292 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:53:32,295 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/SMSLog.csv:4095974+4095975\n",
      "2015-07-02 04:53:32,297 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:53:32,298 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List(Stage 85)\n",
      "2015-07-02 04:53:32,302 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:53:32,306 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:53:32,308 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:53:32,310 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:53:32,780 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71380, boot = -1104, init = 1112, finish = 71372\n",
      "2015-07-02 04:53:32,791 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 80.0 (TID 756). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:32,793 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 88.0 (TID 768, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:53:32,793 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 80.0 (TID 756) in 71399 ms on localhost (13/18)\n",
      "2015-07-02 04:53:32,799 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 88.0 (TID 768)\n",
      "2015-07-02 04:53:32,802 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:0+7742886\n",
      "2015-07-02 04:53:32,879 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 596, boot = -18, init = 34, finish = 580\n",
      "2015-07-02 04:53:32,881 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 85.0 (TID 767). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:32,882 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 88.0 (TID 769, localhost, PROCESS_LOCAL, 1287 bytes)\n",
      "2015-07-02 04:53:32,882 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 88.0 (TID 769)\n",
      "2015-07-02 04:53:32,883 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 85.0 (TID 767) in 602 ms on localhost (2/2)\n",
      "2015-07-02 04:53:32,883 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 85.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:53:32,884 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 85 (reduceByKey at <ipython-input-3-4ba038715317>:57) finished in 294.422 s\n",
      "2015-07-02 04:53:32,884 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:53:32,884 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 88, Stage 80)\n",
      "2015-07-02 04:53:32,884 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 86, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:53:32,885 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:53:32,888 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81)\n",
      "2015-07-02 04:53:32,891 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:53:32,893 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/CallLog.csv:7742886+7742887\n",
      "2015-07-02 04:53:32,898 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 88, Stage 87)\n",
      "2015-07-02 04:53:32,900 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 86: List()\n",
      "2015-07-02 04:53:32,903 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:53:32,906 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:53:32,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:53:32,909 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:53:32,910 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 86 (PairwiseRDD[264] at reduceByKey at <ipython-input-3-4ba038715317>:57), which is now runnable\n",
      "2015-07-02 04:53:32,911 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7280) called with curMem=3344448, maxMem=278302556\n",
      "2015-07-02 04:53:32,912 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_57 stored as values in memory (estimated size 7.1 KB, free 262.2 MB)\n",
      "2015-07-02 04:53:32,918 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 56\n",
      "2015-07-02 04:53:32,919 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4667) called with curMem=3351728, maxMem=278302556\n",
      "2015-07-02 04:53:32,919 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_56\n",
      "2015-07-02 04:53:32,919 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_56 of size 7280 dropped from memory (free 274953441)\n",
      "2015-07-02 04:53:32,920 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_56_piece0\n",
      "2015-07-02 04:53:32,920 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_56_piece0 of size 4670 dropped from memory (free 274958111)\n",
      "2015-07-02 04:53:32,921 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_56_piece0 on localhost:40918 in memory (size: 4.6 KB, free: 265.0 MB)\n",
      "2015-07-02 04:53:32,920 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_57_piece0 stored as bytes in memory (estimated size 4.6 KB, free 262.2 MB)\n",
      "2015-07-02 04:53:32,921 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_57_piece0 in memory on localhost:40918 (size: 4.6 KB, free: 265.0 MB)\n",
      "2015-07-02 04:53:32,922 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_57_piece0\n",
      "2015-07-02 04:53:32,922 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_56_piece0\n",
      "2015-07-02 04:53:32,923 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 57 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:53:32,923 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 56\n",
      "2015-07-02 04:53:32,923 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 86 (PairwiseRDD[264] at reduceByKey at <ipython-input-3-4ba038715317>:57)\n",
      "2015-07-02 04:53:32,924 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 86.0 with 2 tasks\n",
      "2015-07-02 04:53:34,337 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 1537, boot = -510, init = 515, finish = 1532\n",
      "2015-07-02 04:53:34,339 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 88.0 (TID 768). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:34,341 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 86.0 (TID 770, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:53:34,342 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 86.0 (TID 770)\n",
      "2015-07-02 04:53:34,343 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 88.0 (TID 768) in 1550 ms on localhost (1/2)\n",
      "2015-07-02 04:53:34,346 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 1459, boot = -84, init = 96, finish = 1447\n",
      "2015-07-02 04:53:34,352 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 88.0 (TID 769). 1958 bytes result sent to driver\n",
      "2015-07-02 04:53:34,354 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 86.0 (TID 771, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:53:34,352 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:53:34,354 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:53:34,355 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 86.0 (TID 771)\n",
      "2015-07-02 04:53:34,361 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 88.0 (TID 769) in 1478 ms on localhost (2/2)\n",
      "2015-07-02 04:53:34,361 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:53:34,362 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 88 (reduceByKey at <ipython-input-3-4ba038715317>:108) finished in 295.883 s\n",
      "2015-07-02 04:53:34,362 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:53:34,362 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 86, Stage 80)\n",
      "2015-07-02 04:53:34,362 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:53:34,363 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:53:34,362 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:53:34,363 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:53:34,366 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81)\n",
      "2015-07-02 04:53:34,367 INFO  [Executor task launch worker-12] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 8, boot = -1, init = 8, finish = 1\n",
      "2015-07-02 04:53:34,369 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:53:34,373 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 86.0 (TID 771). 1063 bytes result sent to driver\n",
      "2015-07-02 04:53:34,374 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 86.0 (TID 771) in 21 ms on localhost (1/2)\n",
      "2015-07-02 04:53:34,374 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 87)\n",
      "2015-07-02 04:53:34,379 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:53:34,384 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84, Stage 86)\n",
      "2015-07-02 04:53:34,386 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:53:34,387 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:53:34,395 INFO  [Executor task launch worker-11] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -1447, init = 1496, finish = 1\n",
      "2015-07-02 04:53:34,397 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 86.0 (TID 770). 1063 bytes result sent to driver\n",
      "2015-07-02 04:53:34,399 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 86.0 (TID 770) in 58 ms on localhost (2/2)\n",
      "2015-07-02 04:53:34,400 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:53:34,401 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 86 (reduceByKey at <ipython-input-3-4ba038715317>:57) finished in 1.477 s\n",
      "2015-07-02 04:53:34,402 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:53:34,403 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set(Stage 80)\n",
      "2015-07-02 04:53:34,403 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:53:34,403 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:53:34,408 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81)\n",
      "2015-07-02 04:53:34,412 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List(Stage 80)\n",
      "2015-07-02 04:53:34,416 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 87)\n",
      "2015-07-02 04:53:34,419 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:53:34,423 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84)\n",
      "2015-07-02 04:53:34,425 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:53:34,427 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:53:46,796 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65717, boot = -19677, init = 19682, finish = 65712\n",
      "2015-07-02 04:53:46,806 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 80.0 (TID 757). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:46,807 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 80.0 (TID 757) in 65731 ms on localhost (14/18)\n",
      "2015-07-02 04:53:47,877 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 25650, boot = -14558, init = 14576, finish = 25632\n",
      "2015-07-02 04:53:47,881 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 80.0 (TID 761). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:47,882 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 80.0 (TID 761) in 25662 ms on localhost (15/18)\n",
      "2015-07-02 04:53:52,395 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61742, boot = -9573, init = 9582, finish = 61733\n",
      "2015-07-02 04:53:52,403 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 80.0 (TID 758). 2137 bytes result sent to driver\n",
      "2015-07-02 04:53:52,405 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 80.0 (TID 758) in 61754 ms on localhost (16/18)\n",
      "2015-07-02 04:54:00,750 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61492, boot = -8601, init = 8609, finish = 61484\n",
      "2015-07-02 04:54:00,757 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 80.0 (TID 759). 2137 bytes result sent to driver\n",
      "2015-07-02 04:54:00,758 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 80.0 (TID 759) in 61510 ms on localhost (17/18)\n",
      "2015-07-02 04:54:37,475 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 89806, boot = -8408, init = 8411, finish = 89803\n",
      "2015-07-02 04:54:37,482 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 80.0 (TID 760). 2137 bytes result sent to driver\n",
      "2015-07-02 04:54:37,483 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 80.0 (TID 760) in 89817 ms on localhost (18/18)\n",
      "2015-07-02 04:54:37,483 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 80 (reduceByKey at <ipython-input-3-4ba038715317>:46) finished in 359.092 s\n",
      "2015-07-02 04:54:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 81, Stage 89, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:54:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:37,487 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List(Stage 81)\n",
      "2015-07-02 04:54:37,490 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 81: List()\n",
      "2015-07-02 04:54:37,492 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 87)\n",
      "2015-07-02 04:54:37,494 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:54:37,496 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84)\n",
      "2015-07-02 04:54:37,498 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:54:37,499 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:37,499 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 81 (PairwiseRDD[310] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:54:37,499 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13224) called with curMem=3344445, maxMem=278302556\n",
      "2015-07-02 04:54:37,500 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_58 stored as values in memory (estimated size 12.9 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:37,500 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7281) called with curMem=3357669, maxMem=278302556\n",
      "2015-07-02 04:54:37,501 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.1 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:37,501 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_58_piece0 in memory on localhost:40918 (size: 7.1 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:37,501 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_58_piece0\n",
      "2015-07-02 04:54:37,502 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 58 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:37,505 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 44 missing tasks from Stage 81 (PairwiseRDD[310] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:54:37,505 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 81.0 with 44 tasks\n",
      "2015-07-02 04:54:37,506 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 81.0 (TID 772, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,506 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 81.0 (TID 773, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,506 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 81.0 (TID 774, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,507 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 81.0 (TID 775, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,507 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 81.0 (TID 776, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,507 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 81.0 (TID 777, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,507 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 81.0 (TID 778, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,508 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 81.0 (TID 776)\n",
      "2015-07-02 04:54:37,508 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 81.0 (TID 772)\n",
      "2015-07-02 04:54:37,508 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 81.0 (TID 775)\n",
      "2015-07-02 04:54:37,508 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 81.0 (TID 774)\n",
      "2015-07-02 04:54:37,508 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 81.0 (TID 773)\n",
      "2015-07-02 04:54:37,510 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 81.0 (TID 777)\n",
      "2015-07-02 04:54:37,515 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 81.0 (TID 778)\n",
      "2015-07-02 04:54:37,516 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,516 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,524 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,524 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,524 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,524 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,524 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 7, boot = -45113, init = 45120, finish = 0\n",
      "2015-07-02 04:54:37,529 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 12, boot = 3, init = 8, finish = 1\n",
      "2015-07-02 04:54:37,538 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 13, boot = -36758, init = 36770, finish = 1\n",
      "2015-07-02 04:54:37,547 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 17, boot = -49627, init = 49644, finish = 0\n",
      "2015-07-02 04:54:37,550 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,550 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,555 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 39, boot = 25, init = 12, finish = 2\n",
      "2015-07-02 04:54:37,555 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 21, boot = -50704, init = 50725, finish = 0\n",
      "2015-07-02 04:54:37,567 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -63132, init = 63181, finish = 0\n",
      "2015-07-02 04:54:37,570 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -63103, init = 63151, finish = 0\n",
      "2015-07-02 04:54:37,575 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,576 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,575 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,577 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:37,578 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,578 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,583 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 64, boot = 41, init = 22, finish = 1\n",
      "2015-07-02 04:54:37,589 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = 9, init = 31, finish = 1\n",
      "2015-07-02 04:54:37,590 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = 39, init = 28, finish = 0\n",
      "2015-07-02 04:54:37,592 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -34, init = 105, finish = 0\n",
      "2015-07-02 04:54:37,605 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 73, boot = 4, init = 69, finish = 0\n",
      "2015-07-02 04:54:37,612 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 84, boot = -63144, init = 63227, finish = 1\n",
      "2015-07-02 04:54:37,677 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 81.0 (TID 775). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,678 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 81.0 (TID 779, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,678 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 81.0 (TID 779)\n",
      "2015-07-02 04:54:37,679 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 81.0 (TID 775) in 172 ms on localhost (1/44)\n",
      "2015-07-02 04:54:37,679 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 81.0 (TID 772). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,680 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 81.0 (TID 780, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,681 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 81.0 (TID 780)\n",
      "2015-07-02 04:54:37,681 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 81.0 (TID 772) in 176 ms on localhost (2/44)\n",
      "2015-07-02 04:54:37,684 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 81.0 (TID 774). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,684 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 81.0 (TID 781, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,685 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 81.0 (TID 774) in 179 ms on localhost (3/44)\n",
      "2015-07-02 04:54:37,685 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 81.0 (TID 781)\n",
      "2015-07-02 04:54:37,687 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 81.0 (TID 777). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,687 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 81.0 (TID 782, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,688 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 81.0 (TID 782)\n",
      "2015-07-02 04:54:37,688 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 81.0 (TID 777) in 181 ms on localhost (4/44)\n",
      "2015-07-02 04:54:37,689 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 81.0 (TID 773). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,690 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 81.0 (TID 783, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,690 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 81.0 (TID 773) in 184 ms on localhost (5/44)\n",
      "2015-07-02 04:54:37,690 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 81.0 (TID 783)\n",
      "2015-07-02 04:54:37,691 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 81.0 (TID 776). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,691 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 81.0 (TID 778). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,692 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 81.0 (TID 784, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,692 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 81.0 (TID 784)\n",
      "2015-07-02 04:54:37,692 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 81.0 (TID 785, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,693 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 81.0 (TID 785)\n",
      "2015-07-02 04:54:37,693 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 81.0 (TID 776) in 186 ms on localhost (6/44)\n",
      "2015-07-02 04:54:37,693 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 81.0 (TID 778) in 186 ms on localhost (7/44)\n",
      "2015-07-02 04:54:37,696 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,696 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,696 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,696 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,696 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,697 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,697 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,698 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,697 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,698 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,700 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,701 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,701 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,701 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,738 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -146, init = 195, finish = 1\n",
      "2015-07-02 04:54:37,741 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -56, init = 105, finish = 1\n",
      "2015-07-02 04:54:37,742 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -50, init = 93, finish = 1\n",
      "2015-07-02 04:54:37,742 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -115, init = 164, finish = 0\n",
      "2015-07-02 04:54:37,746 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -82, init = 125, finish = 0\n",
      "2015-07-02 04:54:37,765 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -60, init = 107, finish = 2\n",
      "2015-07-02 04:54:37,766 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -144, init = 204, finish = 1\n",
      "2015-07-02 04:54:37,771 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -121, init = 173, finish = 1\n",
      "2015-07-02 04:54:37,775 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 78, boot = -44, init = 122, finish = 0\n",
      "2015-07-02 04:54:37,777 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -109, init = 167, finish = 1\n",
      "2015-07-02 04:54:37,781 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 81.0 (TID 780). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,784 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 81.0 (TID 786, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,786 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 74, boot = -88, init = 161, finish = 1\n",
      "2015-07-02 04:54:37,788 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 81.0 (TID 780) in 105 ms on localhost (8/44)\n",
      "2015-07-02 04:54:37,789 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -157, init = 211, finish = 1\n",
      "2015-07-02 04:54:37,791 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -152, init = 211, finish = 0\n",
      "2015-07-02 04:54:37,793 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 87, boot = -60, init = 147, finish = 0\n",
      "2015-07-02 04:54:37,798 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 81.0 (TID 786)\n",
      "2015-07-02 04:54:37,814 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,815 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,827 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 81.0 (TID 781). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,828 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 81.0 (TID 787, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,828 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 81.0 (TID 787)\n",
      "2015-07-02 04:54:37,828 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 81.0 (TID 781) in 144 ms on localhost (9/44)\n",
      "2015-07-02 04:54:37,835 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 81.0 (TID 785). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,835 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 81.0 (TID 788, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,836 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 81.0 (TID 788)\n",
      "2015-07-02 04:54:37,836 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 81.0 (TID 785) in 144 ms on localhost (10/44)\n",
      "2015-07-02 04:54:37,839 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 81.0 (TID 783). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,839 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 81.0 (TID 789, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,840 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 81.0 (TID 789)\n",
      "2015-07-02 04:54:37,840 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 81.0 (TID 783) in 150 ms on localhost (11/44)\n",
      "2015-07-02 04:54:37,841 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,841 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 81.0 (TID 784). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,841 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,842 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,842 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 81.0 (TID 790, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,841 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,842 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 81.0 (TID 790)\n",
      "2015-07-02 04:54:37,843 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 81.0 (TID 784) in 151 ms on localhost (12/44)\n",
      "2015-07-02 04:54:37,844 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,844 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,846 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 81.0 (TID 782). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,846 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 81.0 (TID 791, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,847 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 81.0 (TID 791)\n",
      "2015-07-02 04:54:37,847 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 81.0 (TID 782) in 160 ms on localhost (13/44)\n",
      "2015-07-02 04:54:37,847 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 81.0 (TID 779). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,848 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,848 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,848 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 81.0 (TID 792, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,848 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 81.0 (TID 792)\n",
      "2015-07-02 04:54:37,848 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 81.0 (TID 779) in 170 ms on localhost (14/44)\n",
      "2015-07-02 04:54:37,850 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,850 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,852 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,852 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,855 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -44, init = 90, finish = 0\n",
      "2015-07-02 04:54:37,873 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -29, init = 83, finish = 1\n",
      "2015-07-02 04:54:37,879 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 81.0 (TID 786). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,879 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 81.0 (TID 793, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,880 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 81.0 (TID 793)\n",
      "2015-07-02 04:54:37,880 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 81.0 (TID 786) in 95 ms on localhost (15/44)\n",
      "2015-07-02 04:54:37,882 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -68, init = 115, finish = 0\n",
      "2015-07-02 04:54:37,882 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -54, init = 96, finish = 1\n",
      "2015-07-02 04:54:37,884 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,885 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,890 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -73, init = 118, finish = 1\n",
      "2015-07-02 04:54:37,895 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -56, init = 99, finish = 1\n",
      "2015-07-02 04:54:37,893 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -77, init = 118, finish = 1\n",
      "2015-07-02 04:54:37,891 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -65, init = 106, finish = 1\n",
      "2015-07-02 04:54:37,891 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -55, init = 99, finish = 1\n",
      "2015-07-02 04:54:37,897 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -71, init = 124, finish = 1\n",
      "2015-07-02 04:54:37,906 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -63, init = 112, finish = 1\n",
      "2015-07-02 04:54:37,909 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 81.0 (TID 788). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,904 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -82, init = 134, finish = 0\n",
      "2015-07-02 04:54:37,910 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 81.0 (TID 794, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,911 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 81.0 (TID 794)\n",
      "2015-07-02 04:54:37,911 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 81.0 (TID 788) in 76 ms on localhost (16/44)\n",
      "2015-07-02 04:54:37,914 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -60, init = 117, finish = 1\n",
      "2015-07-02 04:54:37,915 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -59, init = 106, finish = 0\n",
      "2015-07-02 04:54:37,924 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -8, init = 50, finish = 1\n",
      "2015-07-02 04:54:37,930 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -11, init = 54, finish = 1\n",
      "2015-07-02 04:54:37,938 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,938 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,950 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 81.0 (TID 787). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,950 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 81.0 (TID 795, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,951 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 81.0 (TID 795)\n",
      "2015-07-02 04:54:37,951 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 81.0 (TID 787) in 123 ms on localhost (17/44)\n",
      "2015-07-02 04:54:37,965 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,965 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,971 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 81.0 (TID 790). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,972 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 81.0 (TID 796, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,973 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 81.0 (TID 796)\n",
      "2015-07-02 04:54:37,973 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 81.0 (TID 790) in 131 ms on localhost (18/44)\n",
      "2015-07-02 04:54:37,972 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 81.0 (TID 789). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,974 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 81.0 (TID 791). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,974 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 81.0 (TID 793). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,974 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 81.0 (TID 792). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,974 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 81.0 (TID 797, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,974 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 81.0 (TID 797)\n",
      "2015-07-02 04:54:37,974 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 81.0 (TID 789) in 135 ms on localhost (19/44)\n",
      "2015-07-02 04:54:37,975 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 81.0 (TID 798, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,975 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 81.0 (TID 798)\n",
      "2015-07-02 04:54:37,976 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 81.0 (TID 799, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,976 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 81.0 (TID 799)\n",
      "2015-07-02 04:54:37,976 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 81.0 (TID 800, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,976 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 81.0 (TID 800)\n",
      "2015-07-02 04:54:37,977 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 81.0 (TID 791) in 130 ms on localhost (20/44)\n",
      "2015-07-02 04:54:37,977 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 81.0 (TID 792) in 129 ms on localhost (21/44)\n",
      "2015-07-02 04:54:37,977 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,978 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:37,978 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 81.0 (TID 793) in 99 ms on localhost (22/44)\n",
      "2015-07-02 04:54:37,979 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -6, init = 57, finish = 0\n",
      "2015-07-02 04:54:37,982 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:37,982 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,983 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:37,983 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,984 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 26 non-empty blocks out of 26 blocks\n",
      "2015-07-02 04:54:37,984 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,984 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -3, init = 70, finish = 0\n",
      "2015-07-02 04:54:37,995 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:37,995 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:37,995 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 81.0 (TID 794). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:37,996 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 81.0 (TID 801, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:37,997 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 81.0 (TID 801)\n",
      "2015-07-02 04:54:37,997 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 81.0 (TID 794) in 86 ms on localhost (23/44)\n",
      "2015-07-02 04:54:38,000 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,001 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,006 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -64, init = 110, finish = 1\n",
      "2015-07-02 04:54:38,012 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -56, init = 111, finish = 1\n",
      "2015-07-02 04:54:38,019 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -60, init = 103, finish = 0\n",
      "2015-07-02 04:54:38,021 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 81.0 (TID 795). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,022 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 81.0 (TID 802, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,023 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 81.0 (TID 802)\n",
      "2015-07-02 04:54:38,023 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 81.0 (TID 795) in 73 ms on localhost (24/44)\n",
      "2015-07-02 04:54:38,026 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -46, init = 92, finish = 0\n",
      "2015-07-02 04:54:38,034 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -55, init = 100, finish = 0\n",
      "2015-07-02 04:54:38,035 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -56, init = 107, finish = 0\n",
      "2015-07-02 04:54:38,050 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,050 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,063 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 81.0 (TID 796). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,065 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 81.0 (TID 803, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,066 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 81.0 (TID 803)\n",
      "2015-07-02 04:54:38,067 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 81.0 (TID 796) in 95 ms on localhost (25/44)\n",
      "2015-07-02 04:54:38,073 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 81.0 (TID 797). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,074 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,075 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 81.0 (TID 804, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,076 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 81.0 (TID 804)\n",
      "2015-07-02 04:54:38,076 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 81.0 (TID 797) in 101 ms on localhost (26/44)\n",
      "2015-07-02 04:54:38,076 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 97, boot = -58, init = 64, finish = 91\n",
      "2015-07-02 04:54:38,076 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:38,082 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 82, boot = -5, init = 9, finish = 78\n",
      "2015-07-02 04:54:38,083 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 104, boot = -58, init = 64, finish = 98\n",
      "2015-07-02 04:54:38,086 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 100, boot = -64, init = 163, finish = 1\n",
      "2015-07-02 04:54:38,094 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 89, boot = -8, init = 97, finish = 0\n",
      "2015-07-02 04:54:38,095 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 110, boot = -66, init = 78, finish = 98\n",
      "2015-07-02 04:54:38,095 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 109, boot = -57, init = 165, finish = 1\n",
      "2015-07-02 04:54:38,104 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 117, boot = -46, init = 163, finish = 0\n",
      "2015-07-02 04:54:38,112 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,113 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,134 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 81.0 (TID 799). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,135 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 81.0 (TID 805, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,135 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 81.0 (TID 805)\n",
      "2015-07-02 04:54:38,135 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 81.0 (TID 799) in 160 ms on localhost (27/44)\n",
      "2015-07-02 04:54:38,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,147 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 110, boot = -19, init = 36, finish = 93\n",
      "2015-07-02 04:54:38,151 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 81.0 (TID 800). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,153 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 81.0 (TID 806, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,154 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 81.0 (TID 800) in 178 ms on localhost (28/44)\n",
      "2015-07-02 04:54:38,155 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 81.0 (TID 806)\n",
      "2015-07-02 04:54:38,152 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 81.0 (TID 798). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,157 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 81.0 (TID 807, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,158 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 81.0 (TID 798) in 182 ms on localhost (29/44)\n",
      "2015-07-02 04:54:38,158 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 81.0 (TID 807)\n",
      "2015-07-02 04:54:38,161 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 124, boot = -2, init = 126, finish = 0\n",
      "2015-07-02 04:54:38,166 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,166 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,166 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 84, boot = -38, init = 72, finish = 50\n",
      "2015-07-02 04:54:38,174 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 88, boot = -35, init = 123, finish = 0\n",
      "2015-07-02 04:54:38,177 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,177 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 81.0 (TID 801). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,178 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,178 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 81.0 (TID 808, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,179 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 81.0 (TID 801) in 183 ms on localhost (30/44)\n",
      "2015-07-02 04:54:38,180 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 81.0 (TID 808)\n",
      "2015-07-02 04:54:38,186 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 81.0 (TID 802). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,187 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 81.0 (TID 809, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,188 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 81.0 (TID 809)\n",
      "2015-07-02 04:54:38,188 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 81.0 (TID 802) in 166 ms on localhost (31/44)\n",
      "2015-07-02 04:54:38,190 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,190 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,191 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 122, boot = -26, init = 36, finish = 112\n",
      "2015-07-02 04:54:38,201 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 127, boot = -35, init = 162, finish = 0\n",
      "2015-07-02 04:54:38,209 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 81.0 (TID 804). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,209 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 81.0 (TID 810, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,210 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 81.0 (TID 810)\n",
      "2015-07-02 04:54:38,210 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 81.0 (TID 804) in 136 ms on localhost (32/44)\n",
      "2015-07-02 04:54:38,215 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,216 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,220 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 77, boot = -29, init = 34, finish = 72\n",
      "2015-07-02 04:54:38,225 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -40, init = 50, finish = 57\n",
      "2015-07-02 04:54:38,226 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 84, boot = -48, init = 132, finish = 0\n",
      "2015-07-02 04:54:38,236 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,236 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,237 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 81.0 (TID 803). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,238 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 81.0 (TID 811, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,239 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 81.0 (TID 803) in 173 ms on localhost (33/44)\n",
      "2015-07-02 04:54:38,239 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 81.0 (TID 811)\n",
      "2015-07-02 04:54:38,241 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69, boot = -46, init = 115, finish = 0\n",
      "2015-07-02 04:54:38,253 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 81.0 (TID 806). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,255 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 81.0 (TID 812, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,256 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 81.0 (TID 806) in 104 ms on localhost (34/44)\n",
      "2015-07-02 04:54:38,257 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 81.0 (TID 812)\n",
      "2015-07-02 04:54:38,257 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 81.0 (TID 805). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,257 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,258 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,258 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 81.0 (TID 813, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,258 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 81.0 (TID 813)\n",
      "2015-07-02 04:54:38,259 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 81.0 (TID 805) in 124 ms on localhost (35/44)\n",
      "2015-07-02 04:54:38,265 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,266 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,276 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,277 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,295 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 131, boot = -55, init = 72, finish = 114\n",
      "2015-07-02 04:54:38,302 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 89, boot = -49, init = 54, finish = 84\n",
      "2015-07-02 04:54:38,302 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 136, boot = -62, init = 198, finish = 0\n",
      "2015-07-02 04:54:38,304 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 114, boot = -81, init = 84, finish = 111\n",
      "2015-07-02 04:54:38,320 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 123, boot = -84, init = 207, finish = 0\n",
      "2015-07-02 04:54:38,321 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 113, boot = -31, init = 144, finish = 0\n",
      "2015-07-02 04:54:38,323 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 81.0 (TID 807). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,325 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 81.0 (TID 814, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,325 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 81.0 (TID 807) in 169 ms on localhost (36/44)\n",
      "2015-07-02 04:54:38,329 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 81.0 (TID 814)\n",
      "2015-07-02 04:54:38,332 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 90, boot = -29, init = 48, finish = 71\n",
      "2015-07-02 04:54:38,339 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 92, boot = -40, init = 132, finish = 0\n",
      "2015-07-02 04:54:38,349 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,349 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,351 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 81.0 (TID 808). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,353 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 81.0 (TID 815, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,353 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 81.0 (TID 815)\n",
      "2015-07-02 04:54:38,355 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 81.0 (TID 809). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,357 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 81.0 (TID 808) in 179 ms on localhost (37/44)\n",
      "2015-07-02 04:54:38,358 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 81.0 (TID 809) in 170 ms on localhost (38/44)\n",
      "2015-07-02 04:54:38,366 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 18 non-empty blocks out of 18 blocks\n",
      "2015-07-02 04:54:38,366 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,371 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 155, boot = -31, init = 54, finish = 132\n",
      "2015-07-02 04:54:38,372 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 81.0 (TID 811). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,373 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 81.0 (TID 811) in 136 ms on localhost (39/44)\n",
      "2015-07-02 04:54:38,399 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 131, boot = -7, init = 23, finish = 115\n",
      "2015-07-02 04:54:38,402 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 160, boot = -36, init = 196, finish = 0\n",
      "2015-07-02 04:54:38,404 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 140, boot = 2, init = 138, finish = 0\n",
      "2015-07-02 04:54:38,419 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 81.0 (TID 810). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,421 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 81.0 (TID 810) in 211 ms on localhost (40/44)\n",
      "2015-07-02 04:54:38,422 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 81.0 (TID 813). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,423 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 81.0 (TID 813) in 165 ms on localhost (41/44)\n",
      "2015-07-02 04:54:38,423 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 162, boot = -26, init = 33, finish = 155\n",
      "2015-07-02 04:54:38,429 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69, boot = -29, init = 38, finish = 60\n",
      "2015-07-02 04:54:38,430 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 165, boot = -19, init = 183, finish = 1\n",
      "2015-07-02 04:54:38,437 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 76, boot = -37, init = 113, finish = 0\n",
      "2015-07-02 04:54:38,438 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 99, boot = -15, init = 28, finish = 86\n",
      "2015-07-02 04:54:38,441 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 81.0 (TID 812). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,443 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 81.0 (TID 812) in 189 ms on localhost (42/44)\n",
      "2015-07-02 04:54:38,445 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 106, boot = -4, init = 110, finish = 0\n",
      "2015-07-02 04:54:38,447 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 81.0 (TID 815). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,448 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 81.0 (TID 815) in 96 ms on localhost (43/44)\n",
      "2015-07-02 04:54:38,452 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 81.0 (TID 814). 1105 bytes result sent to driver\n",
      "2015-07-02 04:54:38,452 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 81.0 (TID 814) in 128 ms on localhost (44/44)\n",
      "2015-07-02 04:54:38,453 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 81.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 81 (join at <ipython-input-3-4ba038715317>:131) finished in 0.948 s\n",
      "2015-07-02 04:54:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 84, Stage 89, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:54:38,453 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:38,456 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 84: List()\n",
      "2015-07-02 04:54:38,458 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 87)\n",
      "2015-07-02 04:54:38,460 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:54:38,462 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List(Stage 84)\n",
      "2015-07-02 04:54:38,464 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:54:38,465 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:38,465 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 84 (PairwiseRDD[317] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:54:38,466 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13144) called with curMem=3364950, maxMem=278302556\n",
      "2015-07-02 04:54:38,466 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_59 stored as values in memory (estimated size 12.8 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:38,466 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7219) called with curMem=3378094, maxMem=278302556\n",
      "2015-07-02 04:54:38,467 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_59_piece0 stored as bytes in memory (estimated size 7.0 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:38,467 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_59_piece0 in memory on localhost:40918 (size: 7.0 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:38,467 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_59_piece0\n",
      "2015-07-02 04:54:38,467 INFO  [sparkDriver-akka.actor.default-dispatcher-18] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 59 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:38,471 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 46 missing tasks from Stage 84 (PairwiseRDD[317] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:54:38,471 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 84.0 with 46 tasks\n",
      "2015-07-02 04:54:38,471 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 84.0 (TID 816, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,472 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 84.0 (TID 817, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,472 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 84.0 (TID 818, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,472 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 84.0 (TID 819, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,472 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 84.0 (TID 820, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,473 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 84.0 (TID 821, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,473 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 84.0 (TID 822, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 84.0 (TID 816)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 84.0 (TID 822)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 84.0 (TID 818)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 84.0 (TID 821)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 84.0 (TID 819)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 84.0 (TID 820)\n",
      "2015-07-02 04:54:38,473 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 84.0 (TID 817)\n",
      "2015-07-02 04:54:38,478 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,478 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,481 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,481 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,484 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,484 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,485 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,486 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,486 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,486 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,488 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,489 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,518 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -69, init = 111, finish = 0\n",
      "2015-07-02 04:54:38,520 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -34, init = 75, finish = 0\n",
      "2015-07-02 04:54:38,523 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -129, init = 173, finish = 0\n",
      "2015-07-02 04:54:38,530 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -32, init = 76, finish = 1\n",
      "2015-07-02 04:54:38,531 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -90, init = 140, finish = 0\n",
      "2015-07-02 04:54:38,538 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -31, init = 82, finish = 0\n",
      "2015-07-02 04:54:38,539 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -126, init = 170, finish = 1\n",
      "2015-07-02 04:54:38,542 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 64, boot = -152, init = 215, finish = 1\n",
      "2015-07-02 04:54:38,544 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 68, boot = -43, init = 110, finish = 1\n",
      "2015-07-02 04:54:38,551 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -155, init = 211, finish = 1\n",
      "2015-07-02 04:54:38,555 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -31, init = 77, finish = 1\n",
      "2015-07-02 04:54:38,560 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70, boot = -86, init = 156, finish = 0\n",
      "2015-07-02 04:54:38,563 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -64, init = 119, finish = 0\n",
      "2015-07-02 04:54:38,565 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 80, boot = -40, init = 119, finish = 1\n",
      "2015-07-02 04:54:38,578 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 84.0 (TID 822). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,579 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 84.0 (TID 823, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,579 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 84.0 (TID 823)\n",
      "2015-07-02 04:54:38,579 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 84.0 (TID 822) in 106 ms on localhost (1/46)\n",
      "2015-07-02 04:54:38,594 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,594 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,611 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 84.0 (TID 820). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,612 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 84.0 (TID 824, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,613 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 84.0 (TID 824)\n",
      "2015-07-02 04:54:38,613 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 84.0 (TID 820) in 141 ms on localhost (2/46)\n",
      "2015-07-02 04:54:38,617 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,618 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,618 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 84.0 (TID 821). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,618 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 84.0 (TID 819). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,618 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 84.0 (TID 816). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,618 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 84.0 (TID 818). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,619 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 84.0 (TID 817). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,619 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 84.0 (TID 825, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,619 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 84.0 (TID 825)\n",
      "2015-07-02 04:54:38,619 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 84.0 (TID 821) in 146 ms on localhost (3/46)\n",
      "2015-07-02 04:54:38,620 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 84.0 (TID 819) in 147 ms on localhost (4/46)\n",
      "2015-07-02 04:54:38,620 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 84.0 (TID 826, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,621 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 84.0 (TID 826)\n",
      "2015-07-02 04:54:38,621 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 84.0 (TID 827, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,621 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 84.0 (TID 827)\n",
      "2015-07-02 04:54:38,622 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 84.0 (TID 828, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,622 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 84.0 (TID 818) in 150 ms on localhost (5/46)\n",
      "2015-07-02 04:54:38,622 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 84.0 (TID 828)\n",
      "2015-07-02 04:54:38,623 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 84.0 (TID 816) in 152 ms on localhost (6/46)\n",
      "2015-07-02 04:54:38,623 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 84.0 (TID 829, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,624 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 84.0 (TID 829)\n",
      "2015-07-02 04:54:38,624 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 84.0 (TID 817) in 152 ms on localhost (7/46)\n",
      "2015-07-02 04:54:38,625 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,625 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,626 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,626 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,628 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,628 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,629 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,630 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,631 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,631 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,634 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -36, init = 82, finish = 0\n",
      "2015-07-02 04:54:38,639 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -54, init = 109, finish = 0\n",
      "2015-07-02 04:54:38,645 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 84.0 (TID 823). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,645 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 84.0 (TID 830, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,646 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 84.0 (TID 830)\n",
      "2015-07-02 04:54:38,646 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 84.0 (TID 823) in 67 ms on localhost (8/46)\n",
      "2015-07-02 04:54:38,648 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,648 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,658 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -69, init = 112, finish = 0\n",
      "2015-07-02 04:54:38,662 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -68, init = 112, finish = 0\n",
      "2015-07-02 04:54:38,666 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -64, init = 108, finish = 1\n",
      "2015-07-02 04:54:38,667 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -83, init = 129, finish = 1\n",
      "2015-07-02 04:54:38,667 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -75, init = 117, finish = 0\n",
      "2015-07-02 04:54:38,669 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 84.0 (TID 824). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,671 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 84.0 (TID 831, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,672 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -62, init = 107, finish = 0\n",
      "2015-07-02 04:54:38,673 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -89, init = 136, finish = 0\n",
      "2015-07-02 04:54:38,674 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 84.0 (TID 831)\n",
      "2015-07-02 04:54:38,674 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -62, init = 108, finish = 0\n",
      "2015-07-02 04:54:38,683 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -64, init = 109, finish = 0\n",
      "2015-07-02 04:54:38,694 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 84.0 (TID 829). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,697 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -82, init = 144, finish = 1\n",
      "2015-07-02 04:54:38,701 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,701 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,701 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -85, init = 146, finish = 0\n",
      "2015-07-02 04:54:38,698 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -5, init = 51, finish = 0\n",
      "2015-07-02 04:54:38,697 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 84.0 (TID 824) in 85 ms on localhost (9/46)\n",
      "2015-07-02 04:54:38,707 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 84.0 (TID 829) in 83 ms on localhost (10/46)\n",
      "2015-07-02 04:54:38,702 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -60, init = 107, finish = 1\n",
      "2015-07-02 04:54:38,708 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 84.0 (TID 832, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,710 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 84.0 (TID 832)\n",
      "2015-07-02 04:54:38,713 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 84.0 (TID 825). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,713 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -6, init = 66, finish = 0\n",
      "2015-07-02 04:54:38,714 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 84.0 (TID 833, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,714 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 84.0 (TID 833)\n",
      "2015-07-02 04:54:38,715 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 84.0 (TID 825) in 95 ms on localhost (11/46)\n",
      "2015-07-02 04:54:38,727 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,727 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,736 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 84.0 (TID 827). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,737 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 84.0 (TID 834, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,737 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 84.0 (TID 827) in 116 ms on localhost (12/46)\n",
      "2015-07-02 04:54:38,737 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 84.0 (TID 834)\n",
      "2015-07-02 04:54:38,741 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -25, init = 74, finish = 0\n",
      "2015-07-02 04:54:38,746 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65, boot = -11, init = 76, finish = 0\n",
      "2015-07-02 04:54:38,751 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,751 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,760 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 84.0 (TID 828). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,760 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 84.0 (TID 826). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,761 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 84.0 (TID 835, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,761 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 84.0 (TID 835)\n",
      "2015-07-02 04:54:38,761 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 84.0 (TID 830). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,761 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 84.0 (TID 836, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,762 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 84.0 (TID 826) in 142 ms on localhost (13/46)\n",
      "2015-07-02 04:54:38,762 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 84.0 (TID 828) in 141 ms on localhost (14/46)\n",
      "2015-07-02 04:54:38,762 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 84.0 (TID 836)\n",
      "2015-07-02 04:54:38,763 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 84.0 (TID 837, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,763 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 84.0 (TID 837)\n",
      "2015-07-02 04:54:38,763 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 84.0 (TID 830) in 118 ms on localhost (15/46)\n",
      "2015-07-02 04:54:38,766 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 84.0 (TID 831). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,766 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 84.0 (TID 838, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,767 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,767 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,767 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 84.0 (TID 831) in 97 ms on localhost (16/46)\n",
      "2015-07-02 04:54:38,767 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 84.0 (TID 838)\n",
      "2015-07-02 04:54:38,768 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -27, init = 74, finish = 1\n",
      "2015-07-02 04:54:38,768 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,769 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,769 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -33, init = 88, finish = 0\n",
      "2015-07-02 04:54:38,771 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 84.0 (TID 832). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,772 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 84.0 (TID 839, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,771 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,773 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 84.0 (TID 832) in 65 ms on localhost (17/46)\n",
      "2015-07-02 04:54:38,773 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -24, init = 73, finish = 0\n",
      "2015-07-02 04:54:38,773 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:38,774 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 84.0 (TID 839)\n",
      "2015-07-02 04:54:38,779 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -12, init = 70, finish = 0\n",
      "2015-07-02 04:54:38,780 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,780 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,784 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,784 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,789 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 84.0 (TID 833). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,790 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 84.0 (TID 840, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,790 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 84.0 (TID 840)\n",
      "2015-07-02 04:54:38,790 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 84.0 (TID 833) in 76 ms on localhost (18/46)\n",
      "2015-07-02 04:54:38,792 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -32, init = 78, finish = 1\n",
      "2015-07-02 04:54:38,793 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,793 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,797 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -57, init = 111, finish = 0\n",
      "2015-07-02 04:54:38,802 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 84.0 (TID 834). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,803 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 84.0 (TID 841, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,803 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 84.0 (TID 841)\n",
      "2015-07-02 04:54:38,803 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 84.0 (TID 834) in 66 ms on localhost (19/46)\n",
      "2015-07-02 04:54:38,806 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,806 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,807 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -58, init = 99, finish = 0\n",
      "2015-07-02 04:54:38,810 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -59, init = 103, finish = 1\n",
      "2015-07-02 04:54:38,812 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -55, init = 97, finish = 0\n",
      "2015-07-02 04:54:38,817 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -19, init = 60, finish = 1\n",
      "2015-07-02 04:54:38,818 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -63, init = 111, finish = 1\n",
      "2015-07-02 04:54:38,837 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 46, finish = 1\n",
      "2015-07-02 04:54:38,837 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = 5, init = 37, finish = 1\n",
      "2015-07-02 04:54:38,839 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -50, init = 99, finish = 0\n",
      "2015-07-02 04:54:38,841 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -20, init = 73, finish = 0\n",
      "2015-07-02 04:54:38,843 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 78, boot = -62, init = 140, finish = 0\n",
      "2015-07-02 04:54:38,845 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -5, init = 52, finish = 0\n",
      "2015-07-02 04:54:38,845 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 84.0 (TID 836). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,846 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 84.0 (TID 837). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,847 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = 5, init = 58, finish = 0\n",
      "2015-07-02 04:54:38,846 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 84.0 (TID 842, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,848 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 84.0 (TID 843, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,849 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 84.0 (TID 843)\n",
      "2015-07-02 04:54:38,849 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 84.0 (TID 836) in 88 ms on localhost (20/46)\n",
      "2015-07-02 04:54:38,849 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 84.0 (TID 837) in 86 ms on localhost (21/46)\n",
      "2015-07-02 04:54:38,851 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 84.0 (TID 842)\n",
      "2015-07-02 04:54:38,854 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -3, init = 45, finish = 1\n",
      "2015-07-02 04:54:38,860 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -5, init = 56, finish = 0\n",
      "2015-07-02 04:54:38,868 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,868 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,872 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,872 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,889 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 84.0 (TID 835). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,889 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 84.0 (TID 844, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,890 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 84.0 (TID 844)\n",
      "2015-07-02 04:54:38,890 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 84.0 (TID 835) in 129 ms on localhost (22/46)\n",
      "2015-07-02 04:54:38,898 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 84.0 (TID 838). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,898 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 84.0 (TID 840). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,898 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 84.0 (TID 845, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,899 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 84.0 (TID 838) in 133 ms on localhost (23/46)\n",
      "2015-07-02 04:54:38,899 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 84.0 (TID 845)\n",
      "2015-07-02 04:54:38,899 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 84.0 (TID 840) in 110 ms on localhost (24/46)\n",
      "2015-07-02 04:54:38,899 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 84.0 (TID 846, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,900 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 84.0 (TID 846)\n",
      "2015-07-02 04:54:38,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,902 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,905 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,906 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,905 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 84.0 (TID 839). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,906 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 84.0 (TID 847, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,906 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 84.0 (TID 847)\n",
      "2015-07-02 04:54:38,907 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 84.0 (TID 839) in 134 ms on localhost (25/46)\n",
      "2015-07-02 04:54:38,908 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,908 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,909 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 84.0 (TID 841). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,909 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -36, init = 85, finish = 1\n",
      "2015-07-02 04:54:38,909 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 84.0 (TID 848, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,910 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 84.0 (TID 841) in 107 ms on localhost (26/46)\n",
      "2015-07-02 04:54:38,910 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 84.0 (TID 848)\n",
      "2015-07-02 04:54:38,914 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -43, init = 92, finish = 0\n",
      "2015-07-02 04:54:38,914 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = 2, init = 54, finish = 0\n",
      "2015-07-02 04:54:38,915 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -14, init = 77, finish = 0\n",
      "2015-07-02 04:54:38,920 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,920 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,930 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,931 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:38,939 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 84.0 (TID 843). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,940 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 84.0 (TID 849, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,939 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 84.0 (TID 842). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,940 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 84.0 (TID 843) in 92 ms on localhost (27/46)\n",
      "2015-07-02 04:54:38,940 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 84.0 (TID 849)\n",
      "2015-07-02 04:54:38,941 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 84.0 (TID 850, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,942 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 84.0 (TID 850)\n",
      "2015-07-02 04:54:38,942 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 84.0 (TID 842) in 95 ms on localhost (28/46)\n",
      "2015-07-02 04:54:38,943 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -70, init = 117, finish = 0\n",
      "2015-07-02 04:54:38,946 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,946 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,946 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -49, init = 91, finish = 1\n",
      "2015-07-02 04:54:38,948 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -61, init = 113, finish = 0\n",
      "2015-07-02 04:54:38,949 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -52, init = 97, finish = 0\n",
      "2015-07-02 04:54:38,956 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -70, init = 117, finish = 0\n",
      "2015-07-02 04:54:38,958 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -68, init = 117, finish = 0\n",
      "2015-07-02 04:54:38,961 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -59, init = 110, finish = 1\n",
      "2015-07-02 04:54:38,963 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -73, init = 127, finish = 0\n",
      "2015-07-02 04:54:38,966 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 84.0 (TID 844). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:38,967 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 84.0 (TID 851, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:38,968 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 84.0 (TID 851)\n",
      "2015-07-02 04:54:38,968 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 84.0 (TID 844) in 79 ms on localhost (29/46)\n",
      "2015-07-02 04:54:38,970 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,971 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,971 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -63, init = 106, finish = 1\n",
      "2015-07-02 04:54:38,977 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -52, init = 112, finish = 0\n",
      "2015-07-02 04:54:38,985 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:38,986 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:38,986 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -25, init = 66, finish = 1\n",
      "2015-07-02 04:54:38,992 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -17, init = 61, finish = 0\n",
      "2015-07-02 04:54:39,002 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 84.0 (TID 846). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,002 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 84.0 (TID 852, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,003 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 84.0 (TID 852)\n",
      "2015-07-02 04:54:39,003 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 84.0 (TID 846) in 104 ms on localhost (30/46)\n",
      "2015-07-02 04:54:39,011 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 84.0 (TID 845). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,011 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -32, init = 87, finish = 1\n",
      "2015-07-02 04:54:39,011 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 84.0 (TID 853, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,012 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 84.0 (TID 845) in 114 ms on localhost (31/46)\n",
      "2015-07-02 04:54:39,012 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 84.0 (TID 853)\n",
      "2015-07-02 04:54:39,015 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,015 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,017 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69, boot = -17, init = 86, finish = 0\n",
      "2015-07-02 04:54:39,022 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 84.0 (TID 847). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,022 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 84.0 (TID 854, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,023 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 84.0 (TID 854)\n",
      "2015-07-02 04:54:39,022 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 84.0 (TID 848). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,023 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 84.0 (TID 847) in 117 ms on localhost (32/46)\n",
      "2015-07-02 04:54:39,024 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 84.0 (TID 855, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,025 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 84.0 (TID 848) in 116 ms on localhost (33/46)\n",
      "2015-07-02 04:54:39,025 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 84.0 (TID 855)\n",
      "2015-07-02 04:54:39,026 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -13, init = 59, finish = 0\n",
      "2015-07-02 04:54:39,027 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = 1, init = 56, finish = 0\n",
      "2015-07-02 04:54:39,031 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,031 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,043 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 84.0 (TID 850). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,044 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 84.0 (TID 856, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,044 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 84.0 (TID 856)\n",
      "2015-07-02 04:54:39,044 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 84.0 (TID 850) in 103 ms on localhost (34/46)\n",
      "2015-07-02 04:54:39,053 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,053 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,055 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -46, init = 92, finish = 1\n",
      "2015-07-02 04:54:39,058 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 84.0 (TID 849). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,058 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 84.0 (TID 851). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,059 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 84.0 (TID 857, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,059 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 84.0 (TID 857)\n",
      "2015-07-02 04:54:39,059 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 84.0 (TID 849) in 119 ms on localhost (35/46)\n",
      "2015-07-02 04:54:39,060 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 84.0 (TID 858, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,061 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 84.0 (TID 858)\n",
      "2015-07-02 04:54:39,061 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 84.0 (TID 851) in 94 ms on localhost (36/46)\n",
      "2015-07-02 04:54:39,061 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -42, init = 94, finish = 0\n",
      "2015-07-02 04:54:39,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,068 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,069 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,073 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -49, init = 105, finish = 0\n",
      "2015-07-02 04:54:39,074 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 84.0 (TID 852). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,075 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 84.0 (TID 859, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,076 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 84.0 (TID 859)\n",
      "2015-07-02 04:54:39,076 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 84.0 (TID 852) in 74 ms on localhost (37/46)\n",
      "2015-07-02 04:54:39,078 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -56, init = 114, finish = 1\n",
      "2015-07-02 04:54:39,080 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 44 blocks\n",
      "2015-07-02 04:54:39,080 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,083 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -52, init = 99, finish = 0\n",
      "2015-07-02 04:54:39,083 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -55, init = 106, finish = 1\n",
      "2015-07-02 04:54:39,085 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -46, init = 104, finish = 0\n",
      "2015-07-02 04:54:39,092 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -52, init = 111, finish = 0\n",
      "2015-07-02 04:54:39,094 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 84.0 (TID 853). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,095 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 84.0 (TID 860, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,095 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -54, init = 99, finish = 0\n",
      "2015-07-02 04:54:39,096 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 84.0 (TID 853) in 84 ms on localhost (38/46)\n",
      "2015-07-02 04:54:39,097 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 84.0 (TID 860)\n",
      "2015-07-02 04:54:39,100 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -51, init = 100, finish = 1\n",
      "2015-07-02 04:54:39,105 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:39,106 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,107 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 84.0 (TID 854). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,108 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -29, init = 73, finish = 0\n",
      "2015-07-02 04:54:39,108 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 84.0 (TID 861, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,109 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 84.0 (TID 854) in 86 ms on localhost (39/46)\n",
      "2015-07-02 04:54:39,110 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -29, init = 74, finish = 0\n",
      "2015-07-02 04:54:39,110 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 84.0 (TID 861)\n",
      "2015-07-02 04:54:39,110 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 84.0 (TID 855). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,111 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -40, init = 88, finish = 1\n",
      "2015-07-02 04:54:39,113 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 84.0 (TID 855) in 89 ms on localhost (40/46)\n",
      "2015-07-02 04:54:39,115 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:39,116 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,122 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -40, init = 87, finish = 0\n",
      "2015-07-02 04:54:39,125 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -13, init = 55, finish = 0\n",
      "2015-07-02 04:54:39,126 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 84.0 (TID 856). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,127 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 84.0 (TID 856) in 84 ms on localhost (41/46)\n",
      "2015-07-02 04:54:39,131 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -12, init = 60, finish = 0\n",
      "2015-07-02 04:54:39,146 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 84.0 (TID 858). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,146 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -17, init = 60, finish = 1\n",
      "2015-07-02 04:54:39,147 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 84.0 (TID 857). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,147 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 84.0 (TID 858) in 87 ms on localhost (42/46)\n",
      "2015-07-02 04:54:39,148 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 84.0 (TID 857) in 90 ms on localhost (43/46)\n",
      "2015-07-02 04:54:39,148 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 84.0 (TID 859). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,149 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 84.0 (TID 859) in 74 ms on localhost (44/46)\n",
      "2015-07-02 04:54:39,153 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -19, init = 67, finish = 1\n",
      "2015-07-02 04:54:39,158 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -14, init = 58, finish = 0\n",
      "2015-07-02 04:54:39,160 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 84.0 (TID 860). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,160 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 84.0 (TID 860) in 65 ms on localhost (45/46)\n",
      "2015-07-02 04:54:39,164 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -12, init = 59, finish = 1\n",
      "2015-07-02 04:54:39,170 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 84.0 (TID 861). 1107 bytes result sent to driver\n",
      "2015-07-02 04:54:39,171 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 84.0 (TID 861) in 63 ms on localhost (46/46)\n",
      "2015-07-02 04:54:39,171 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 84.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:39,171 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 84 (join at <ipython-input-3-4ba038715317>:131) finished in 0.700 s\n",
      "2015-07-02 04:54:39,171 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:39,171 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:39,172 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 89, Stage 90, Stage 87, Stage 91, Stage 92)\n",
      "2015-07-02 04:54:39,172 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:39,175 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List(Stage 87)\n",
      "2015-07-02 04:54:39,177 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:54:39,179 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 87: List()\n",
      "2015-07-02 04:54:39,180 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:54:39,181 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:39,181 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 87 (PairwiseRDD[324] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:54:39,182 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13144) called with curMem=3385313, maxMem=278302556\n",
      "2015-07-02 04:54:39,182 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_60 stored as values in memory (estimated size 12.8 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:39,183 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7218) called with curMem=3398457, maxMem=278302556\n",
      "2015-07-02 04:54:39,183 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_60_piece0 stored as bytes in memory (estimated size 7.0 KB, free 262.2 MB)\n",
      "2015-07-02 04:54:39,183 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_60_piece0 in memory on localhost:40918 (size: 7.0 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:39,183 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_60_piece0\n",
      "2015-07-02 04:54:39,184 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 60 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 48 missing tasks from Stage 87 (PairwiseRDD[324] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:54:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 87.0 with 48 tasks\n",
      "2015-07-02 04:54:39,188 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 87.0 (TID 862, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 87.0 (TID 863, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 87.0 (TID 864, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 87.0 (TID 865, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 87.0 (TID 866, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,190 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 87.0 (TID 867, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,190 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 87.0 (TID 868, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 87.0 (TID 865)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 87.0 (TID 863)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 87.0 (TID 864)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 87.0 (TID 868)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 87.0 (TID 866)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 87.0 (TID 867)\n",
      "2015-07-02 04:54:39,190 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 87.0 (TID 862)\n",
      "2015-07-02 04:54:39,195 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,195 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,197 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,197 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,200 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,200 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,204 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,204 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,206 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,206 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,207 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,208 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,236 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -59, init = 100, finish = 0\n",
      "2015-07-02 04:54:39,238 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -71, init = 116, finish = 0\n",
      "2015-07-02 04:54:39,266 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -34, init = 82, finish = 1\n",
      "2015-07-02 04:54:39,266 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -32, init = 77, finish = 1\n",
      "2015-07-02 04:54:39,266 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -42, init = 92, finish = 0\n",
      "2015-07-02 04:54:39,266 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -40, init = 86, finish = 0\n",
      "2015-07-02 04:54:39,267 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -69, init = 113, finish = 0\n",
      "2015-07-02 04:54:39,267 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 74, boot = -63, init = 136, finish = 1\n",
      "2015-07-02 04:54:39,268 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -68, init = 116, finish = 1\n",
      "2015-07-02 04:54:39,271 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -93, init = 140, finish = 0\n",
      "2015-07-02 04:54:39,274 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 76, boot = -70, init = 145, finish = 1\n",
      "2015-07-02 04:54:39,275 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 87.0 (TID 867). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,276 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 87.0 (TID 869, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,277 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 87.0 (TID 869)\n",
      "2015-07-02 04:54:39,277 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 87.0 (TID 867) in 87 ms on localhost (1/48)\n",
      "2015-07-02 04:54:39,278 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 75, boot = -83, init = 158, finish = 0\n",
      "2015-07-02 04:54:39,290 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,291 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,297 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 75, boot = -85, init = 160, finish = 0\n",
      "2015-07-02 04:54:39,298 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 80, boot = -97, init = 177, finish = 0\n",
      "2015-07-02 04:54:39,301 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 87.0 (TID 866). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,303 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 87.0 (TID 870, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,303 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 87.0 (TID 866) in 114 ms on localhost (2/48)\n",
      "2015-07-02 04:54:39,303 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 87.0 (TID 870)\n",
      "2015-07-02 04:54:39,319 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,319 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,321 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 87.0 (TID 863). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,321 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 87.0 (TID 862). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,322 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 87.0 (TID 871, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,322 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 87.0 (TID 871)\n",
      "2015-07-02 04:54:39,322 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 87.0 (TID 872, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,323 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 87.0 (TID 863) in 135 ms on localhost (3/48)\n",
      "2015-07-02 04:54:39,323 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 87.0 (TID 872)\n",
      "2015-07-02 04:54:39,323 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 87.0 (TID 862) in 135 ms on localhost (4/48)\n",
      "2015-07-02 04:54:39,330 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -5, init = 49, finish = 0\n",
      "2015-07-02 04:54:39,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,337 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -20, init = 71, finish = 0\n",
      "2015-07-02 04:54:39,342 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,342 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,349 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 87.0 (TID 868). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,350 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 87.0 (TID 873, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,351 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 87.0 (TID 865). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,351 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 87.0 (TID 868) in 161 ms on localhost (5/48)\n",
      "2015-07-02 04:54:39,351 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 87.0 (TID 874, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,351 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 87.0 (TID 873)\n",
      "2015-07-02 04:54:39,352 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 87.0 (TID 865) in 163 ms on localhost (6/48)\n",
      "2015-07-02 04:54:39,352 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 87.0 (TID 874)\n",
      "2015-07-02 04:54:39,352 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 87.0 (TID 864). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,353 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 87.0 (TID 875, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,353 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 87.0 (TID 864) in 164 ms on localhost (7/48)\n",
      "2015-07-02 04:54:39,355 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,355 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,355 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 87.0 (TID 875)\n",
      "2015-07-02 04:54:39,357 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,357 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,357 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 87.0 (TID 869). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,358 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 87.0 (TID 876, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,358 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 87.0 (TID 876)\n",
      "2015-07-02 04:54:39,359 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 87.0 (TID 869) in 82 ms on localhost (8/48)\n",
      "2015-07-02 04:54:39,359 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -52, init = 98, finish = 1\n",
      "2015-07-02 04:54:39,360 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -60, init = 115, finish = 0\n",
      "2015-07-02 04:54:39,363 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,364 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,365 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,365 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,373 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 87.0 (TID 870). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,374 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 87.0 (TID 877, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,374 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 87.0 (TID 877)\n",
      "2015-07-02 04:54:39,374 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 87.0 (TID 870) in 73 ms on localhost (9/48)\n",
      "2015-07-02 04:54:39,376 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -62, init = 111, finish = 0\n",
      "2015-07-02 04:54:39,377 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,378 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,381 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -73, init = 125, finish = 1\n",
      "2015-07-02 04:54:39,382 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -43, init = 97, finish = 0\n",
      "2015-07-02 04:54:39,389 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -61, init = 120, finish = 0\n",
      "2015-07-02 04:54:39,393 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 87.0 (TID 872). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,394 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 87.0 (TID 878, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,395 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 87.0 (TID 872) in 72 ms on localhost (10/48)\n",
      "2015-07-02 04:54:39,395 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 87.0 (TID 878)\n",
      "2015-07-02 04:54:39,396 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -66, init = 108, finish = 0\n",
      "2015-07-02 04:54:39,398 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -71, init = 113, finish = 1\n",
      "2015-07-02 04:54:39,403 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -90, init = 134, finish = 0\n",
      "2015-07-02 04:54:39,403 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 87.0 (TID 871). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,404 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,405 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 87.0 (TID 879, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,405 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -72, init = 118, finish = 0\n",
      "2015-07-02 04:54:39,405 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 87.0 (TID 879)\n",
      "2015-07-02 04:54:39,405 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,407 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -21, init = 64, finish = 1\n",
      "2015-07-02 04:54:39,408 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -94, init = 143, finish = 0\n",
      "2015-07-02 04:54:39,405 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 87.0 (TID 871) in 84 ms on localhost (11/48)\n",
      "2015-07-02 04:54:39,417 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -18, init = 67, finish = 0\n",
      "2015-07-02 04:54:39,419 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -6, init = 48, finish = 1\n",
      "2015-07-02 04:54:39,420 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -91, init = 136, finish = 0\n",
      "2015-07-02 04:54:39,425 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 87.0 (TID 875). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,426 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -7, init = 51, finish = 0\n",
      "2015-07-02 04:54:39,427 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 87.0 (TID 880, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,427 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 87.0 (TID 880)\n",
      "2015-07-02 04:54:39,428 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 87.0 (TID 875) in 76 ms on localhost (12/48)\n",
      "2015-07-02 04:54:39,434 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,435 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,444 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -9, init = 56, finish = 0\n",
      "2015-07-02 04:54:39,450 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,450 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,451 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -12, init = 60, finish = 0\n",
      "2015-07-02 04:54:39,475 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 87.0 (TID 876). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,476 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 87.0 (TID 881, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,477 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 87.0 (TID 881)\n",
      "2015-07-02 04:54:39,477 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 87.0 (TID 876) in 119 ms on localhost (13/48)\n",
      "2015-07-02 04:54:39,476 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -20, init = 81, finish = 0\n",
      "2015-07-02 04:54:39,479 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 87.0 (TID 873). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,480 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 87.0 (TID 882, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,481 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 87.0 (TID 882)\n",
      "2015-07-02 04:54:39,481 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 87.0 (TID 873) in 131 ms on localhost (14/48)\n",
      "2015-07-02 04:54:39,483 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 87.0 (TID 874). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,484 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 87.0 (TID 883, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,483 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -13, init = 84, finish = 0\n",
      "2015-07-02 04:54:39,485 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 87.0 (TID 874) in 134 ms on localhost (15/48)\n",
      "2015-07-02 04:54:39,485 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 87.0 (TID 883)\n",
      "2015-07-02 04:54:39,488 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 87.0 (TID 877). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,489 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 87.0 (TID 884, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,489 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 87.0 (TID 884)\n",
      "2015-07-02 04:54:39,489 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 87.0 (TID 877) in 115 ms on localhost (16/48)\n",
      "2015-07-02 04:54:39,491 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -19, init = 71, finish = 1\n",
      "2015-07-02 04:54:39,495 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,496 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,496 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,496 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,499 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -7, init = 70, finish = 0\n",
      "2015-07-02 04:54:39,505 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 87.0 (TID 878). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,505 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 87.0 (TID 885, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,506 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 87.0 (TID 878) in 113 ms on localhost (17/48)\n",
      "2015-07-02 04:54:39,506 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 87.0 (TID 885)\n",
      "2015-07-02 04:54:39,508 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,508 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,511 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,511 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,518 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,518 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,523 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 87.0 (TID 879). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,523 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 87.0 (TID 886, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,524 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 87.0 (TID 886)\n",
      "2015-07-02 04:54:39,524 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 87.0 (TID 880). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,524 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 87.0 (TID 879) in 120 ms on localhost (18/48)\n",
      "2015-07-02 04:54:39,524 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 87.0 (TID 887, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,525 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 87.0 (TID 887)\n",
      "2015-07-02 04:54:39,525 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 87.0 (TID 880) in 99 ms on localhost (19/48)\n",
      "2015-07-02 04:54:39,527 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,527 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,528 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,528 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,536 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -64, init = 111, finish = 0\n",
      "2015-07-02 04:54:39,538 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -68, init = 113, finish = 1\n",
      "2015-07-02 04:54:39,543 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -58, init = 113, finish = 0\n",
      "2015-07-02 04:54:39,545 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -53, init = 113, finish = 0\n",
      "2015-07-02 04:54:39,552 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -71, init = 118, finish = 1\n",
      "2015-07-02 04:54:39,552 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -74, init = 119, finish = 1\n",
      "2015-07-02 04:54:39,559 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -63, init = 106, finish = 0\n",
      "2015-07-02 04:54:39,559 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -84, init = 146, finish = 1\n",
      "2015-07-02 04:54:39,559 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65, boot = -75, init = 140, finish = 0\n",
      "2015-07-02 04:54:39,569 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -26, init = 68, finish = 0\n",
      "2015-07-02 04:54:39,570 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -24, init = 67, finish = 1\n",
      "2015-07-02 04:54:39,570 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -51, init = 104, finish = 0\n",
      "2015-07-02 04:54:39,576 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -38, init = 82, finish = 0\n",
      "2015-07-02 04:54:39,578 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -38, init = 83, finish = 0\n",
      "2015-07-02 04:54:39,594 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 87.0 (TID 882). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,595 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 87.0 (TID 888, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,596 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 87.0 (TID 888)\n",
      "2015-07-02 04:54:39,596 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 87.0 (TID 882) in 116 ms on localhost (20/48)\n",
      "2015-07-02 04:54:39,612 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,612 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,623 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 87.0 (TID 881). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,624 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 87.0 (TID 889, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,624 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 87.0 (TID 889)\n",
      "2015-07-02 04:54:39,624 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 87.0 (TID 881) in 148 ms on localhost (21/48)\n",
      "2015-07-02 04:54:39,630 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,631 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,630 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 87.0 (TID 883). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,631 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 87.0 (TID 884). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,631 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 87.0 (TID 890, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,631 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 87.0 (TID 890)\n",
      "2015-07-02 04:54:39,632 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 87.0 (TID 891, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,632 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 87.0 (TID 884) in 143 ms on localhost (22/48)\n",
      "2015-07-02 04:54:39,633 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 87.0 (TID 891)\n",
      "2015-07-02 04:54:39,633 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 87.0 (TID 883) in 149 ms on localhost (23/48)\n",
      "2015-07-02 04:54:39,633 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 87.0 (TID 886). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,634 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 87.0 (TID 885). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,634 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 87.0 (TID 892, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,634 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 87.0 (TID 893, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,635 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 87.0 (TID 893)\n",
      "2015-07-02 04:54:39,635 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 87.0 (TID 892)\n",
      "2015-07-02 04:54:39,635 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 87.0 (TID 886) in 112 ms on localhost (24/48)\n",
      "2015-07-02 04:54:39,637 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 87.0 (TID 885) in 132 ms on localhost (25/48)\n",
      "2015-07-02 04:54:39,638 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,638 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,640 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,640 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,641 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 87.0 (TID 887). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,641 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 87.0 (TID 894, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,642 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 87.0 (TID 894)\n",
      "2015-07-02 04:54:39,642 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 87.0 (TID 887) in 118 ms on localhost (26/48)\n",
      "2015-07-02 04:54:39,643 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,644 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,644 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,644 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,647 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,647 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,656 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -53, init = 104, finish = 0\n",
      "2015-07-02 04:54:39,675 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -75, init = 118, finish = 0\n",
      "2015-07-02 04:54:39,676 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -45, init = 104, finish = 1\n",
      "2015-07-02 04:54:39,680 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -70, init = 114, finish = 0\n",
      "2015-07-02 04:54:39,681 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -71, init = 122, finish = 0\n",
      "2015-07-02 04:54:39,685 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -66, init = 109, finish = 0\n",
      "2015-07-02 04:54:39,689 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -55, init = 104, finish = 1\n",
      "2015-07-02 04:54:39,691 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -58, init = 111, finish = 0\n",
      "2015-07-02 04:54:39,692 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -60, init = 104, finish = 1\n",
      "2015-07-02 04:54:39,696 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -59, init = 105, finish = 0\n",
      "2015-07-02 04:54:39,700 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -60, init = 111, finish = 1\n",
      "2015-07-02 04:54:39,713 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -67, init = 122, finish = 0\n",
      "2015-07-02 04:54:39,716 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -67, init = 114, finish = 0\n",
      "2015-07-02 04:54:39,721 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -71, init = 131, finish = 0\n",
      "2015-07-02 04:54:39,721 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 87.0 (TID 888). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,722 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 87.0 (TID 895, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,723 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 87.0 (TID 888) in 128 ms on localhost (27/48)\n",
      "2015-07-02 04:54:39,723 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 87.0 (TID 895)\n",
      "2015-07-02 04:54:39,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,751 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 87.0 (TID 889). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,752 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 87.0 (TID 896, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,752 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 87.0 (TID 896)\n",
      "2015-07-02 04:54:39,752 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 87.0 (TID 889) in 128 ms on localhost (28/48)\n",
      "2015-07-02 04:54:39,766 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,766 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,773 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 87.0 (TID 892). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,773 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 87.0 (TID 891). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,774 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 87.0 (TID 897, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,774 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 87.0 (TID 897)\n",
      "2015-07-02 04:54:39,774 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 87.0 (TID 892) in 140 ms on localhost (29/48)\n",
      "2015-07-02 04:54:39,775 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 87.0 (TID 898, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,775 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 87.0 (TID 893). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,775 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 87.0 (TID 891) in 143 ms on localhost (30/48)\n",
      "2015-07-02 04:54:39,775 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 87.0 (TID 898)\n",
      "2015-07-02 04:54:39,776 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 87.0 (TID 899, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,776 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 87.0 (TID 893) in 142 ms on localhost (31/48)\n",
      "2015-07-02 04:54:39,776 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 87.0 (TID 899)\n",
      "2015-07-02 04:54:39,777 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 87.0 (TID 890). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,778 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 87.0 (TID 900, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,777 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 87.0 (TID 894). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,778 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 87.0 (TID 890) in 147 ms on localhost (32/48)\n",
      "2015-07-02 04:54:39,779 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 87.0 (TID 901, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,778 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 87.0 (TID 900)\n",
      "2015-07-02 04:54:39,778 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,779 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 87.0 (TID 901)\n",
      "2015-07-02 04:54:39,779 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 87.0 (TID 894) in 138 ms on localhost (33/48)\n",
      "2015-07-02 04:54:39,779 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,781 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,781 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,780 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:39,781 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:39,782 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,782 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -65, init = 112, finish = 1\n",
      "2015-07-02 04:54:39,782 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,787 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -58, init = 115, finish = 0\n",
      "2015-07-02 04:54:39,784 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,788 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 4 ms\n",
      "2015-07-02 04:54:39,796 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 87.0 (TID 895). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,797 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 87.0 (TID 902, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,797 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 87.0 (TID 902)\n",
      "2015-07-02 04:54:39,797 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 87.0 (TID 895) in 75 ms on localhost (34/48)\n",
      "2015-07-02 04:54:39,799 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,799 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,807 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -49, init = 95, finish = 0\n",
      "2015-07-02 04:54:39,811 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -75, init = 129, finish = 0\n",
      "2015-07-02 04:54:39,817 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 87.0 (TID 896). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,818 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 87.0 (TID 903, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,818 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 87.0 (TID 903)\n",
      "2015-07-02 04:54:39,818 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 87.0 (TID 896) in 67 ms on localhost (35/48)\n",
      "2015-07-02 04:54:39,819 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -61, init = 102, finish = 0\n",
      "2015-07-02 04:54:39,820 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -72, init = 114, finish = 0\n",
      "2015-07-02 04:54:39,822 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,823 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -63, init = 106, finish = 1\n",
      "2015-07-02 04:54:39,827 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -75, init = 117, finish = 1\n",
      "2015-07-02 04:54:39,829 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -64, init = 109, finish = 1\n",
      "2015-07-02 04:54:39,829 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -59, init = 104, finish = 0\n",
      "2015-07-02 04:54:39,833 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -84, init = 128, finish = 0\n",
      "2015-07-02 04:54:39,829 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -65, init = 112, finish = 0\n",
      "2015-07-02 04:54:39,838 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 16 ms\n",
      "2015-07-02 04:54:39,840 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -78, init = 126, finish = 0\n",
      "2015-07-02 04:54:39,847 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -76, init = 124, finish = 1\n",
      "2015-07-02 04:54:39,850 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 47, finish = 0\n",
      "2015-07-02 04:54:39,852 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -7, init = 59, finish = 1\n",
      "2015-07-02 04:54:39,854 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 87.0 (TID 902). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,855 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 87.0 (TID 904, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,856 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 87.0 (TID 904)\n",
      "2015-07-02 04:54:39,857 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 87.0 (TID 902) in 59 ms on localhost (36/48)\n",
      "2015-07-02 04:54:39,863 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 04:54:39,869 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -5, init = 49, finish = 0\n",
      "2015-07-02 04:54:39,877 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,877 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,884 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 87.0 (TID 898). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,884 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 87.0 (TID 897). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,885 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 87.0 (TID 905, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,885 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 87.0 (TID 905)\n",
      "2015-07-02 04:54:39,885 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 87.0 (TID 906, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,886 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 87.0 (TID 906)\n",
      "2015-07-02 04:54:39,886 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 87.0 (TID 898) in 111 ms on localhost (37/48)\n",
      "2015-07-02 04:54:39,887 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 87.0 (TID 897) in 114 ms on localhost (38/48)\n",
      "2015-07-02 04:54:39,899 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,899 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,912 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 87.0 (TID 900). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,912 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 87.0 (TID 899). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,912 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 87.0 (TID 901). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,913 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 87.0 (TID 903). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,913 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 87.0 (TID 907, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,913 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 87.0 (TID 900) in 135 ms on localhost (39/48)\n",
      "2015-07-02 04:54:39,914 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 87.0 (TID 899) in 139 ms on localhost (40/48)\n",
      "2015-07-02 04:54:39,914 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 87.0 (TID 907)\n",
      "2015-07-02 04:54:39,914 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 87.0 (TID 908, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,914 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 87.0 (TID 908)\n",
      "2015-07-02 04:54:39,914 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 87.0 (TID 909, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:39,915 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 87.0 (TID 909)\n",
      "2015-07-02 04:54:39,915 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 87.0 (TID 901) in 136 ms on localhost (41/48)\n",
      "2015-07-02 04:54:39,915 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 87.0 (TID 903) in 97 ms on localhost (42/48)\n",
      "2015-07-02 04:54:39,916 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 46 blocks\n",
      "2015-07-02 04:54:39,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:39,918 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -6, init = 55, finish = 0\n",
      "2015-07-02 04:54:39,918 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:39,918 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,919 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:39,919 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:39,922 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -3, init = 63, finish = 0\n",
      "2015-07-02 04:54:39,928 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 87.0 (TID 904). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,929 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 87.0 (TID 904) in 74 ms on localhost (43/48)\n",
      "2015-07-02 04:54:39,939 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -42, init = 87, finish = 0\n",
      "2015-07-02 04:54:39,940 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -35, init = 87, finish = 1\n",
      "2015-07-02 04:54:39,942 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -55, init = 101, finish = 1\n",
      "2015-07-02 04:54:39,949 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -50, init = 105, finish = 0\n",
      "2015-07-02 04:54:39,954 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 87.0 (TID 905). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,956 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 87.0 (TID 905) in 70 ms on localhost (44/48)\n",
      "2015-07-02 04:54:39,957 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -58, init = 98, finish = 0\n",
      "2015-07-02 04:54:39,957 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -61, init = 103, finish = 0\n",
      "2015-07-02 04:54:39,959 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 87.0 (TID 907). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,959 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 87.0 (TID 906). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,959 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -57, init = 98, finish = 1\n",
      "2015-07-02 04:54:39,960 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 87.0 (TID 907) in 47 ms on localhost (45/48)\n",
      "2015-07-02 04:54:39,960 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 87.0 (TID 906) in 75 ms on localhost (46/48)\n",
      "2015-07-02 04:54:39,961 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -67, init = 110, finish = 1\n",
      "2015-07-02 04:54:39,970 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -67, init = 110, finish = 1\n",
      "2015-07-02 04:54:39,972 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -79, init = 125, finish = 1\n",
      "2015-07-02 04:54:39,984 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 87.0 (TID 909). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,985 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 87.0 (TID 909) in 70 ms on localhost (47/48)\n",
      "2015-07-02 04:54:39,985 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 87.0 (TID 908). 1109 bytes result sent to driver\n",
      "2015-07-02 04:54:39,986 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 87.0 (TID 908) in 72 ms on localhost (48/48)\n",
      "2015-07-02 04:54:39,986 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 87.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 87 (join at <ipython-input-3-4ba038715317>:131) finished in 0.798 s\n",
      "2015-07-02 04:54:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 89, Stage 90, Stage 91, Stage 92)\n",
      "2015-07-02 04:54:39,987 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:39,989 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 89: List()\n",
      "2015-07-02 04:54:39,991 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List(Stage 89)\n",
      "2015-07-02 04:54:39,993 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:54:39,994 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:39,994 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 89 (PairwiseRDD[331] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:54:39,995 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(13160) called with curMem=3405675, maxMem=278302556\n",
      "2015-07-02 04:54:39,995 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_61 stored as values in memory (estimated size 12.9 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:39,996 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7212) called with curMem=3418835, maxMem=278302556\n",
      "2015-07-02 04:54:39,996 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_61_piece0 stored as bytes in memory (estimated size 7.0 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:39,996 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_61_piece0 in memory on localhost:40918 (size: 7.0 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:39,996 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_61_piece0\n",
      "2015-07-02 04:54:39,997 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 61 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:40,000 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 50 missing tasks from Stage 89 (PairwiseRDD[331] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:54:40,000 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 89.0 with 50 tasks\n",
      "2015-07-02 04:54:40,001 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 89.0 (TID 910, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,001 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 89.0 (TID 911, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,002 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 89.0 (TID 912, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,002 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 89.0 (TID 913, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,002 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 89.0 (TID 914, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,002 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 89.0 (TID 915, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,003 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 89.0 (TID 916, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 89.0 (TID 910)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 89.0 (TID 911)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 89.0 (TID 915)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 89.0 (TID 912)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 89.0 (TID 913)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 89.0 (TID 914)\n",
      "2015-07-02 04:54:40,003 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 89.0 (TID 916)\n",
      "2015-07-02 04:54:40,006 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,008 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,008 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,008 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,009 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,009 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,014 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,014 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,015 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,015 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,014 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,015 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,046 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -51, init = 92, finish = 0\n",
      "2015-07-02 04:54:40,048 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 79, finish = 1\n",
      "2015-07-02 04:54:40,053 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -36, init = 78, finish = 0\n",
      "2015-07-02 04:54:40,055 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -30, init = 75, finish = 1\n",
      "2015-07-02 04:54:40,056 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -57, init = 99, finish = 0\n",
      "2015-07-02 04:54:40,058 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -34, init = 77, finish = 1\n",
      "2015-07-02 04:54:40,062 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -52, init = 104, finish = 0\n",
      "2015-07-02 04:54:40,075 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 89.0 (TID 912). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,096 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 68, boot = -34, init = 101, finish = 1\n",
      "2015-07-02 04:54:40,096 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -130, init = 185, finish = 0\n",
      "2015-07-02 04:54:40,096 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 89.0 (TID 917, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,098 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 89.0 (TID 917)\n",
      "2015-07-02 04:54:40,098 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 89.0 (TID 912) in 96 ms on localhost (1/50)\n",
      "2015-07-02 04:54:40,100 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -36, init = 81, finish = 1\n",
      "2015-07-02 04:54:40,101 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -77, init = 127, finish = 1\n",
      "2015-07-02 04:54:40,104 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -134, init = 188, finish = 1\n",
      "2015-07-02 04:54:40,106 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 93, boot = -54, init = 146, finish = 1\n",
      "2015-07-02 04:54:40,106 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -75, init = 134, finish = 0\n",
      "2015-07-02 04:54:40,118 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,118 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,142 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 89.0 (TID 910). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,143 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 89.0 (TID 918, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,146 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 89.0 (TID 918)\n",
      "2015-07-02 04:54:40,146 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 89.0 (TID 910) in 145 ms on localhost (2/50)\n",
      "2015-07-02 04:54:40,158 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -38, init = 85, finish = 0\n",
      "2015-07-02 04:54:40,163 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,164 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,164 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -38, init = 97, finish = 0\n",
      "2015-07-02 04:54:40,164 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 89.0 (TID 916). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,165 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 89.0 (TID 919, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,167 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 89.0 (TID 916) in 165 ms on localhost (3/50)\n",
      "2015-07-02 04:54:40,167 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 89.0 (TID 919)\n",
      "2015-07-02 04:54:40,169 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 89.0 (TID 915). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,170 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 89.0 (TID 920, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,171 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 89.0 (TID 915) in 169 ms on localhost (4/50)\n",
      "2015-07-02 04:54:40,171 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 89.0 (TID 920)\n",
      "2015-07-02 04:54:40,173 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 89.0 (TID 913). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,174 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,174 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,174 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 89.0 (TID 921, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,175 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 89.0 (TID 913) in 173 ms on localhost (5/50)\n",
      "2015-07-02 04:54:40,175 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 89.0 (TID 921)\n",
      "2015-07-02 04:54:40,177 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 89.0 (TID 911). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,177 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 89.0 (TID 914). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,177 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 89.0 (TID 922, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,178 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 89.0 (TID 922)\n",
      "2015-07-02 04:54:40,178 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 89.0 (TID 911) in 177 ms on localhost (6/50)\n",
      "2015-07-02 04:54:40,178 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 89.0 (TID 923, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,179 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 89.0 (TID 923)\n",
      "2015-07-02 04:54:40,179 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 89.0 (TID 914) in 177 ms on localhost (7/50)\n",
      "2015-07-02 04:54:40,183 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,183 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,186 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,187 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,189 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,189 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,189 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,190 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,193 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 89.0 (TID 917). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,193 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 89.0 (TID 924, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,194 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 89.0 (TID 924)\n",
      "2015-07-02 04:54:40,194 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 89.0 (TID 917) in 98 ms on localhost (8/50)\n",
      "2015-07-02 04:54:40,196 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,196 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,204 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -76, init = 123, finish = 1\n",
      "2015-07-02 04:54:40,210 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -69, init = 127, finish = 0\n",
      "2015-07-02 04:54:40,215 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -89, init = 133, finish = 0\n",
      "2015-07-02 04:54:40,218 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 89.0 (TID 918). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,219 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 89.0 (TID 925, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,220 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 89.0 (TID 925)\n",
      "2015-07-02 04:54:40,220 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 89.0 (TID 918) in 77 ms on localhost (9/50)\n",
      "2015-07-02 04:54:40,220 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -99, init = 147, finish = 0\n",
      "2015-07-02 04:54:40,222 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,223 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,224 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -93, init = 137, finish = 1\n",
      "2015-07-02 04:54:40,228 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -84, init = 129, finish = 0\n",
      "2015-07-02 04:54:40,231 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -90, init = 142, finish = 1\n",
      "2015-07-02 04:54:40,232 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 89.0 (TID 919). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,234 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 89.0 (TID 926, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,234 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -85, init = 133, finish = 1\n",
      "2015-07-02 04:54:40,234 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 89.0 (TID 919) in 69 ms on localhost (10/50)\n",
      "2015-07-02 04:54:40,235 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -89, init = 134, finish = 1\n",
      "2015-07-02 04:54:40,236 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -76, init = 126, finish = 0\n",
      "2015-07-02 04:54:40,240 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -86, init = 143, finish = 0\n",
      "2015-07-02 04:54:40,241 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -78, init = 138, finish = 1\n",
      "2015-07-02 04:54:40,246 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -29, init = 73, finish = 1\n",
      "2015-07-02 04:54:40,252 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 89.0 (TID 926)\n",
      "2015-07-02 04:54:40,257 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -27, init = 79, finish = 1\n",
      "2015-07-02 04:54:40,263 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -10, init = 51, finish = 0\n",
      "2015-07-02 04:54:40,269 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -7, init = 50, finish = 0\n",
      "2015-07-02 04:54:40,274 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,274 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,279 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 89.0 (TID 920). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,279 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 89.0 (TID 927, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,280 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 89.0 (TID 927)\n",
      "2015-07-02 04:54:40,280 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 89.0 (TID 920) in 110 ms on localhost (11/50)\n",
      "2015-07-02 04:54:40,284 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 89.0 (TID 922). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,285 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 89.0 (TID 928, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,285 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 89.0 (TID 928)\n",
      "2015-07-02 04:54:40,285 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 89.0 (TID 922) in 108 ms on localhost (12/50)\n",
      "2015-07-02 04:54:40,294 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,294 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,298 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,298 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,308 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 89.0 (TID 923). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,308 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 89.0 (TID 929, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,309 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 89.0 (TID 929)\n",
      "2015-07-02 04:54:40,309 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 89.0 (TID 923) in 131 ms on localhost (13/50)\n",
      "2015-07-02 04:54:40,309 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 89.0 (TID 921). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,310 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 89.0 (TID 924). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,310 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 89.0 (TID 930, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,310 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 89.0 (TID 930)\n",
      "2015-07-02 04:54:40,310 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 89.0 (TID 931, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,311 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 89.0 (TID 925). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,311 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 89.0 (TID 931)\n",
      "2015-07-02 04:54:40,311 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 89.0 (TID 921) in 137 ms on localhost (14/50)\n",
      "2015-07-02 04:54:40,311 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 89.0 (TID 924) in 118 ms on localhost (15/50)\n",
      "2015-07-02 04:54:40,312 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 89.0 (TID 925) in 92 ms on localhost (16/50)\n",
      "2015-07-02 04:54:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 89.0 (TID 932, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,313 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,313 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 89.0 (TID 932)\n",
      "2015-07-02 04:54:40,313 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,314 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,314 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,315 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -40, init = 89, finish = 1\n",
      "2015-07-02 04:54:40,318 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,319 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,320 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,320 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,321 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -31, init = 91, finish = 1\n",
      "2015-07-02 04:54:40,328 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 89.0 (TID 926). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,328 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 89.0 (TID 933, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,329 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 89.0 (TID 933)\n",
      "2015-07-02 04:54:40,329 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 89.0 (TID 926) in 96 ms on localhost (17/50)\n",
      "2015-07-02 04:54:40,331 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,331 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,334 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -26, init = 73, finish = 0\n",
      "2015-07-02 04:54:40,338 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -44, init = 97, finish = 0\n",
      "2015-07-02 04:54:40,339 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -36, init = 83, finish = 1\n",
      "2015-07-02 04:54:40,346 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -34, init = 88, finish = 0\n",
      "2015-07-02 04:54:40,349 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 89.0 (TID 927). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,351 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 89.0 (TID 934, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,351 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 89.0 (TID 934)\n",
      "2015-07-02 04:54:40,351 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 89.0 (TID 927) in 72 ms on localhost (18/50)\n",
      "2015-07-02 04:54:40,353 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -63, init = 104, finish = 1\n",
      "2015-07-02 04:54:40,355 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,355 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,355 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -59, init = 100, finish = 1\n",
      "2015-07-02 04:54:40,358 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 89.0 (TID 928). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,361 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 89.0 (TID 935, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,362 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -53, init = 98, finish = 0\n",
      "2015-07-02 04:54:40,362 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 89.0 (TID 928) in 77 ms on localhost (19/50)\n",
      "2015-07-02 04:54:40,364 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -54, init = 98, finish = 0\n",
      "2015-07-02 04:54:40,366 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -45, init = 87, finish = 1\n",
      "2015-07-02 04:54:40,370 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 89.0 (TID 935)\n",
      "2015-07-02 04:54:40,375 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -55, init = 105, finish = 0\n",
      "2015-07-02 04:54:40,375 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -67, init = 111, finish = 0\n",
      "2015-07-02 04:54:40,375 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 04:54:40,381 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -42, init = 94, finish = 0\n",
      "2015-07-02 04:54:40,381 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -4, init = 50, finish = 0\n",
      "2015-07-02 04:54:40,394 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,394 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,395 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -11, init = 51, finish = 0\n",
      "2015-07-02 04:54:40,399 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 89.0 (TID 930). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,400 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -8, init = 50, finish = 1\n",
      "2015-07-02 04:54:40,400 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 89.0 (TID 936, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,400 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 89.0 (TID 936)\n",
      "2015-07-02 04:54:40,401 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 89.0 (TID 930) in 90 ms on localhost (20/50)\n",
      "2015-07-02 04:54:40,415 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,415 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,434 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -35, init = 81, finish = 0\n",
      "2015-07-02 04:54:40,436 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 89.0 (TID 931). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,437 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 89.0 (TID 929). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,448 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 89.0 (TID 933). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,447 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 89.0 (TID 937, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,449 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 89.0 (TID 932). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,449 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 89.0 (TID 938, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,449 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 89.0 (TID 937)\n",
      "2015-07-02 04:54:40,450 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 89.0 (TID 931) in 140 ms on localhost (21/50)\n",
      "2015-07-02 04:54:40,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 89.0 (TID 939, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,451 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 89.0 (TID 939)\n",
      "2015-07-02 04:54:40,450 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 89.0 (TID 938)\n",
      "2015-07-02 04:54:40,451 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 89.0 (TID 940, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,452 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 89.0 (TID 940)\n",
      "2015-07-02 04:54:40,450 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -21, init = 83, finish = 1\n",
      "2015-07-02 04:54:40,452 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 89.0 (TID 934). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,452 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 89.0 (TID 929) in 144 ms on localhost (22/50)\n",
      "2015-07-02 04:54:40,453 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 89.0 (TID 941, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,454 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 89.0 (TID 941)\n",
      "2015-07-02 04:54:40,454 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 89.0 (TID 932) in 142 ms on localhost (23/50)\n",
      "2015-07-02 04:54:40,454 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 89.0 (TID 933) in 126 ms on localhost (24/50)\n",
      "2015-07-02 04:54:40,455 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 89.0 (TID 934) in 105 ms on localhost (25/50)\n",
      "2015-07-02 04:54:40,455 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -26, init = 72, finish = 0\n",
      "2015-07-02 04:54:40,463 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -35, init = 88, finish = 0\n",
      "2015-07-02 04:54:40,467 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,467 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,468 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,468 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,472 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,472 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,480 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,480 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,487 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 89.0 (TID 935). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,487 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 89.0 (TID 942, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,488 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 89.0 (TID 942)\n",
      "2015-07-02 04:54:40,488 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 89.0 (TID 935) in 127 ms on localhost (26/50)\n",
      "2015-07-02 04:54:40,489 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 89.0 (TID 936). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,489 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 89.0 (TID 943, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,489 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 89.0 (TID 943)\n",
      "2015-07-02 04:54:40,489 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 89.0 (TID 936) in 89 ms on localhost (27/50)\n",
      "2015-07-02 04:54:40,491 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,492 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,492 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,492 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,507 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -67, init = 120, finish = 0\n",
      "2015-07-02 04:54:40,508 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -76, init = 132, finish = 0\n",
      "2015-07-02 04:54:40,508 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -82, init = 138, finish = 0\n",
      "2015-07-02 04:54:40,510 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -71, init = 128, finish = 0\n",
      "2015-07-02 04:54:40,517 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -58, init = 109, finish = 0\n",
      "2015-07-02 04:54:40,520 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -57, init = 116, finish = 0\n",
      "2015-07-02 04:54:40,522 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -77, init = 140, finish = 0\n",
      "2015-07-02 04:54:40,524 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -74, init = 136, finish = 1\n",
      "2015-07-02 04:54:40,528 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70, boot = -66, init = 136, finish = 0\n",
      "2015-07-02 04:54:40,529 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69, boot = -89, init = 158, finish = 0\n",
      "2015-07-02 04:54:40,534 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -26, init = 68, finish = 1\n",
      "2015-07-02 04:54:40,536 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -47, init = 91, finish = 0\n",
      "2015-07-02 04:54:40,541 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -25, init = 69, finish = 0\n",
      "2015-07-02 04:54:40,549 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -45, init = 93, finish = 1\n",
      "2015-07-02 04:54:40,582 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 89.0 (TID 939). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,582 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 89.0 (TID 944, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,583 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 89.0 (TID 939) in 133 ms on localhost (28/50)\n",
      "2015-07-02 04:54:40,583 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 89.0 (TID 944)\n",
      "2015-07-02 04:54:40,597 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 89.0 (TID 937). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,597 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 89.0 (TID 945, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,598 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 89.0 (TID 937) in 161 ms on localhost (29/50)\n",
      "2015-07-02 04:54:40,598 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 89.0 (TID 945)\n",
      "2015-07-02 04:54:40,602 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 89.0 (TID 941). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,602 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 89.0 (TID 940). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,604 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 89.0 (TID 938). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,604 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 89.0 (TID 946, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,604 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 89.0 (TID 946)\n",
      "2015-07-02 04:54:40,605 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 89.0 (TID 947, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,605 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 89.0 (TID 948, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,605 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 89.0 (TID 947)\n",
      "2015-07-02 04:54:40,606 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 89.0 (TID 948)\n",
      "2015-07-02 04:54:40,606 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 89.0 (TID 941) in 153 ms on localhost (30/50)\n",
      "2015-07-02 04:54:40,607 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 89.0 (TID 938) in 158 ms on localhost (31/50)\n",
      "2015-07-02 04:54:40,608 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 89.0 (TID 940) in 156 ms on localhost (32/50)\n",
      "2015-07-02 04:54:40,611 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,611 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,614 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,614 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,618 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,618 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,619 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 89.0 (TID 943). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,620 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 89.0 (TID 949, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,621 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 89.0 (TID 943) in 131 ms on localhost (33/50)\n",
      "2015-07-02 04:54:40,621 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 89.0 (TID 949)\n",
      "2015-07-02 04:54:40,622 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 89.0 (TID 942). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,623 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 89.0 (TID 950, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,624 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,624 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 89.0 (TID 942) in 137 ms on localhost (34/50)\n",
      "2015-07-02 04:54:40,624 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,624 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 89.0 (TID 950)\n",
      "2015-07-02 04:54:40,624 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,624 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,629 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,629 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,630 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,630 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,651 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -71, init = 129, finish = 0\n",
      "2015-07-02 04:54:40,656 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -67, init = 134, finish = 0\n",
      "2015-07-02 04:54:40,662 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -75, init = 125, finish = 0\n",
      "2015-07-02 04:54:40,663 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -71, init = 122, finish = 0\n",
      "2015-07-02 04:54:40,667 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -78, init = 134, finish = 0\n",
      "2015-07-02 04:54:40,672 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -69, init = 122, finish = 1\n",
      "2015-07-02 04:54:40,677 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -83, init = 136, finish = 0\n",
      "2015-07-02 04:54:40,679 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -79, init = 120, finish = 1\n",
      "2015-07-02 04:54:40,681 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -74, init = 140, finish = 1\n",
      "2015-07-02 04:54:40,689 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -77, init = 130, finish = 1\n",
      "2015-07-02 04:54:40,691 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 73, boot = -73, init = 145, finish = 1\n",
      "2015-07-02 04:54:40,699 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65, boot = -77, init = 141, finish = 1\n",
      "2015-07-02 04:54:40,704 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 63, boot = -82, init = 145, finish = 0\n",
      "2015-07-02 04:54:40,707 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -75, init = 142, finish = 0\n",
      "2015-07-02 04:54:40,846 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 89.0 (TID 948). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,846 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 89.0 (TID 944). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,846 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 89.0 (TID 951, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,847 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 89.0 (TID 951)\n",
      "2015-07-02 04:54:40,847 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 89.0 (TID 952, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,847 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 89.0 (TID 952)\n",
      "2015-07-02 04:54:40,847 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 89.0 (TID 948) in 242 ms on localhost (35/50)\n",
      "2015-07-02 04:54:40,848 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 89.0 (TID 944) in 265 ms on localhost (36/50)\n",
      "2015-07-02 04:54:40,858 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,858 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,863 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 89.0 (TID 946). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,863 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,863 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 89.0 (TID 947). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,863 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,864 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 89.0 (TID 953, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,864 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 89.0 (TID 950). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,864 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 89.0 (TID 953)\n",
      "2015-07-02 04:54:40,864 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 89.0 (TID 949). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,864 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 89.0 (TID 945). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,864 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 89.0 (TID 946) in 260 ms on localhost (37/50)\n",
      "2015-07-02 04:54:40,865 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 89.0 (TID 954, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,865 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 89.0 (TID 954)\n",
      "2015-07-02 04:54:40,865 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 89.0 (TID 947) in 261 ms on localhost (38/50)\n",
      "2015-07-02 04:54:40,866 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 89.0 (TID 955, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,867 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 89.0 (TID 955)\n",
      "2015-07-02 04:54:40,867 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 89.0 (TID 950) in 244 ms on localhost (39/50)\n",
      "2015-07-02 04:54:40,867 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 89.0 (TID 956, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,868 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 89.0 (TID 956)\n",
      "2015-07-02 04:54:40,868 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 89.0 (TID 957, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,868 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 89.0 (TID 949) in 248 ms on localhost (40/50)\n",
      "2015-07-02 04:54:40,869 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 89.0 (TID 945) in 271 ms on localhost (41/50)\n",
      "2015-07-02 04:54:40,869 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 89.0 (TID 957)\n",
      "2015-07-02 04:54:40,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,873 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,874 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 44 non-empty blocks out of 48 blocks\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,879 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:40,898 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -194, init = 235, finish = 0\n",
      "2015-07-02 04:54:40,902 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -177, init = 227, finish = 0\n",
      "2015-07-02 04:54:40,904 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -199, init = 245, finish = 1\n",
      "2015-07-02 04:54:40,911 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -181, init = 237, finish = 0\n",
      "2015-07-02 04:54:40,912 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -164, init = 210, finish = 0\n",
      "2015-07-02 04:54:40,914 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -197, init = 239, finish = 1\n",
      "2015-07-02 04:54:40,918 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 89.0 (TID 952). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,919 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 89.0 (TID 958, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,920 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 89.0 (TID 958)\n",
      "2015-07-02 04:54:40,921 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -166, init = 212, finish = 0\n",
      "2015-07-02 04:54:40,921 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -170, init = 216, finish = 0\n",
      "2015-07-02 04:54:40,924 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -77, init = 125, finish = 1\n",
      "2015-07-02 04:54:40,919 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -185, init = 232, finish = 0\n",
      "2015-07-02 04:54:40,933 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -162, init = 219, finish = 0\n",
      "2015-07-02 04:54:40,920 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -166, init = 213, finish = 0\n",
      "2015-07-02 04:54:40,920 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 89.0 (TID 952) in 73 ms on localhost (42/50)\n",
      "2015-07-02 04:54:40,936 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -190, init = 240, finish = 1\n",
      "2015-07-02 04:54:40,950 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:40,951 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:40,953 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -181, init = 252, finish = 0\n",
      "2015-07-02 04:54:40,991 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -16, init = 82, finish = 1\n",
      "2015-07-02 04:54:40,991 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 89.0 (TID 956). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:40,992 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 89.0 (TID 959, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:40,992 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 89.0 (TID 959)\n",
      "2015-07-02 04:54:40,992 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 89.0 (TID 956) in 125 ms on localhost (43/50)\n",
      "2015-07-02 04:54:40,998 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 69, boot = -17, init = 86, finish = 0\n",
      "2015-07-02 04:54:41,001 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 89.0 (TID 957). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,002 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 89.0 (TID 951). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,002 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 89.0 (TID 953). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,003 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 89.0 (TID 957) in 134 ms on localhost (44/50)\n",
      "2015-07-02 04:54:41,003 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 89.0 (TID 951) in 157 ms on localhost (45/50)\n",
      "2015-07-02 04:54:41,004 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 89.0 (TID 953) in 141 ms on localhost (46/50)\n",
      "2015-07-02 04:54:41,008 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 89.0 (TID 955). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,009 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 89.0 (TID 955) in 142 ms on localhost (47/50)\n",
      "2015-07-02 04:54:41,010 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 04:54:41,010 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,010 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 89.0 (TID 954). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,011 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 89.0 (TID 954) in 146 ms on localhost (48/50)\n",
      "2015-07-02 04:54:41,014 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 89.0 (TID 958). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,015 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 89.0 (TID 958) in 95 ms on localhost (49/50)\n",
      "2015-07-02 04:54:41,051 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -79, init = 122, finish = 0\n",
      "2015-07-02 04:54:41,055 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -52, init = 106, finish = 0\n",
      "2015-07-02 04:54:41,062 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 89.0 (TID 959). 1111 bytes result sent to driver\n",
      "2015-07-02 04:54:41,063 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 89.0 (TID 959) in 71 ms on localhost (50/50)\n",
      "2015-07-02 04:54:41,063 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:41,063 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 89 (join at <ipython-input-3-4ba038715317>:131) finished in 1.063 s\n",
      "2015-07-02 04:54:41,064 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:41,064 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:41,064 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 90, Stage 91, Stage 92)\n",
      "2015-07-02 04:54:41,064 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:41,067 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 90: List()\n",
      "2015-07-02 04:54:41,068 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List(Stage 90)\n",
      "2015-07-02 04:54:41,069 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:41,069 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 90 (PairwiseRDD[338] at join at <ipython-input-3-4ba038715317>:131), which is now runnable\n",
      "2015-07-02 04:54:41,071 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(14208) called with curMem=3426047, maxMem=278302556\n",
      "2015-07-02 04:54:41,071 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_62 stored as values in memory (estimated size 13.9 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:41,072 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(8038) called with curMem=3440255, maxMem=278302556\n",
      "2015-07-02 04:54:41,072 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_62_piece0 stored as bytes in memory (estimated size 7.8 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:41,072 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_62_piece0 in memory on localhost:40918 (size: 7.8 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:41,073 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_62_piece0\n",
      "2015-07-02 04:54:41,073 INFO  [sparkDriver-akka.actor.default-dispatcher-18] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 62 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:41,076 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 90 (PairwiseRDD[338] at join at <ipython-input-3-4ba038715317>:131)\n",
      "2015-07-02 04:54:41,077 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 90.0 with 52 tasks\n",
      "2015-07-02 04:54:41,077 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 90.0 (TID 960, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,078 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 90.0 (TID 961, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,078 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 90.0 (TID 962, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,078 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 90.0 (TID 963, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,078 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 90.0 (TID 964, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,079 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 90.0 (TID 965, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,079 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 90.0 (TID 966, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,079 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 90.0 (TID 965)\n",
      "2015-07-02 04:54:41,079 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 90.0 (TID 966)\n",
      "2015-07-02 04:54:41,080 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 90.0 (TID 960)\n",
      "2015-07-02 04:54:41,080 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 90.0 (TID 961)\n",
      "2015-07-02 04:54:41,079 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 90.0 (TID 962)\n",
      "2015-07-02 04:54:41,079 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 90.0 (TID 964)\n",
      "2015-07-02 04:54:41,079 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 90.0 (TID 963)\n",
      "2015-07-02 04:54:41,084 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,084 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,085 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,085 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,086 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,086 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,087 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,088 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,090 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,090 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,090 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,090 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,125 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -133, init = 174, finish = 1\n",
      "2015-07-02 04:54:41,126 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -163, init = 206, finish = 1\n",
      "2015-07-02 04:54:41,127 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -29, init = 69, finish = 0\n",
      "2015-07-02 04:54:41,131 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -81, init = 128, finish = 1\n",
      "2015-07-02 04:54:41,135 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -80, init = 128, finish = 0\n",
      "2015-07-02 04:54:41,135 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -136, init = 182, finish = 0\n",
      "2015-07-02 04:54:41,136 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -146, init = 196, finish = 0\n",
      "2015-07-02 04:54:41,139 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -136, init = 190, finish = 0\n",
      "2015-07-02 04:54:41,139 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -141, init = 187, finish = 0\n",
      "2015-07-02 04:54:41,141 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 59, boot = -167, init = 226, finish = 0\n",
      "2015-07-02 04:54:41,142 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 90.0 (TID 964). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,143 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -27, init = 71, finish = 1\n",
      "2015-07-02 04:54:41,143 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 90.0 (TID 967, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,145 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 90.0 (TID 967)\n",
      "2015-07-02 04:54:41,145 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 90.0 (TID 964) in 67 ms on localhost (1/52)\n",
      "2015-07-02 04:54:41,147 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -137, init = 197, finish = 0\n",
      "2015-07-02 04:54:41,149 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -145, init = 189, finish = 0\n",
      "2015-07-02 04:54:41,151 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,153 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:41,161 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -143, init = 198, finish = 1\n",
      "2015-07-02 04:54:41,173 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 90.0 (TID 966). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,176 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 90.0 (TID 968, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,176 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 90.0 (TID 968)\n",
      "2015-07-02 04:54:41,176 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 90.0 (TID 966) in 97 ms on localhost (2/52)\n",
      "2015-07-02 04:54:41,191 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,192 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,193 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = 10, init = 32, finish = 0\n",
      "2015-07-02 04:54:41,200 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = 21, init = 25, finish = 1\n",
      "2015-07-02 04:54:41,225 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 90.0 (TID 960). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,226 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 90.0 (TID 969, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,226 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 90.0 (TID 969)\n",
      "2015-07-02 04:54:41,226 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 90.0 (TID 960) in 149 ms on localhost (3/52)\n",
      "2015-07-02 04:54:41,233 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 90.0 (TID 961). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,234 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 90.0 (TID 970, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,234 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,234 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 90.0 (TID 970)\n",
      "2015-07-02 04:54:41,234 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,235 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 90.0 (TID 965). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,235 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 90.0 (TID 961) in 158 ms on localhost (4/52)\n",
      "2015-07-02 04:54:41,236 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 90.0 (TID 962). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,236 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 90.0 (TID 971, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,236 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -16, init = 61, finish = 1\n",
      "2015-07-02 04:54:41,236 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 90.0 (TID 965) in 157 ms on localhost (5/52)\n",
      "2015-07-02 04:54:41,237 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 90.0 (TID 971)\n",
      "2015-07-02 04:54:41,237 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 90.0 (TID 972, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,238 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 90.0 (TID 972)\n",
      "2015-07-02 04:54:41,238 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 90.0 (TID 962) in 160 ms on localhost (6/52)\n",
      "2015-07-02 04:54:41,242 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,243 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -13, init = 71, finish = 0\n",
      "2015-07-02 04:54:41,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,243 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 90.0 (TID 963). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,243 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,245 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 90.0 (TID 967). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,245 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 90.0 (TID 973, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,247 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 90.0 (TID 973)\n",
      "2015-07-02 04:54:41,247 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 90.0 (TID 974, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,248 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 90.0 (TID 974)\n",
      "2015-07-02 04:54:41,248 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 90.0 (TID 963) in 170 ms on localhost (7/52)\n",
      "2015-07-02 04:54:41,249 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 90.0 (TID 967) in 105 ms on localhost (8/52)\n",
      "2015-07-02 04:54:41,255 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,255 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,255 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,255 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,260 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 90.0 (TID 968). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,260 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 90.0 (TID 975, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,260 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 90.0 (TID 975)\n",
      "2015-07-02 04:54:41,261 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 90.0 (TID 968) in 86 ms on localhost (9/52)\n",
      "2015-07-02 04:54:41,263 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,263 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,275 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -92, init = 135, finish = 0\n",
      "2015-07-02 04:54:41,276 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -71, init = 120, finish = 0\n",
      "2015-07-02 04:54:41,283 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -71, init = 112, finish = 1\n",
      "2015-07-02 04:54:41,284 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -64, init = 109, finish = 0\n",
      "2015-07-02 04:54:41,286 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -69, init = 110, finish = 1\n",
      "2015-07-02 04:54:41,288 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -67, init = 117, finish = 0\n",
      "2015-07-02 04:54:41,291 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -71, init = 115, finish = 0\n",
      "2015-07-02 04:54:41,297 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -72, init = 119, finish = 1\n",
      "2015-07-02 04:54:41,297 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -51, init = 94, finish = 0\n",
      "2015-07-02 04:54:41,297 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -49, init = 92, finish = 0\n",
      "2015-07-02 04:54:41,304 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -77, init = 126, finish = 0\n",
      "2015-07-02 04:54:41,309 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -100, init = 149, finish = 0\n",
      "2015-07-02 04:54:41,313 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -13, init = 58, finish = 1\n",
      "2015-07-02 04:54:41,314 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -19, init = 71, finish = 0\n",
      "2015-07-02 04:54:41,343 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 90.0 (TID 969). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,344 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 90.0 (TID 976, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,344 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 90.0 (TID 976)\n",
      "2015-07-02 04:54:41,344 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 90.0 (TID 969) in 119 ms on localhost (10/52)\n",
      "2015-07-02 04:54:41,361 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,361 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,374 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 90.0 (TID 972). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,374 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 90.0 (TID 970). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,374 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 90.0 (TID 971). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,374 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 90.0 (TID 974). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,375 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 90.0 (TID 977, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,375 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 90.0 (TID 978, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,376 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 90.0 (TID 970) in 142 ms on localhost (11/52)\n",
      "2015-07-02 04:54:41,376 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 90.0 (TID 978)\n",
      "2015-07-02 04:54:41,376 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 90.0 (TID 972) in 139 ms on localhost (12/52)\n",
      "2015-07-02 04:54:41,376 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 90.0 (TID 979, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,377 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 90.0 (TID 973). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,377 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 90.0 (TID 971) in 142 ms on localhost (13/52)\n",
      "2015-07-02 04:54:41,377 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 90.0 (TID 979)\n",
      "2015-07-02 04:54:41,377 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 90.0 (TID 980, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,376 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 90.0 (TID 977)\n",
      "2015-07-02 04:54:41,377 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 90.0 (TID 975). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,378 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 90.0 (TID 974) in 130 ms on localhost (14/52)\n",
      "2015-07-02 04:54:41,378 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 90.0 (TID 981, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,379 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 90.0 (TID 981)\n",
      "2015-07-02 04:54:41,379 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 90.0 (TID 980)\n",
      "2015-07-02 04:54:41,379 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 90.0 (TID 982, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,380 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 90.0 (TID 982)\n",
      "2015-07-02 04:54:41,380 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 90.0 (TID 973) in 135 ms on localhost (15/52)\n",
      "2015-07-02 04:54:41,380 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 90.0 (TID 975) in 120 ms on localhost (16/52)\n",
      "2015-07-02 04:54:41,380 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,381 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,382 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,382 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,385 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,386 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,388 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,388 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,388 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,389 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,389 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,389 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,401 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -70, init = 118, finish = 0\n",
      "2015-07-02 04:54:41,402 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -59, init = 116, finish = 0\n",
      "2015-07-02 04:54:41,403 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 90.0 (TID 976). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,404 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 90.0 (TID 983, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,405 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 90.0 (TID 983)\n",
      "2015-07-02 04:54:41,405 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 90.0 (TID 976) in 62 ms on localhost (17/52)\n",
      "2015-07-02 04:54:41,408 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,408 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,421 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -60, init = 102, finish = 0\n",
      "2015-07-02 04:54:41,423 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -89, init = 130, finish = 0\n",
      "2015-07-02 04:54:41,428 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -79, init = 123, finish = 0\n",
      "2015-07-02 04:54:41,448 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -65, init = 109, finish = 1\n",
      "2015-07-02 04:54:41,449 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = 4, init = 38, finish = 0\n",
      "2015-07-02 04:54:41,448 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -71, init = 113, finish = 1\n",
      "2015-07-02 04:54:41,448 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -66, init = 112, finish = 1\n",
      "2015-07-02 04:54:41,448 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -62, init = 103, finish = 0\n",
      "2015-07-02 04:54:41,456 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70, boot = -77, init = 147, finish = 0\n",
      "2015-07-02 04:54:41,449 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -60, init = 105, finish = 0\n",
      "2015-07-02 04:54:41,458 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 90.0 (TID 981). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,459 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 90.0 (TID 984, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,460 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 90.0 (TID 981) in 82 ms on localhost (18/52)\n",
      "2015-07-02 04:54:41,460 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 90.0 (TID 984)\n",
      "2015-07-02 04:54:41,460 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 77, boot = -59, init = 136, finish = 0\n",
      "2015-07-02 04:54:41,462 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 90.0 (TID 982). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,463 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 90.0 (TID 985, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,465 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 90.0 (TID 982) in 85 ms on localhost (19/52)\n",
      "2015-07-02 04:54:41,465 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 90.0 (TID 985)\n",
      "2015-07-02 04:54:41,467 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = 2, init = 42, finish = 0\n",
      "2015-07-02 04:54:41,470 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -72, init = 143, finish = 0\n",
      "2015-07-02 04:54:41,471 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 70, boot = -62, init = 132, finish = 0\n",
      "2015-07-02 04:54:41,471 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 90.0 (TID 978). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,472 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 90.0 (TID 986, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,473 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 90.0 (TID 986)\n",
      "2015-07-02 04:54:41,473 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 90.0 (TID 978) in 98 ms on localhost (20/52)\n",
      "2015-07-02 04:54:41,481 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,481 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,482 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,482 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,483 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 90.0 (TID 979). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,484 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 90.0 (TID 987, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,484 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 90.0 (TID 987)\n",
      "2015-07-02 04:54:41,484 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 90.0 (TID 979) in 108 ms on localhost (21/52)\n",
      "2015-07-02 04:54:41,490 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,490 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,498 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,498 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,512 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 90.0 (TID 983). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,513 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 90.0 (TID 977). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,513 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 90.0 (TID 980). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,513 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 90.0 (TID 988, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,514 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 90.0 (TID 988)\n",
      "2015-07-02 04:54:41,514 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 90.0 (TID 989, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,515 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 90.0 (TID 989)\n",
      "2015-07-02 04:54:41,515 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 90.0 (TID 983) in 111 ms on localhost (22/52)\n",
      "2015-07-02 04:54:41,515 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 90.0 (TID 980) in 138 ms on localhost (23/52)\n",
      "2015-07-02 04:54:41,516 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 90.0 (TID 990, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,517 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 90.0 (TID 990)\n",
      "2015-07-02 04:54:41,517 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 90.0 (TID 977) in 142 ms on localhost (24/52)\n",
      "2015-07-02 04:54:41,520 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,520 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,520 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,520 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -32, init = 79, finish = 0\n",
      "2015-07-02 04:54:41,520 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,521 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,521 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = 6, init = 47, finish = 0\n",
      "2015-07-02 04:54:41,522 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,523 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 90.0 (TID 985). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,525 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = 0, init = 49, finish = 0\n",
      "2015-07-02 04:54:41,525 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 90.0 (TID 991, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,525 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 90.0 (TID 991)\n",
      "2015-07-02 04:54:41,525 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 90.0 (TID 985) in 62 ms on localhost (25/52)\n",
      "2015-07-02 04:54:41,530 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -39, init = 87, finish = 0\n",
      "2015-07-02 04:54:41,532 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -19, init = 78, finish = 1\n",
      "2015-07-02 04:54:41,538 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -46, init = 101, finish = 1\n",
      "2015-07-02 04:54:41,539 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -47, init = 93, finish = 0\n",
      "2015-07-02 04:54:41,540 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -49, init = 103, finish = 0\n",
      "2015-07-02 04:54:41,541 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 90.0 (TID 987). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,542 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 90.0 (TID 992, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,543 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 90.0 (TID 987) in 60 ms on localhost (26/52)\n",
      "2015-07-02 04:54:41,543 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 90.0 (TID 992)\n",
      "2015-07-02 04:54:41,544 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,545 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,553 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,553 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,560 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -54, init = 97, finish = 0\n",
      "2015-07-02 04:54:41,560 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -48, init = 90, finish = 0\n",
      "2015-07-02 04:54:41,562 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -50, init = 94, finish = 0\n",
      "2015-07-02 04:54:41,563 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 90.0 (TID 988). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,565 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 90.0 (TID 993, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,566 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -55, init = 96, finish = 1\n",
      "2015-07-02 04:54:41,566 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 90.0 (TID 988) in 53 ms on localhost (27/52)\n",
      "2015-07-02 04:54:41,566 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -69, init = 114, finish = 0\n",
      "2015-07-02 04:54:41,567 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 90.0 (TID 993)\n",
      "2015-07-02 04:54:41,573 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -71, init = 120, finish = 0\n",
      "2015-07-02 04:54:41,578 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 90.0 (TID 984). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,579 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 90.0 (TID 994, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,579 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 90.0 (TID 994)\n",
      "2015-07-02 04:54:41,579 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 90.0 (TID 984) in 120 ms on localhost (28/52)\n",
      "2015-07-02 04:54:41,579 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 90.0 (TID 986). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,580 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 90.0 (TID 995, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,581 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 90.0 (TID 995)\n",
      "2015-07-02 04:54:41,581 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 90.0 (TID 986) in 108 ms on localhost (29/52)\n",
      "2015-07-02 04:54:41,585 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = 0, init = 52, finish = 0\n",
      "2015-07-02 04:54:41,588 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,588 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,591 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = 7, init = 48, finish = 0\n",
      "2015-07-02 04:54:41,592 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,593 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,595 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -1, init = 44, finish = 1\n",
      "2015-07-02 04:54:41,600 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,600 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,602 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = 6, init = 45, finish = 1\n",
      "2015-07-02 04:54:41,621 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 90.0 (TID 989). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,623 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 90.0 (TID 996, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,623 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 90.0 (TID 990). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,623 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 90.0 (TID 996)\n",
      "2015-07-02 04:54:41,624 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 90.0 (TID 997, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,624 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 90.0 (TID 997)\n",
      "2015-07-02 04:54:41,624 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 90.0 (TID 989) in 110 ms on localhost (30/52)\n",
      "2015-07-02 04:54:41,625 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 90.0 (TID 990) in 108 ms on localhost (31/52)\n",
      "2015-07-02 04:54:41,627 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,627 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,628 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,628 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,628 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 90.0 (TID 991). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,629 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 90.0 (TID 992). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,629 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -42, init = 90, finish = 0\n",
      "2015-07-02 04:54:41,629 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 90.0 (TID 998, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,630 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 90.0 (TID 991) in 104 ms on localhost (32/52)\n",
      "2015-07-02 04:54:41,630 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 90.0 (TID 998)\n",
      "2015-07-02 04:54:41,630 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 90.0 (TID 999, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,631 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 90.0 (TID 992) in 89 ms on localhost (33/52)\n",
      "2015-07-02 04:54:41,632 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 90.0 (TID 999)\n",
      "2015-07-02 04:54:41,632 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = 7, init = 54, finish = 0\n",
      "2015-07-02 04:54:41,633 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 90.0 (TID 993). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,634 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 90.0 (TID 1000, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,634 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 90.0 (TID 993) in 69 ms on localhost (34/52)\n",
      "2015-07-02 04:54:41,635 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 90.0 (TID 1000)\n",
      "2015-07-02 04:54:41,636 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -39, init = 86, finish = 0\n",
      "2015-07-02 04:54:41,637 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 56, boot = -9, init = 64, finish = 1\n",
      "2015-07-02 04:54:41,639 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 90.0 (TID 994). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,640 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 90.0 (TID 1001, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,641 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 90.0 (TID 994) in 63 ms on localhost (35/52)\n",
      "2015-07-02 04:54:41,641 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -42, init = 93, finish = 1\n",
      "2015-07-02 04:54:41,641 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 90.0 (TID 1001)\n",
      "2015-07-02 04:54:41,642 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 60, boot = -44, init = 104, finish = 0\n",
      "2015-07-02 04:54:41,643 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,643 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,643 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 90.0 (TID 995). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,642 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,644 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 90.0 (TID 1002, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,645 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 90.0 (TID 1002)\n",
      "2015-07-02 04:54:41,645 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 90.0 (TID 995) in 65 ms on localhost (36/52)\n",
      "2015-07-02 04:54:41,644 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:41,650 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,651 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,652 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,653 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,653 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,653 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,668 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -47, init = 88, finish = 0\n",
      "2015-07-02 04:54:41,669 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -51, init = 93, finish = 0\n",
      "2015-07-02 04:54:41,674 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -47, init = 91, finish = 0\n",
      "2015-07-02 04:54:41,674 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -48, init = 91, finish = 0\n",
      "2015-07-02 04:54:41,683 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -29, init = 76, finish = 0\n",
      "2015-07-02 04:54:41,684 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -28, init = 77, finish = 0\n",
      "2015-07-02 04:54:41,686 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -36, init = 87, finish = 0\n",
      "2015-07-02 04:54:41,687 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 90.0 (TID 999). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,689 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 90.0 (TID 1003, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,690 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 90.0 (TID 1003)\n",
      "2015-07-02 04:54:41,690 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -36, init = 87, finish = 1\n",
      "2015-07-02 04:54:41,693 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = 0, init = 45, finish = 1\n",
      "2015-07-02 04:54:41,693 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = 10, init = 37, finish = 0\n",
      "2015-07-02 04:54:41,695 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = 4, init = 53, finish = 0\n",
      "2015-07-02 04:54:41,696 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 90.0 (TID 999) in 66 ms on localhost (37/52)\n",
      "2015-07-02 04:54:41,698 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = 2, init = 41, finish = 0\n",
      "2015-07-02 04:54:41,702 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -6, init = 57, finish = 0\n",
      "2015-07-02 04:54:41,708 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = 4, init = 48, finish = 0\n",
      "2015-07-02 04:54:41,714 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 90.0 (TID 996). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,714 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 90.0 (TID 997). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,714 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 90.0 (TID 1000). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,715 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 90.0 (TID 1004, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,716 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 90.0 (TID 1004)\n",
      "2015-07-02 04:54:41,716 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 90.0 (TID 1005, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,717 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 90.0 (TID 1005)\n",
      "2015-07-02 04:54:41,717 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 90.0 (TID 1006, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,718 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 90.0 (TID 1006)\n",
      "2015-07-02 04:54:41,718 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 90.0 (TID 996) in 96 ms on localhost (38/52)\n",
      "2015-07-02 04:54:41,719 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 90.0 (TID 1000) in 85 ms on localhost (39/52)\n",
      "2015-07-02 04:54:41,719 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 90.0 (TID 997) in 96 ms on localhost (40/52)\n",
      "2015-07-02 04:54:41,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,732 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,734 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,734 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,737 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,737 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,742 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 90.0 (TID 1001). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,742 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 90.0 (TID 1007, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,743 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 90.0 (TID 1007)\n",
      "2015-07-02 04:54:41,743 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 90.0 (TID 1001) in 103 ms on localhost (41/52)\n",
      "2015-07-02 04:54:41,751 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,751 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,753 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 90.0 (TID 998). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,753 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 90.0 (TID 1002). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,753 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 90.0 (TID 1008, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,754 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 90.0 (TID 1008)\n",
      "2015-07-02 04:54:41,754 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 90.0 (TID 1009, localhost, PROCESS_LOCAL, 1154 bytes)\n",
      "2015-07-02 04:54:41,754 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 90.0 (TID 1009)\n",
      "2015-07-02 04:54:41,754 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 90.0 (TID 998) in 125 ms on localhost (42/52)\n",
      "2015-07-02 04:54:41,755 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 90.0 (TID 1002) in 111 ms on localhost (43/52)\n",
      "2015-07-02 04:54:41,757 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,757 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,758 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 50 blocks\n",
      "2015-07-02 04:54:41,758 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,771 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -45, init = 92, finish = 0\n",
      "2015-07-02 04:54:41,771 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -11, init = 55, finish = 0\n",
      "2015-07-02 04:54:41,772 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -39, init = 93, finish = 0\n",
      "2015-07-02 04:54:41,774 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 90.0 (TID 1005). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,776 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 90.0 (TID 1010, localhost, PROCESS_LOCAL, 1402 bytes)\n",
      "2015-07-02 04:54:41,777 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 90.0 (TID 1005) in 61 ms on localhost (44/52)\n",
      "2015-07-02 04:54:41,778 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -45, init = 93, finish = 1\n",
      "2015-07-02 04:54:41,778 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 90.0 (TID 1010)\n",
      "2015-07-02 04:54:41,779 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -5, init = 59, finish = 0\n",
      "2015-07-02 04:54:41,784 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -12, init = 60, finish = 0\n",
      "2015-07-02 04:54:41,789 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65, boot = -39, init = 104, finish = 0\n",
      "2015-07-02 04:54:41,789 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 90, boot = 14, init = 75, finish = 1\n",
      "2015-07-02 04:54:41,791 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 90.0 (TID 1006). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,792 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 90.0 (TID 1011, localhost, PROCESS_LOCAL, 1402 bytes)\n",
      "2015-07-02 04:54:41,793 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 90.0 (TID 1011)\n",
      "2015-07-02 04:54:41,795 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 90.0 (TID 1006) in 78 ms on localhost (45/52)\n",
      "2015-07-02 04:54:41,795 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -35, init = 80, finish = 1\n",
      "2015-07-02 04:54:41,799 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -46, init = 88, finish = 1\n",
      "2015-07-02 04:54:41,800 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -38, init = 79, finish = 1\n",
      "2015-07-02 04:54:41,808 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -45, init = 89, finish = 1\n",
      "2015-07-02 04:54:41,809 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -40, init = 85, finish = 0\n",
      "2015-07-02 04:54:41,813 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -25, init = 78, finish = 0\n",
      "2015-07-02 04:54:41,822 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/gender_labels.csv:0+996\n",
      "2015-07-02 04:54:41,827 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/input/gender_labels.csv:996+997\n",
      "2015-07-02 04:54:41,850 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 90.0 (TID 1004). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,851 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 90.0 (TID 1004) in 136 ms on localhost (46/52)\n",
      "2015-07-02 04:54:41,859 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 90.0 (TID 1003). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,860 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 90.0 (TID 1003) in 172 ms on localhost (47/52)\n",
      "2015-07-02 04:54:41,860 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 90.0 (TID 1008). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,861 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 90.0 (TID 1008) in 108 ms on localhost (48/52)\n",
      "2015-07-02 04:54:41,861 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 90.0 (TID 1009). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,861 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 90.0 (TID 1007). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,862 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 90.0 (TID 1009) in 108 ms on localhost (49/52)\n",
      "2015-07-02 04:54:41,863 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 90.0 (TID 1007) in 120 ms on localhost (50/52)\n",
      "2015-07-02 04:54:41,864 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -16, init = 68, finish = 1\n",
      "2015-07-02 04:54:41,880 INFO  [stdout writer for python] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -22, init = 68, finish = 1\n",
      "2015-07-02 04:54:41,884 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 83, boot = 2, init = 81, finish = 0\n",
      "2015-07-02 04:54:41,889 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 84, boot = -7, init = 90, finish = 1\n",
      "2015-07-02 04:54:41,893 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 90.0 (TID 1010). 2008 bytes result sent to driver\n",
      "2015-07-02 04:54:41,894 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 90.0 (TID 1010) in 119 ms on localhost (51/52)\n",
      "2015-07-02 04:54:41,898 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 90.0 (TID 1011). 2008 bytes result sent to driver\n",
      "2015-07-02 04:54:41,898 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 90.0 (TID 1011) in 106 ms on localhost (52/52)\n",
      "2015-07-02 04:54:41,899 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 90 (join at <ipython-input-3-4ba038715317>:131) finished in 0.822 s\n",
      "2015-07-02 04:54:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 91, Stage 92)\n",
      "2015-07-02 04:54:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:41,901 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 91: List()\n",
      "2015-07-02 04:54:41,902 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List(Stage 91)\n",
      "2015-07-02 04:54:41,902 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 91 (PairwiseRDD[343] at reduceByKey at <ipython-input-3-4ba038715317>:139), which is now runnable\n",
      "2015-07-02 04:54:41,903 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9984) called with curMem=3448293, maxMem=278302556\n",
      "2015-07-02 04:54:41,903 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_63 stored as values in memory (estimated size 9.8 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:41,904 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6216) called with curMem=3458277, maxMem=278302556\n",
      "2015-07-02 04:54:41,904 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_63_piece0 stored as bytes in memory (estimated size 6.1 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:41,904 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_63_piece0 in memory on localhost:40918 (size: 6.1 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:41,905 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_63_piece0\n",
      "2015-07-02 04:54:41,905 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 63 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:41,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 91 (PairwiseRDD[343] at reduceByKey at <ipython-input-3-4ba038715317>:139)\n",
      "2015-07-02 04:54:41,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 91.0 with 52 tasks\n",
      "2015-07-02 04:54:41,909 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 91.0 (TID 1012, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,909 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 91.0 (TID 1013, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,909 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 91.0 (TID 1014, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,910 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 91.0 (TID 1015, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,910 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 91.0 (TID 1016, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,910 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 91.0 (TID 1017, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,910 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 91.0 (TID 1018, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,911 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 91.0 (TID 1014)\n",
      "2015-07-02 04:54:41,911 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 91.0 (TID 1016)\n",
      "2015-07-02 04:54:41,911 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 91.0 (TID 1015)\n",
      "2015-07-02 04:54:41,911 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 91.0 (TID 1018)\n",
      "2015-07-02 04:54:41,911 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 91.0 (TID 1017)\n",
      "2015-07-02 04:54:41,912 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 91.0 (TID 1012)\n",
      "2015-07-02 04:54:41,912 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 91.0 (TID 1013)\n",
      "2015-07-02 04:54:41,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,914 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:41,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,914 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,914 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,920 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,920 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,953 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -126, init = 168, finish = 0\n",
      "2015-07-02 04:54:41,954 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -98, init = 139, finish = 0\n",
      "2015-07-02 04:54:41,954 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -124, init = 165, finish = 0\n",
      "2015-07-02 04:54:41,957 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -96, init = 138, finish = 0\n",
      "2015-07-02 04:54:41,959 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -118, init = 164, finish = 0\n",
      "2015-07-02 04:54:41,968 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -97, init = 152, finish = 0\n",
      "2015-07-02 04:54:41,969 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 91.0 (TID 1017). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,969 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -117, init = 169, finish = 1\n",
      "2015-07-02 04:54:41,970 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 91.0 (TID 1013). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,971 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 91.0 (TID 1019, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,972 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 91.0 (TID 1020, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,973 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 91.0 (TID 1020)\n",
      "2015-07-02 04:54:41,975 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 91.0 (TID 1017) in 65 ms on localhost (1/52)\n",
      "2015-07-02 04:54:41,975 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 91.0 (TID 1019)\n",
      "2015-07-02 04:54:41,978 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 91.0 (TID 1013) in 69 ms on localhost (2/52)\n",
      "2015-07-02 04:54:41,986 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 91.0 (TID 1014). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:41,986 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 91.0 (TID 1021, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:41,987 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 91.0 (TID 1021)\n",
      "2015-07-02 04:54:41,987 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 91.0 (TID 1014) in 78 ms on localhost (3/52)\n",
      "2015-07-02 04:54:41,991 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,991 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,993 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,993 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:41,998 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:41,998 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,028 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 91.0 (TID 1018). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,029 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 91.0 (TID 1022, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,029 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 91.0 (TID 1015). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,030 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 91.0 (TID 1022)\n",
      "2015-07-02 04:54:42,030 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 91.0 (TID 1012). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,030 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 91.0 (TID 1023, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,030 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 91.0 (TID 1023)\n",
      "2015-07-02 04:54:42,030 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 91.0 (TID 1018) in 120 ms on localhost (4/52)\n",
      "2015-07-02 04:54:42,030 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 91.0 (TID 1016). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,031 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 91.0 (TID 1024, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,032 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 91.0 (TID 1015) in 121 ms on localhost (5/52)\n",
      "2015-07-02 04:54:42,032 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 91.0 (TID 1024)\n",
      "2015-07-02 04:54:42,032 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -167, init = 218, finish = 0\n",
      "2015-07-02 04:54:42,032 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 91.0 (TID 1025, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,033 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 91.0 (TID 1016) in 122 ms on localhost (6/52)\n",
      "2015-07-02 04:54:42,033 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 91.0 (TID 1025)\n",
      "2015-07-02 04:54:42,033 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 91.0 (TID 1012) in 124 ms on localhost (7/52)\n",
      "2015-07-02 04:54:42,034 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,035 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,034 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,036 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,036 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,036 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,039 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,040 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,039 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -174, init = 224, finish = 1\n",
      "2015-07-02 04:54:42,041 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -169, init = 221, finish = 0\n",
      "2015-07-02 04:54:42,050 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 91.0 (TID 1019). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,051 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 91.0 (TID 1026, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,051 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 91.0 (TID 1026)\n",
      "2015-07-02 04:54:42,051 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 91.0 (TID 1019) in 80 ms on localhost (8/52)\n",
      "2015-07-02 04:54:42,057 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,057 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,063 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 91.0 (TID 1021). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,064 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 91.0 (TID 1027, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,064 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 91.0 (TID 1020). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,064 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 91.0 (TID 1021) in 78 ms on localhost (9/52)\n",
      "2015-07-02 04:54:42,064 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 91.0 (TID 1027)\n",
      "2015-07-02 04:54:42,065 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 91.0 (TID 1028, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,065 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 91.0 (TID 1028)\n",
      "2015-07-02 04:54:42,065 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 91.0 (TID 1020) in 93 ms on localhost (10/52)\n",
      "2015-07-02 04:54:42,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,076 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -155, init = 198, finish = 0\n",
      "2015-07-02 04:54:42,076 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -157, init = 202, finish = 0\n",
      "2015-07-02 04:54:42,077 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -156, init = 198, finish = 1\n",
      "2015-07-02 04:54:42,080 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -145, init = 189, finish = 1\n",
      "2015-07-02 04:54:42,097 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -84, init = 128, finish = 0\n",
      "2015-07-02 04:54:42,098 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 91.0 (TID 1026). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,099 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 91.0 (TID 1029, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,100 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 91.0 (TID 1026) in 49 ms on localhost (11/52)\n",
      "2015-07-02 04:54:42,100 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 91.0 (TID 1029)\n",
      "2015-07-02 04:54:42,106 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 91.0 (TID 1022). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,106 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 91.0 (TID 1030, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,107 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -98, init = 138, finish = 0\n",
      "2015-07-02 04:54:42,107 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 91.0 (TID 1030)\n",
      "2015-07-02 04:54:42,107 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 91.0 (TID 1022) in 78 ms on localhost (12/52)\n",
      "2015-07-02 04:54:42,108 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -81, init = 122, finish = 1\n",
      "2015-07-02 04:54:42,110 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,110 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,116 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,117 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,138 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 91.0 (TID 1024). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,139 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 91.0 (TID 1031, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,139 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 91.0 (TID 1024) in 108 ms on localhost (13/52)\n",
      "2015-07-02 04:54:42,140 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 91.0 (TID 1031)\n",
      "2015-07-02 04:54:42,140 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 91.0 (TID 1023). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,141 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 91.0 (TID 1032, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,141 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 91.0 (TID 1032)\n",
      "2015-07-02 04:54:42,141 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 91.0 (TID 1023) in 111 ms on localhost (14/52)\n",
      "2015-07-02 04:54:42,142 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 91.0 (TID 1025). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,143 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 91.0 (TID 1033, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,143 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 91.0 (TID 1033)\n",
      "2015-07-02 04:54:42,143 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 91.0 (TID 1025) in 111 ms on localhost (15/52)\n",
      "2015-07-02 04:54:42,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,147 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,147 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,148 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,148 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,150 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 91.0 (TID 1027). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,151 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 91.0 (TID 1034, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,151 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 91.0 (TID 1034)\n",
      "2015-07-02 04:54:42,151 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 91.0 (TID 1027) in 87 ms on localhost (16/52)\n",
      "2015-07-02 04:54:42,152 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -131, init = 180, finish = 0\n",
      "2015-07-02 04:54:42,152 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 91.0 (TID 1028). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,153 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 91.0 (TID 1035, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,153 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 91.0 (TID 1035)\n",
      "2015-07-02 04:54:42,154 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 91.0 (TID 1028) in 88 ms on localhost (17/52)\n",
      "2015-07-02 04:54:42,154 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,154 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,157 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,157 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,158 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -132, init = 180, finish = 0\n",
      "2015-07-02 04:54:42,170 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 91.0 (TID 1029). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,170 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 91.0 (TID 1036, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,171 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 91.0 (TID 1036)\n",
      "2015-07-02 04:54:42,171 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 91.0 (TID 1029) in 72 ms on localhost (18/52)\n",
      "2015-07-02 04:54:42,172 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 91.0 (TID 1030). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,172 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 91.0 (TID 1037, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,173 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 91.0 (TID 1037)\n",
      "2015-07-02 04:54:42,173 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 91.0 (TID 1030) in 67 ms on localhost (19/52)\n",
      "2015-07-02 04:54:42,173 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,173 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,175 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,175 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,188 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -172, init = 216, finish = 1\n",
      "2015-07-02 04:54:42,188 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -159, init = 203, finish = 1\n",
      "2015-07-02 04:54:42,190 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -100, init = 145, finish = 0\n",
      "2015-07-02 04:54:42,210 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -105, init = 150, finish = 0\n",
      "2015-07-02 04:54:42,210 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -107, init = 150, finish = 0\n",
      "2015-07-02 04:54:42,216 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -67, init = 109, finish = 1\n",
      "2015-07-02 04:54:42,218 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -85, init = 128, finish = 0\n",
      "2015-07-02 04:54:42,252 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 91.0 (TID 1031). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,252 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 91.0 (TID 1038, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,253 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 91.0 (TID 1038)\n",
      "2015-07-02 04:54:42,253 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 91.0 (TID 1031) in 115 ms on localhost (20/52)\n",
      "2015-07-02 04:54:42,262 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,262 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,272 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 91.0 (TID 1032). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,273 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 91.0 (TID 1039, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,274 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 91.0 (TID 1039)\n",
      "2015-07-02 04:54:42,274 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 91.0 (TID 1032) in 133 ms on localhost (21/52)\n",
      "2015-07-02 04:54:42,275 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 91.0 (TID 1033). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,276 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 91.0 (TID 1040, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,276 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 91.0 (TID 1040)\n",
      "2015-07-02 04:54:42,276 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 91.0 (TID 1033) in 133 ms on localhost (22/52)\n",
      "2015-07-02 04:54:42,280 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,280 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,280 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,281 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,281 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 91.0 (TID 1035). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,282 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 91.0 (TID 1034). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,282 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 91.0 (TID 1041, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,283 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 91.0 (TID 1041)\n",
      "2015-07-02 04:54:42,283 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 91.0 (TID 1042, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,283 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 91.0 (TID 1042)\n",
      "2015-07-02 04:54:42,283 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 91.0 (TID 1036). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,284 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 91.0 (TID 1035) in 131 ms on localhost (23/52)\n",
      "2015-07-02 04:54:42,284 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 91.0 (TID 1034) in 134 ms on localhost (24/52)\n",
      "2015-07-02 04:54:42,285 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 91.0 (TID 1043, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,285 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 91.0 (TID 1043)\n",
      "2015-07-02 04:54:42,285 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 91.0 (TID 1036) in 115 ms on localhost (25/52)\n",
      "2015-07-02 04:54:42,286 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 91.0 (TID 1037). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,286 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,286 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,286 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,286 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 91.0 (TID 1044, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,287 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,287 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 91.0 (TID 1037) in 115 ms on localhost (26/52)\n",
      "2015-07-02 04:54:42,287 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 91.0 (TID 1044)\n",
      "2015-07-02 04:54:42,288 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,288 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,289 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,289 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,303 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -165, init = 213, finish = 0\n",
      "2015-07-02 04:54:42,312 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 91.0 (TID 1038). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,313 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 91.0 (TID 1045, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,313 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 91.0 (TID 1045)\n",
      "2015-07-02 04:54:42,314 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 91.0 (TID 1038) in 61 ms on localhost (27/52)\n",
      "2015-07-02 04:54:42,315 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,315 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,321 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -187, init = 232, finish = 1\n",
      "2015-07-02 04:54:42,322 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -186, init = 230, finish = 0\n",
      "2015-07-02 04:54:42,328 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -169, init = 211, finish = 1\n",
      "2015-07-02 04:54:42,333 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -166, init = 208, finish = 2\n",
      "2015-07-02 04:54:42,333 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -120, init = 163, finish = 0\n",
      "2015-07-02 04:54:42,333 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -125, init = 169, finish = 0\n",
      "2015-07-02 04:54:42,357 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -110, init = 151, finish = 0\n",
      "2015-07-02 04:54:42,409 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 91.0 (TID 1039). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,410 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 91.0 (TID 1040). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,410 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 91.0 (TID 1046, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,410 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 91.0 (TID 1047, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,411 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 91.0 (TID 1046)\n",
      "2015-07-02 04:54:42,411 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 91.0 (TID 1047)\n",
      "2015-07-02 04:54:42,411 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 91.0 (TID 1041). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,411 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 91.0 (TID 1040) in 134 ms on localhost (28/52)\n",
      "2015-07-02 04:54:42,412 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 91.0 (TID 1039) in 139 ms on localhost (29/52)\n",
      "2015-07-02 04:54:42,412 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 91.0 (TID 1048, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,413 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 91.0 (TID 1048)\n",
      "2015-07-02 04:54:42,413 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 91.0 (TID 1041) in 131 ms on localhost (30/52)\n",
      "2015-07-02 04:54:42,414 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 91.0 (TID 1042). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,415 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 91.0 (TID 1049, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,415 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 91.0 (TID 1049)\n",
      "2015-07-02 04:54:42,417 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 91.0 (TID 1042) in 132 ms on localhost (31/52)\n",
      "2015-07-02 04:54:42,418 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 91.0 (TID 1043). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,418 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 91.0 (TID 1044). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,418 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 91.0 (TID 1050, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,419 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 91.0 (TID 1051, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,419 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 91.0 (TID 1050)\n",
      "2015-07-02 04:54:42,419 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 91.0 (TID 1051)\n",
      "2015-07-02 04:54:42,419 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 91.0 (TID 1043) in 135 ms on localhost (32/52)\n",
      "2015-07-02 04:54:42,420 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 91.0 (TID 1044) in 134 ms on localhost (33/52)\n",
      "2015-07-02 04:54:42,424 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,424 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,424 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,424 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,425 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,425 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,427 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,427 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,428 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,428 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,429 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,429 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,431 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 91.0 (TID 1045). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,431 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 91.0 (TID 1052, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,432 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 91.0 (TID 1052)\n",
      "2015-07-02 04:54:42,432 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 91.0 (TID 1045) in 120 ms on localhost (34/52)\n",
      "2015-07-02 04:54:42,433 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,433 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,465 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -192, init = 236, finish = 1\n",
      "2015-07-02 04:54:42,469 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -204, init = 256, finish = 1\n",
      "2015-07-02 04:54:42,471 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -210, init = 261, finish = 0\n",
      "2015-07-02 04:54:42,475 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -205, init = 261, finish = 1\n",
      "2015-07-02 04:54:42,475 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -192, init = 242, finish = 1\n",
      "2015-07-02 04:54:42,476 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -206, init = 259, finish = 0\n",
      "2015-07-02 04:54:42,481 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -123, init = 169, finish = 0\n",
      "2015-07-02 04:54:42,652 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 91.0 (TID 1047). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,653 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 91.0 (TID 1048). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,653 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 91.0 (TID 1046). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,653 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 91.0 (TID 1053, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,654 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 91.0 (TID 1047) in 244 ms on localhost (35/52)\n",
      "2015-07-02 04:54:42,652 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 91.0 (TID 1050). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,654 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 91.0 (TID 1053)\n",
      "2015-07-02 04:54:42,654 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 91.0 (TID 1054, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,655 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 91.0 (TID 1052). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,655 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 91.0 (TID 1055, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,655 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 91.0 (TID 1049). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,656 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 91.0 (TID 1055)\n",
      "2015-07-02 04:54:42,656 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 91.0 (TID 1046) in 245 ms on localhost (36/52)\n",
      "2015-07-02 04:54:42,656 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 91.0 (TID 1048) in 244 ms on localhost (37/52)\n",
      "2015-07-02 04:54:42,656 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 91.0 (TID 1054)\n",
      "2015-07-02 04:54:42,657 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 91.0 (TID 1056, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,657 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 91.0 (TID 1050) in 239 ms on localhost (38/52)\n",
      "2015-07-02 04:54:42,657 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 91.0 (TID 1056)\n",
      "2015-07-02 04:54:42,657 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 91.0 (TID 1057, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,658 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 91.0 (TID 1057)\n",
      "2015-07-02 04:54:42,658 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 91.0 (TID 1058, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,658 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 91.0 (TID 1058)\n",
      "2015-07-02 04:54:42,658 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 91.0 (TID 1049) in 243 ms on localhost (39/52)\n",
      "2015-07-02 04:54:42,659 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 91.0 (TID 1052) in 227 ms on localhost (40/52)\n",
      "2015-07-02 04:54:42,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,660 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 91.0 (TID 1051). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,661 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 91.0 (TID 1059, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,661 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 91.0 (TID 1059)\n",
      "2015-07-02 04:54:42,661 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 91.0 (TID 1051) in 242 ms on localhost (41/52)\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,665 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,664 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,665 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,665 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,665 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,665 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,666 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,700 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -327, init = 371, finish = 0\n",
      "2015-07-02 04:54:42,705 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -309, init = 355, finish = 0\n",
      "2015-07-02 04:54:42,705 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -320, init = 366, finish = 1\n",
      "2015-07-02 04:54:42,705 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -299, init = 341, finish = 0\n",
      "2015-07-02 04:54:42,729 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -318, init = 366, finish = 0\n",
      "2015-07-02 04:54:42,729 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -317, init = 368, finish = 0\n",
      "2015-07-02 04:54:42,729 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -321, init = 374, finish = 0\n",
      "2015-07-02 04:54:42,758 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 91.0 (TID 1053). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 91.0 (TID 1060, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,759 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 91.0 (TID 1060)\n",
      "2015-07-02 04:54:42,760 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 91.0 (TID 1053) in 106 ms on localhost (42/52)\n",
      "2015-07-02 04:54:42,768 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,768 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,794 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 91.0 (TID 1054). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,794 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 91.0 (TID 1059). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,794 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 91.0 (TID 1061, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,795 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 91.0 (TID 1054) in 141 ms on localhost (43/52)\n",
      "2015-07-02 04:54:42,795 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 91.0 (TID 1061)\n",
      "2015-07-02 04:54:42,795 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 91.0 (TID 1062, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,796 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 91.0 (TID 1062)\n",
      "2015-07-02 04:54:42,796 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 91.0 (TID 1059) in 136 ms on localhost (44/52)\n",
      "2015-07-02 04:54:42,800 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,800 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,800 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,800 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,801 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 91.0 (TID 1057). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,802 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 91.0 (TID 1063, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:42,802 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 91.0 (TID 1063)\n",
      "2015-07-02 04:54:42,803 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 91.0 (TID 1057) in 146 ms on localhost (45/52)\n",
      "2015-07-02 04:54:42,803 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 91.0 (TID 1055). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,804 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 91.0 (TID 1055) in 149 ms on localhost (46/52)\n",
      "2015-07-02 04:54:42,805 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 91.0 (TID 1058). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,805 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,805 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,806 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 91.0 (TID 1056). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,806 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 91.0 (TID 1058) in 147 ms on localhost (47/52)\n",
      "2015-07-02 04:54:42,806 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 91.0 (TID 1056) in 150 ms on localhost (48/52)\n",
      "2015-07-02 04:54:42,809 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -280, init = 327, finish = 0\n",
      "2015-07-02 04:54:42,817 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 91.0 (TID 1060). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,818 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 91.0 (TID 1060) in 59 ms on localhost (49/52)\n",
      "2015-07-02 04:54:42,846 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -310, init = 357, finish = 0\n",
      "2015-07-02 04:54:42,846 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -327, init = 369, finish = 0\n",
      "2015-07-02 04:54:42,848 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -313, init = 362, finish = 0\n",
      "2015-07-02 04:54:42,881 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 91.0 (TID 1063). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,882 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 91.0 (TID 1063) in 81 ms on localhost (50/52)\n",
      "2015-07-02 04:54:42,883 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 91.0 (TID 1061). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,883 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 91.0 (TID 1062). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:42,884 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 91.0 (TID 1061) in 90 ms on localhost (51/52)\n",
      "2015-07-02 04:54:42,885 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 91.0 (TID 1062) in 89 ms on localhost (52/52)\n",
      "2015-07-02 04:54:42,885 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:42,885 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 91 (reduceByKey at <ipython-input-3-4ba038715317>:139) finished in 0.977 s\n",
      "2015-07-02 04:54:42,885 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:42,885 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:42,885 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 92)\n",
      "2015-07-02 04:54:42,885 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:42,886 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 92: List()\n",
      "2015-07-02 04:54:42,887 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 92 (PythonRDD[346] at collect at <ipython-input-3-4ba038715317>:139), which is now runnable\n",
      "2015-07-02 04:54:42,887 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4752) called with curMem=3464493, maxMem=278302556\n",
      "2015-07-02 04:54:42,888 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_64 stored as values in memory (estimated size 4.6 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:42,888 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(2985) called with curMem=3469245, maxMem=278302556\n",
      "2015-07-02 04:54:42,889 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_64_piece0 stored as bytes in memory (estimated size 2.9 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:42,889 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_64_piece0 in memory on localhost:40918 (size: 2.9 KB, free: 265.0 MB)\n",
      "2015-07-02 04:54:42,889 INFO  [sparkDriver-akka.actor.default-dispatcher-18] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_64_piece0\n",
      "2015-07-02 04:54:42,890 INFO  [sparkDriver-akka.actor.default-dispatcher-18] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 64 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:42,891 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 92 (PythonRDD[346] at collect at <ipython-input-3-4ba038715317>:139)\n",
      "2015-07-02 04:54:42,891 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 92.0 with 52 tasks\n",
      "2015-07-02 04:54:42,892 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 92.0 (TID 1064, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,892 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 92.0 (TID 1065, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,893 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 92.0 (TID 1066, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,893 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 92.0 (TID 1067, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,893 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 92.0 (TID 1068, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,894 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 92.0 (TID 1069, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,894 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 92.0 (TID 1070, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 92.0 (TID 1064)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 92.0 (TID 1070)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 92.0 (TID 1066)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 92.0 (TID 1067)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 92.0 (TID 1065)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 92.0 (TID 1068)\n",
      "2015-07-02 04:54:42,894 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 92.0 (TID 1069)\n",
      "2015-07-02 04:54:42,896 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,897 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,897 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,897 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,898 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,898 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,898 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,898 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,899 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,899 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,902 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,902 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,937 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -404, init = 444, finish = 0\n",
      "2015-07-02 04:54:42,937 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 92.0 (TID 1064). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,938 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -187, init = 228, finish = 1\n",
      "2015-07-02 04:54:42,938 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 92.0 (TID 1071, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,938 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 92.0 (TID 1066). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,939 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -411, init = 453, finish = 0\n",
      "2015-07-02 04:54:42,939 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 92.0 (TID 1072, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,940 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 92.0 (TID 1067). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,940 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 92.0 (TID 1064) in 48 ms on localhost (1/52)\n",
      "2015-07-02 04:54:42,940 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 92.0 (TID 1072)\n",
      "2015-07-02 04:54:42,940 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 92.0 (TID 1071)\n",
      "2015-07-02 04:54:42,940 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 92.0 (TID 1073, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,951 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 92.0 (TID 1066) in 58 ms on localhost (2/52)\n",
      "2015-07-02 04:54:42,952 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 92.0 (TID 1067) in 59 ms on localhost (3/52)\n",
      "2015-07-02 04:54:42,952 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 92.0 (TID 1073)\n",
      "2015-07-02 04:54:42,952 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -180, init = 225, finish = 2\n",
      "2015-07-02 04:54:42,952 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -173, init = 218, finish = 0\n",
      "2015-07-02 04:54:42,953 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 92.0 (TID 1065). 986 bytes result sent to driver\n",
      "2015-07-02 04:54:42,954 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 92.0 (TID 1074, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,954 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 92.0 (TID 1074)\n",
      "2015-07-02 04:54:42,954 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 92.0 (TID 1065) in 62 ms on localhost (4/52)\n",
      "2015-07-02 04:54:42,955 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,952 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,955 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,953 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 92.0 (TID 1068). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,952 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -171, init = 213, finish = 0\n",
      "2015-07-02 04:54:42,952 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -412, init = 458, finish = 0\n",
      "2015-07-02 04:54:42,956 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 92.0 (TID 1075, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,955 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 3 ms\n",
      "2015-07-02 04:54:42,957 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 92.0 (TID 1075)\n",
      "2015-07-02 04:54:42,957 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 92.0 (TID 1068) in 64 ms on localhost (5/52)\n",
      "2015-07-02 04:54:42,959 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,960 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,960 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 92.0 (TID 1070). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,960 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,961 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,960 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,961 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 92.0 (TID 1069). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,961 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 92.0 (TID 1076, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,962 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 92.0 (TID 1070) in 68 ms on localhost (6/52)\n",
      "2015-07-02 04:54:42,962 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 92.0 (TID 1077, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,961 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,963 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 92.0 (TID 1077)\n",
      "2015-07-02 04:54:42,962 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 92.0 (TID 1076)\n",
      "2015-07-02 04:54:42,963 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 92.0 (TID 1069) in 70 ms on localhost (7/52)\n",
      "2015-07-02 04:54:42,965 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,966 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,966 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,966 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,991 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -229, init = 270, finish = 0\n",
      "2015-07-02 04:54:42,992 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 92.0 (TID 1071). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,992 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 92.0 (TID 1078, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,992 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 92.0 (TID 1078)\n",
      "2015-07-02 04:54:42,993 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 92.0 (TID 1071) in 54 ms on localhost (8/52)\n",
      "2015-07-02 04:54:42,995 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,995 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -232, init = 273, finish = 0\n",
      "2015-07-02 04:54:42,995 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:42,995 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 92.0 (TID 1072). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:42,996 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 92.0 (TID 1079, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:42,996 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 92.0 (TID 1079)\n",
      "2015-07-02 04:54:42,997 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 92.0 (TID 1072) in 57 ms on localhost (9/52)\n",
      "2015-07-02 04:54:42,999 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:42,999 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:42,999 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -236, init = 281, finish = 0\n",
      "2015-07-02 04:54:43,000 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 92.0 (TID 1073). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,001 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 92.0 (TID 1080, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,001 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 92.0 (TID 1080)\n",
      "2015-07-02 04:54:43,002 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 92.0 (TID 1073) in 62 ms on localhost (10/52)\n",
      "2015-07-02 04:54:43,003 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -104, init = 145, finish = 0\n",
      "2015-07-02 04:54:43,004 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 92.0 (TID 1075). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,004 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -141, init = 185, finish = 0\n",
      "2015-07-02 04:54:43,004 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 92.0 (TID 1074). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,005 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 92.0 (TID 1081, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,005 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 92.0 (TID 1081)\n",
      "2015-07-02 04:54:43,005 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 92.0 (TID 1075) in 49 ms on localhost (11/52)\n",
      "2015-07-02 04:54:43,005 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -106, init = 147, finish = 0\n",
      "2015-07-02 04:54:43,006 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 92.0 (TID 1076). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,008 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 92.0 (TID 1082, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,008 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -107, init = 148, finish = 0\n",
      "2015-07-02 04:54:43,009 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 92.0 (TID 1083, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,009 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 92.0 (TID 1083)\n",
      "2015-07-02 04:54:43,010 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 92.0 (TID 1074) in 56 ms on localhost (12/52)\n",
      "2015-07-02 04:54:43,010 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 92.0 (TID 1076) in 50 ms on localhost (13/52)\n",
      "2015-07-02 04:54:43,008 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,008 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 92.0 (TID 1082)\n",
      "2015-07-02 04:54:43,012 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 92.0 (TID 1077). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,013 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,013 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,013 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 92.0 (TID 1084, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,014 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 92.0 (TID 1077) in 52 ms on localhost (14/52)\n",
      "2015-07-02 04:54:43,014 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 92.0 (TID 1084)\n",
      "2015-07-02 04:54:43,016 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,016 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,016 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,016 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,034 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -33, init = 74, finish = 0\n",
      "2015-07-02 04:54:43,034 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 92.0 (TID 1078). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,035 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 92.0 (TID 1085, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,036 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 92.0 (TID 1085)\n",
      "2015-07-02 04:54:43,036 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 92.0 (TID 1078) in 44 ms on localhost (15/52)\n",
      "2015-07-02 04:54:43,038 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,038 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,039 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -46, init = 87, finish = 0\n",
      "2015-07-02 04:54:43,040 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 92.0 (TID 1079). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,041 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 92.0 (TID 1086, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,041 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 92.0 (TID 1086)\n",
      "2015-07-02 04:54:43,041 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 92.0 (TID 1079) in 45 ms on localhost (16/52)\n",
      "2015-07-02 04:54:43,043 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,043 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,048 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -53, init = 98, finish = 0\n",
      "2015-07-02 04:54:43,049 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 92.0 (TID 1080). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,049 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 92.0 (TID 1087, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,050 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 92.0 (TID 1080) in 49 ms on localhost (17/52)\n",
      "2015-07-02 04:54:43,050 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 92.0 (TID 1087)\n",
      "2015-07-02 04:54:43,052 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,052 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,052 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -47, init = 93, finish = 0\n",
      "2015-07-02 04:54:43,053 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 92.0 (TID 1081). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,054 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 92.0 (TID 1088, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,054 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 92.0 (TID 1088)\n",
      "2015-07-02 04:54:43,055 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 92.0 (TID 1081) in 50 ms on localhost (18/52)\n",
      "2015-07-02 04:54:43,056 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -54, init = 98, finish = 0\n",
      "2015-07-02 04:54:43,056 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -59, init = 102, finish = 0\n",
      "2015-07-02 04:54:43,057 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 92.0 (TID 1082). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,057 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 92.0 (TID 1083). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,057 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,057 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,058 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 92.0 (TID 1089, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,058 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 92.0 (TID 1090, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,059 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 92.0 (TID 1083) in 50 ms on localhost (19/52)\n",
      "2015-07-02 04:54:43,059 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 92.0 (TID 1082) in 51 ms on localhost (20/52)\n",
      "2015-07-02 04:54:43,059 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 92.0 (TID 1090)\n",
      "2015-07-02 04:54:43,059 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 92.0 (TID 1089)\n",
      "2015-07-02 04:54:43,061 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -57, init = 101, finish = 0\n",
      "2015-07-02 04:54:43,061 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 92.0 (TID 1084). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,065 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 92.0 (TID 1091, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,066 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 92.0 (TID 1091)\n",
      "2015-07-02 04:54:43,066 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,066 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 92.0 (TID 1084) in 53 ms on localhost (21/52)\n",
      "2015-07-02 04:54:43,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,068 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,068 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,079 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -26, init = 68, finish = 0\n",
      "2015-07-02 04:54:43,079 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 92.0 (TID 1085). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,080 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 92.0 (TID 1092, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,080 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 92.0 (TID 1092)\n",
      "2015-07-02 04:54:43,080 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 92.0 (TID 1085) in 45 ms on localhost (22/52)\n",
      "2015-07-02 04:54:43,082 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,083 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,083 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -24, init = 65, finish = 0\n",
      "2015-07-02 04:54:43,084 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 92.0 (TID 1086). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,085 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 92.0 (TID 1093, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,086 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 92.0 (TID 1093)\n",
      "2015-07-02 04:54:43,087 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 92.0 (TID 1086) in 45 ms on localhost (23/52)\n",
      "2015-07-02 04:54:43,089 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,089 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,092 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -32, init = 73, finish = 0\n",
      "2015-07-02 04:54:43,093 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 92.0 (TID 1087). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,094 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 92.0 (TID 1094, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,094 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 92.0 (TID 1094)\n",
      "2015-07-02 04:54:43,094 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 92.0 (TID 1087) in 45 ms on localhost (24/52)\n",
      "2015-07-02 04:54:43,096 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,096 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,097 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -44, init = 85, finish = 0\n",
      "2015-07-02 04:54:43,098 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 92.0 (TID 1088). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,099 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 92.0 (TID 1095, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,099 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 92.0 (TID 1088) in 46 ms on localhost (25/52)\n",
      "2015-07-02 04:54:43,099 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 92.0 (TID 1095)\n",
      "2015-07-02 04:54:43,101 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,101 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,107 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -46, init = 91, finish = 0\n",
      "2015-07-02 04:54:43,107 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 92.0 (TID 1089). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,107 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -46, init = 88, finish = 0\n",
      "2015-07-02 04:54:43,107 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 92.0 (TID 1090). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,108 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 92.0 (TID 1096, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,108 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 92.0 (TID 1089) in 50 ms on localhost (26/52)\n",
      "2015-07-02 04:54:43,109 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 92.0 (TID 1096)\n",
      "2015-07-02 04:54:43,109 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -49, init = 90, finish = 0\n",
      "2015-07-02 04:54:43,109 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 92.0 (TID 1097, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,110 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 92.0 (TID 1097)\n",
      "2015-07-02 04:54:43,110 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 92.0 (TID 1090) in 52 ms on localhost (27/52)\n",
      "2015-07-02 04:54:43,112 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 92.0 (TID 1091). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,113 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 92.0 (TID 1098, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,113 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 92.0 (TID 1091) in 49 ms on localhost (28/52)\n",
      "2015-07-02 04:54:43,114 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 92.0 (TID 1098)\n",
      "2015-07-02 04:54:43,114 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,115 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,118 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,118 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,119 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,119 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,122 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -41, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,123 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 92.0 (TID 1092). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,123 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 92.0 (TID 1099, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,124 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 92.0 (TID 1092) in 43 ms on localhost (29/52)\n",
      "2015-07-02 04:54:43,124 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 92.0 (TID 1099)\n",
      "2015-07-02 04:54:43,126 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,126 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,129 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:43,130 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 92.0 (TID 1093). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,130 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 92.0 (TID 1100, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,131 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 92.0 (TID 1100)\n",
      "2015-07-02 04:54:43,131 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 92.0 (TID 1093) in 45 ms on localhost (30/52)\n",
      "2015-07-02 04:54:43,133 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,133 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,135 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -40, init = 80, finish = 0\n",
      "2015-07-02 04:54:43,136 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 92.0 (TID 1094). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,137 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 92.0 (TID 1101, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,137 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 92.0 (TID 1101)\n",
      "2015-07-02 04:54:43,138 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 92.0 (TID 1094) in 44 ms on localhost (31/52)\n",
      "2015-07-02 04:54:43,140 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,140 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,141 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -35, init = 75, finish = 0\n",
      "2015-07-02 04:54:43,142 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 92.0 (TID 1095). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,142 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 92.0 (TID 1102, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,143 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 92.0 (TID 1102)\n",
      "2015-07-02 04:54:43,143 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 92.0 (TID 1095) in 45 ms on localhost (32/52)\n",
      "2015-07-02 04:54:43,145 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,146 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,154 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -44, init = 87, finish = 0\n",
      "2015-07-02 04:54:43,155 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 92.0 (TID 1096). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,155 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 92.0 (TID 1103, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,156 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 92.0 (TID 1103)\n",
      "2015-07-02 04:54:43,156 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 92.0 (TID 1096) in 49 ms on localhost (33/52)\n",
      "2015-07-02 04:54:43,158 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,158 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,158 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -45, init = 87, finish = 0\n",
      "2015-07-02 04:54:43,159 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 92.0 (TID 1098). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,159 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 92.0 (TID 1104, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,160 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -46, init = 94, finish = 0\n",
      "2015-07-02 04:54:43,160 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 92.0 (TID 1104)\n",
      "2015-07-02 04:54:43,160 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 92.0 (TID 1097). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,162 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 92.0 (TID 1105, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,162 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 92.0 (TID 1105)\n",
      "2015-07-02 04:54:43,162 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 92.0 (TID 1098) in 50 ms on localhost (34/52)\n",
      "2015-07-02 04:54:43,163 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 92.0 (TID 1097) in 54 ms on localhost (35/52)\n",
      "2015-07-02 04:54:43,164 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,164 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,165 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,165 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,167 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -39, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,168 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 92.0 (TID 1099). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,169 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 92.0 (TID 1106, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,169 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 92.0 (TID 1106)\n",
      "2015-07-02 04:54:43,170 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 92.0 (TID 1099) in 47 ms on localhost (36/52)\n",
      "2015-07-02 04:54:43,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,174 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,174 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 92.0 (TID 1100). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,175 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 92.0 (TID 1107, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,175 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 92.0 (TID 1107)\n",
      "2015-07-02 04:54:43,175 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 92.0 (TID 1100) in 45 ms on localhost (37/52)\n",
      "2015-07-02 04:54:43,177 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,178 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,180 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,181 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 92.0 (TID 1101). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,181 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 92.0 (TID 1108, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,182 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 92.0 (TID 1108)\n",
      "2015-07-02 04:54:43,182 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 92.0 (TID 1101) in 45 ms on localhost (38/52)\n",
      "2015-07-02 04:54:43,184 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,184 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,185 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -41, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,186 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 92.0 (TID 1102). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 92.0 (TID 1109, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,187 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 92.0 (TID 1109)\n",
      "2015-07-02 04:54:43,188 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 92.0 (TID 1102) in 45 ms on localhost (39/52)\n",
      "2015-07-02 04:54:43,189 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,190 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,198 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,199 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 92.0 (TID 1103). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,199 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 92.0 (TID 1110, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,200 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 92.0 (TID 1110)\n",
      "2015-07-02 04:54:43,200 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 92.0 (TID 1103) in 45 ms on localhost (40/52)\n",
      "2015-07-02 04:54:43,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,202 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,204 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -44, init = 87, finish = 0\n",
      "2015-07-02 04:54:43,205 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 92.0 (TID 1104). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,206 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 92.0 (TID 1111, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,206 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -46, init = 87, finish = 0\n",
      "2015-07-02 04:54:43,206 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 92.0 (TID 1104) in 47 ms on localhost (41/52)\n",
      "2015-07-02 04:54:43,206 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 92.0 (TID 1111)\n",
      "2015-07-02 04:54:43,207 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 92.0 (TID 1105). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,207 INFO  [sparkDriver-akka.actor.default-dispatcher-18] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 92.0 (TID 1112, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,208 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 92.0 (TID 1112)\n",
      "2015-07-02 04:54:43,208 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 92.0 (TID 1105) in 47 ms on localhost (42/52)\n",
      "2015-07-02 04:54:43,209 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,209 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,210 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,210 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,213 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -41, init = 84, finish = 0\n",
      "2015-07-02 04:54:43,214 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 92.0 (TID 1106). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,215 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 92.0 (TID 1113, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,215 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 92.0 (TID 1113)\n",
      "2015-07-02 04:54:43,215 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 92.0 (TID 1106) in 47 ms on localhost (43/52)\n",
      "2015-07-02 04:54:43,217 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -42, init = 82, finish = 0\n",
      "2015-07-02 04:54:43,217 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,217 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,217 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 92.0 (TID 1107). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,218 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 92.0 (TID 1114, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,219 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 92.0 (TID 1114)\n",
      "2015-07-02 04:54:43,219 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 92.0 (TID 1107) in 44 ms on localhost (44/52)\n",
      "2015-07-02 04:54:43,221 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,221 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,224 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,225 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 92.0 (TID 1108). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,225 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 92.0 (TID 1115, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:43,226 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 92.0 (TID 1115)\n",
      "2015-07-02 04:54:43,226 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 92.0 (TID 1108) in 45 ms on localhost (45/52)\n",
      "2015-07-02 04:54:43,227 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,227 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,229 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -41, init = 81, finish = 0\n",
      "2015-07-02 04:54:43,230 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 92.0 (TID 1109). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,231 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 92.0 (TID 1109) in 45 ms on localhost (46/52)\n",
      "2015-07-02 04:54:43,242 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:43,243 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 92.0 (TID 1110). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,244 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 92.0 (TID 1110) in 44 ms on localhost (47/52)\n",
      "2015-07-02 04:54:43,249 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -41, init = 83, finish = 0\n",
      "2015-07-02 04:54:43,250 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 92.0 (TID 1111). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,251 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 92.0 (TID 1111) in 46 ms on localhost (48/52)\n",
      "2015-07-02 04:54:43,251 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -40, init = 82, finish = 0\n",
      "2015-07-02 04:54:43,252 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 92.0 (TID 1112). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,252 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 92.0 (TID 1112) in 45 ms on localhost (49/52)\n",
      "2015-07-02 04:54:43,257 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:43,258 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 92.0 (TID 1113). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,259 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 92.0 (TID 1113) in 44 ms on localhost (50/52)\n",
      "2015-07-02 04:54:43,261 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:43,262 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 92.0 (TID 1114). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,263 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 92.0 (TID 1114) in 44 ms on localhost (51/52)\n",
      "2015-07-02 04:54:43,268 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 79, finish = 1\n",
      "2015-07-02 04:54:43,268 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 92.0 (TID 1115). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:43,269 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 92.0 (TID 1115) in 44 ms on localhost (52/52)\n",
      "2015-07-02 04:54:43,269 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:43,270 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 92 (collect at <ipython-input-3-4ba038715317>:139) finished in 0.378 s\n",
      "2015-07-02 04:54:43,271 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 6 finished: collect at <ipython-input-3-4ba038715317>:139, took 364.996558 s\n",
      "2015-07-02 04:54:43,303 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-3-4ba038715317>:141\n",
      "2015-07-02 04:54:43,304 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 45 is 154 bytes\n",
      "2015-07-02 04:54:43,304 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 44 is 157 bytes\n",
      "2015-07-02 04:54:43,305 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 43 is 169 bytes\n",
      "2015-07-02 04:54:43,306 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 42 is 390 bytes\n",
      "2015-07-02 04:54:43,306 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 41 is 349 bytes\n",
      "2015-07-02 04:54:43,307 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 40 is 156 bytes\n",
      "2015-07-02 04:54:43,308 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 39 is 347 bytes\n",
      "2015-07-02 04:54:43,309 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 38 is 156 bytes\n",
      "2015-07-02 04:54:43,310 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 37 is 368 bytes\n",
      "2015-07-02 04:54:43,311 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 36 is 406 bytes\n",
      "2015-07-02 04:54:43,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 35 is 508 bytes\n",
      "2015-07-02 04:54:43,313 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 34 is 157 bytes\n",
      "2015-07-02 04:54:43,313 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 33 is 157 bytes\n",
      "2015-07-02 04:54:43,314 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 32 is 466 bytes\n",
      "2015-07-02 04:54:43,315 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 31 is 157 bytes\n",
      "2015-07-02 04:54:43,316 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 30 is 157 bytes\n",
      "2015-07-02 04:54:43,316 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 29 is 509 bytes\n",
      "2015-07-02 04:54:43,317 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 28 is 157 bytes\n",
      "2015-07-02 04:54:43,318 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 27 is 476 bytes\n",
      "2015-07-02 04:54:43,320 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 26 is 482 bytes\n",
      "2015-07-02 04:54:43,320 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 349 (reduceByKey at <ipython-input-3-4ba038715317>:141)\n",
      "2015-07-02 04:54:43,321 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 7 (collect at <ipython-input-3-4ba038715317>:141) with 52 output partitions (allowLocal=false)\n",
      "2015-07-02 04:54:43,321 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 114(collect at <ipython-input-3-4ba038715317>:141)\n",
      "2015-07-02 04:54:43,321 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 113)\n",
      "2015-07-02 04:54:43,322 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 113)\n",
      "2015-07-02 04:54:43,324 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 113 (PairwiseRDD[349] at reduceByKey at <ipython-input-3-4ba038715317>:141), which has no missing parents\n",
      "2015-07-02 04:54:43,324 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9984) called with curMem=3472230, maxMem=278302556\n",
      "2015-07-02 04:54:43,324 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_65 stored as values in memory (estimated size 9.8 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:43,325 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6216) called with curMem=3482214, maxMem=278302556\n",
      "2015-07-02 04:54:43,325 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_65_piece0 stored as bytes in memory (estimated size 6.1 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:43,326 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_65_piece0 in memory on localhost:40918 (size: 6.1 KB, free: 264.9 MB)\n",
      "2015-07-02 04:54:43,326 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_65_piece0\n",
      "2015-07-02 04:54:43,326 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 65 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:43,329 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 113 (PairwiseRDD[349] at reduceByKey at <ipython-input-3-4ba038715317>:141)\n",
      "2015-07-02 04:54:43,329 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 113.0 with 52 tasks\n",
      "2015-07-02 04:54:43,329 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 113.0 (TID 1116, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,330 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 113.0 (TID 1117, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,330 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 113.0 (TID 1118, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,330 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 113.0 (TID 1119, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,330 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 113.0 (TID 1120, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,331 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 113.0 (TID 1121, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,331 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 113.0 (TID 1122, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,331 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 113.0 (TID 1117)\n",
      "2015-07-02 04:54:43,332 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 113.0 (TID 1121)\n",
      "2015-07-02 04:54:43,332 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 113.0 (TID 1118)\n",
      "2015-07-02 04:54:43,332 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 113.0 (TID 1120)\n",
      "2015-07-02 04:54:43,331 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 113.0 (TID 1122)\n",
      "2015-07-02 04:54:43,331 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 113.0 (TID 1116)\n",
      "2015-07-02 04:54:43,331 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 113.0 (TID 1119)\n",
      "2015-07-02 04:54:43,334 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,334 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,334 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,334 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,336 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,335 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,336 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,336 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,340 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,340 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,340 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,341 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,376 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -141, init = 183, finish = 0\n",
      "2015-07-02 04:54:43,379 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -119, init = 163, finish = 1\n",
      "2015-07-02 04:54:43,382 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -128, init = 171, finish = 0\n",
      "2015-07-02 04:54:43,384 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 113.0 (TID 1117). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,386 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 113.0 (TID 1123, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,386 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 113.0 (TID 1117) in 56 ms on localhost (1/52)\n",
      "2015-07-02 04:54:43,377 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -122, init = 163, finish = 0\n",
      "2015-07-02 04:54:43,384 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -103, init = 147, finish = 1\n",
      "2015-07-02 04:54:43,402 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 113.0 (TID 1123)\n",
      "2015-07-02 04:54:43,403 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 68, boot = -110, init = 178, finish = 0\n",
      "2015-07-02 04:54:43,404 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 68, boot = -114, init = 182, finish = 0\n",
      "2015-07-02 04:54:43,432 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,432 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,475 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -167, init = 238, finish = 0\n",
      "2015-07-02 04:54:43,488 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 113.0 (TID 1122). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,490 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 113.0 (TID 1124, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,491 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 113.0 (TID 1124)\n",
      "2015-07-02 04:54:43,491 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 113.0 (TID 1122) in 160 ms on localhost (2/52)\n",
      "2015-07-02 04:54:43,494 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 113.0 (TID 1120). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,495 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 113.0 (TID 1125, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,495 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 113.0 (TID 1125)\n",
      "2015-07-02 04:54:43,495 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 113.0 (TID 1120) in 165 ms on localhost (3/52)\n",
      "2015-07-02 04:54:43,499 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,499 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,500 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 113.0 (TID 1121). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,500 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 113.0 (TID 1119). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,501 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 113.0 (TID 1126, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,501 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 113.0 (TID 1118). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,501 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 113.0 (TID 1127, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,501 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 113.0 (TID 1127)\n",
      "2015-07-02 04:54:43,501 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 113.0 (TID 1128, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,502 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 113.0 (TID 1121) in 171 ms on localhost (4/52)\n",
      "2015-07-02 04:54:43,502 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 113.0 (TID 1119) in 172 ms on localhost (5/52)\n",
      "2015-07-02 04:54:43,502 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 113.0 (TID 1126)\n",
      "2015-07-02 04:54:43,503 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 113.0 (TID 1128)\n",
      "2015-07-02 04:54:43,504 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 113.0 (TID 1118) in 174 ms on localhost (6/52)\n",
      "2015-07-02 04:54:43,504 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 113.0 (TID 1116). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,505 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 113.0 (TID 1129, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,505 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 113.0 (TID 1116) in 176 ms on localhost (7/52)\n",
      "2015-07-02 04:54:43,506 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 113.0 (TID 1129)\n",
      "2015-07-02 04:54:43,509 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,509 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,512 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,512 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,513 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,513 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,513 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,514 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,515 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,515 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,521 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 113.0 (TID 1123). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,522 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 113.0 (TID 1130, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,522 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 113.0 (TID 1130)\n",
      "2015-07-02 04:54:43,522 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 113.0 (TID 1123) in 137 ms on localhost (8/52)\n",
      "2015-07-02 04:54:43,523 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,524 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,540 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -244, init = 291, finish = 1\n",
      "2015-07-02 04:54:43,549 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 113.0 (TID 1124). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,550 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 113.0 (TID 1131, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,550 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -236, init = 281, finish = 0\n",
      "2015-07-02 04:54:43,550 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 113.0 (TID 1131)\n",
      "2015-07-02 04:54:43,550 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 113.0 (TID 1124) in 61 ms on localhost (9/52)\n",
      "2015-07-02 04:54:43,558 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 58, boot = -239, init = 296, finish = 1\n",
      "2015-07-02 04:54:43,559 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -240, init = 289, finish = 0\n",
      "2015-07-02 04:54:43,558 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -243, init = 294, finish = 0\n",
      "2015-07-02 04:54:43,565 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -233, init = 285, finish = 1\n",
      "2015-07-02 04:54:43,568 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -122, init = 165, finish = 0\n",
      "2015-07-02 04:54:43,570 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 113.0 (TID 1130). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,571 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 113.0 (TID 1132, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,572 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 113.0 (TID 1130) in 51 ms on localhost (10/52)\n",
      "2015-07-02 04:54:43,572 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 113.0 (TID 1132)\n",
      "2015-07-02 04:54:43,577 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,577 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,581 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,581 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,618 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 65, boot = -170, init = 235, finish = 0\n",
      "2015-07-02 04:54:43,618 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 113.0 (TID 1126). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,619 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 113.0 (TID 1129). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,619 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 113.0 (TID 1128). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,620 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 113.0 (TID 1133, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,620 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 113.0 (TID 1125). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,620 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 113.0 (TID 1134, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,621 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 113.0 (TID 1133)\n",
      "2015-07-02 04:54:43,622 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 113.0 (TID 1129) in 115 ms on localhost (11/52)\n",
      "2015-07-02 04:54:43,622 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 113.0 (TID 1134)\n",
      "2015-07-02 04:54:43,623 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 113.0 (TID 1135, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,624 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 113.0 (TID 1135)\n",
      "2015-07-02 04:54:43,626 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 113.0 (TID 1136, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,626 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -188, init = 235, finish = 1\n",
      "2015-07-02 04:54:43,627 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 113.0 (TID 1126) in 125 ms on localhost (12/52)\n",
      "2015-07-02 04:54:43,627 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 113.0 (TID 1125) in 132 ms on localhost (13/52)\n",
      "2015-07-02 04:54:43,627 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 113.0 (TID 1136)\n",
      "2015-07-02 04:54:43,628 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 113.0 (TID 1128) in 127 ms on localhost (14/52)\n",
      "2015-07-02 04:54:43,630 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 113.0 (TID 1127). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,631 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 113.0 (TID 1137, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,631 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 113.0 (TID 1127) in 130 ms on localhost (15/52)\n",
      "2015-07-02 04:54:43,631 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 113.0 (TID 1137)\n",
      "2015-07-02 04:54:43,635 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,635 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,637 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,637 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,639 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,639 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,639 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,639 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,655 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 113.0 (TID 1131). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,655 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 113.0 (TID 1132). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,656 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 113.0 (TID 1138, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,656 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 113.0 (TID 1138)\n",
      "2015-07-02 04:54:43,657 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 113.0 (TID 1131) in 107 ms on localhost (16/52)\n",
      "2015-07-02 04:54:43,657 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 113.0 (TID 1139, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,658 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 113.0 (TID 1132) in 86 ms on localhost (17/52)\n",
      "2015-07-02 04:54:43,658 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 113.0 (TID 1139)\n",
      "2015-07-02 04:54:43,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,676 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -215, init = 266, finish = 0\n",
      "2015-07-02 04:54:43,678 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -243, init = 293, finish = 1\n",
      "2015-07-02 04:54:43,680 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -217, init = 270, finish = 0\n",
      "2015-07-02 04:54:43,682 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -225, init = 275, finish = 0\n",
      "2015-07-02 04:54:43,687 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -151, init = 201, finish = 0\n",
      "2015-07-02 04:54:43,696 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 113.0 (TID 1134). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,698 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 113.0 (TID 1140, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,698 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 113.0 (TID 1140)\n",
      "2015-07-02 04:54:43,698 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 113.0 (TID 1134) in 78 ms on localhost (18/52)\n",
      "2015-07-02 04:54:43,700 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -112, init = 152, finish = 1\n",
      "2015-07-02 04:54:43,701 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -84, init = 125, finish = 1\n",
      "2015-07-02 04:54:43,711 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,711 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,740 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 113.0 (TID 1135). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,741 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 113.0 (TID 1141, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,742 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 113.0 (TID 1135) in 118 ms on localhost (19/52)\n",
      "2015-07-02 04:54:43,741 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 113.0 (TID 1141)\n",
      "2015-07-02 04:54:43,752 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -137, init = 187, finish = 1\n",
      "2015-07-02 04:54:43,753 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,753 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,757 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 113.0 (TID 1136). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,758 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 113.0 (TID 1142, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,759 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 113.0 (TID 1142)\n",
      "2015-07-02 04:54:43,759 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 113.0 (TID 1136) in 133 ms on localhost (20/52)\n",
      "2015-07-02 04:54:43,768 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 113.0 (TID 1133). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,768 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,769 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,769 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 113.0 (TID 1143, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,770 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 113.0 (TID 1133) in 149 ms on localhost (21/52)\n",
      "2015-07-02 04:54:43,770 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 113.0 (TID 1143)\n",
      "2015-07-02 04:54:43,770 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 113.0 (TID 1137). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,771 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 113.0 (TID 1144, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,772 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 113.0 (TID 1137) in 141 ms on localhost (22/52)\n",
      "2015-07-02 04:54:43,772 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 113.0 (TID 1144)\n",
      "2015-07-02 04:54:43,772 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 113.0 (TID 1138). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,773 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 113.0 (TID 1145, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,774 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 113.0 (TID 1138) in 117 ms on localhost (23/52)\n",
      "2015-07-02 04:54:43,774 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,774 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,774 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 113.0 (TID 1145)\n",
      "2015-07-02 04:54:43,775 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 113.0 (TID 1139). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,775 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 113.0 (TID 1146, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,775 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 113.0 (TID 1139) in 118 ms on localhost (24/52)\n",
      "2015-07-02 04:54:43,776 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 113.0 (TID 1146)\n",
      "2015-07-02 04:54:43,779 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,779 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,779 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,779 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,782 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,782 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,785 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 113.0 (TID 1140). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,785 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 113.0 (TID 1147, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,786 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 113.0 (TID 1147)\n",
      "2015-07-02 04:54:43,786 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 113.0 (TID 1140) in 89 ms on localhost (25/52)\n",
      "2015-07-02 04:54:43,788 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,788 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,793 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -171, init = 221, finish = 0\n",
      "2015-07-02 04:54:43,804 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 113.0 (TID 1141). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,804 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 113.0 (TID 1148, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,805 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 113.0 (TID 1148)\n",
      "2015-07-02 04:54:43,805 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 113.0 (TID 1141) in 64 ms on localhost (26/52)\n",
      "2015-07-02 04:54:43,806 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,806 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,808 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -202, init = 250, finish = 0\n",
      "2015-07-02 04:54:43,814 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -204, init = 246, finish = 0\n",
      "2015-07-02 04:54:43,836 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -100, init = 142, finish = 0\n",
      "2015-07-02 04:54:43,837 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -209, init = 262, finish = 0\n",
      "2015-07-02 04:54:43,838 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -142, init = 185, finish = 0\n",
      "2015-07-02 04:54:43,838 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -148, init = 192, finish = 1\n",
      "2015-07-02 04:54:43,842 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 113.0 (TID 1142). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,843 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 113.0 (TID 1149, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,844 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 113.0 (TID 1142) in 85 ms on localhost (27/52)\n",
      "2015-07-02 04:54:43,844 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 113.0 (TID 1149)\n",
      "2015-07-02 04:54:43,848 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -105, init = 146, finish = 1\n",
      "2015-07-02 04:54:43,858 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,858 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,872 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 113.0 (TID 1143). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,873 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 113.0 (TID 1150, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,873 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 113.0 (TID 1150)\n",
      "2015-07-02 04:54:43,873 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 113.0 (TID 1143) in 104 ms on localhost (28/52)\n",
      "2015-07-02 04:54:43,880 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,880 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,899 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -153, init = 205, finish = 0\n",
      "2015-07-02 04:54:43,900 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 113.0 (TID 1144). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,901 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 113.0 (TID 1151, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,901 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 113.0 (TID 1151)\n",
      "2015-07-02 04:54:43,901 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 113.0 (TID 1144) in 130 ms on localhost (29/52)\n",
      "2015-07-02 04:54:43,903 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 113.0 (TID 1147). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,904 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 113.0 (TID 1152, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,904 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 113.0 (TID 1152)\n",
      "2015-07-02 04:54:43,904 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 113.0 (TID 1147) in 119 ms on localhost (30/52)\n",
      "2015-07-02 04:54:43,905 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 113.0 (TID 1145). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,906 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 113.0 (TID 1153, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,906 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 113.0 (TID 1153)\n",
      "2015-07-02 04:54:43,907 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,907 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,907 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 113.0 (TID 1145) in 134 ms on localhost (31/52)\n",
      "2015-07-02 04:54:43,907 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,908 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,910 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 113.0 (TID 1148). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,910 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 113.0 (TID 1146). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,911 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 113.0 (TID 1154, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,912 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 113.0 (TID 1154)\n",
      "2015-07-02 04:54:43,912 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 113.0 (TID 1155, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,912 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,912 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 113.0 (TID 1155)\n",
      "2015-07-02 04:54:43,913 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 113.0 (TID 1148) in 109 ms on localhost (32/52)\n",
      "2015-07-02 04:54:43,914 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 113.0 (TID 1146) in 139 ms on localhost (33/52)\n",
      "2015-07-02 04:54:43,916 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,916 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,916 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,917 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,921 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 113.0 (TID 1149). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,921 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -183, init = 229, finish = 1\n",
      "2015-07-02 04:54:43,922 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 113.0 (TID 1156, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,922 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 113.0 (TID 1156)\n",
      "2015-07-02 04:54:43,922 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 113.0 (TID 1149) in 79 ms on localhost (34/52)\n",
      "2015-07-02 04:54:43,925 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,926 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:43,932 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 113.0 (TID 1150). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,932 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 113.0 (TID 1157, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,933 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 113.0 (TID 1157)\n",
      "2015-07-02 04:54:43,933 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 113.0 (TID 1150) in 61 ms on localhost (35/52)\n",
      "2015-07-02 04:54:43,935 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,935 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:43,947 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -206, init = 249, finish = 1\n",
      "2015-07-02 04:54:43,949 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -195, init = 238, finish = 0\n",
      "2015-07-02 04:54:43,954 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -201, init = 244, finish = 1\n",
      "2015-07-02 04:54:43,959 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -114, init = 158, finish = 0\n",
      "2015-07-02 04:54:43,959 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -152, init = 197, finish = 0\n",
      "2015-07-02 04:54:43,968 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -108, init = 150, finish = 1\n",
      "2015-07-02 04:54:43,973 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 113.0 (TID 1151). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:43,973 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 113.0 (TID 1158, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:43,974 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 113.0 (TID 1158)\n",
      "2015-07-02 04:54:43,974 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 113.0 (TID 1151) in 73 ms on localhost (36/52)\n",
      "2015-07-02 04:54:43,977 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -104, init = 146, finish = 0\n",
      "2015-07-02 04:54:43,983 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:43,984 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,004 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 113.0 (TID 1152). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,005 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 113.0 (TID 1159, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,005 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 113.0 (TID 1159)\n",
      "2015-07-02 04:54:44,005 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 113.0 (TID 1152) in 101 ms on localhost (37/52)\n",
      "2015-07-02 04:54:44,013 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 113.0 (TID 1153). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,013 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 113.0 (TID 1160, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,013 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,013 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,013 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 113.0 (TID 1153) in 108 ms on localhost (38/52)\n",
      "2015-07-02 04:54:44,013 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 113.0 (TID 1160)\n",
      "2015-07-02 04:54:44,021 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,021 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,026 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 113.0 (TID 1154). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,026 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 113.0 (TID 1161, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,026 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 113.0 (TID 1154) in 115 ms on localhost (39/52)\n",
      "2015-07-02 04:54:44,027 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 113.0 (TID 1161)\n",
      "2015-07-02 04:54:44,026 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -138, init = 188, finish = 0\n",
      "2015-07-02 04:54:44,032 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 113.0 (TID 1157). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,033 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 113.0 (TID 1155). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,034 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 113.0 (TID 1162, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,033 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 113.0 (TID 1156). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,034 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 113.0 (TID 1157) in 102 ms on localhost (40/52)\n",
      "2015-07-02 04:54:44,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 113.0 (TID 1163, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,036 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 113.0 (TID 1163)\n",
      "2015-07-02 04:54:44,036 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 113.0 (TID 1164, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,034 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 113.0 (TID 1162)\n",
      "2015-07-02 04:54:44,036 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 113.0 (TID 1164)\n",
      "2015-07-02 04:54:44,036 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 113.0 (TID 1155) in 124 ms on localhost (41/52)\n",
      "2015-07-02 04:54:44,037 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 113.0 (TID 1156) in 115 ms on localhost (42/52)\n",
      "2015-07-02 04:54:44,040 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,040 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,040 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,040 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,042 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,045 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 113.0 (TID 1158). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,045 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 113.0 (TID 1165, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,046 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 113.0 (TID 1165)\n",
      "2015-07-02 04:54:44,046 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 113.0 (TID 1158) in 73 ms on localhost (43/52)\n",
      "2015-07-02 04:54:44,048 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,048 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,053 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -165, init = 211, finish = 0\n",
      "2015-07-02 04:54:44,062 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -180, init = 226, finish = 1\n",
      "2015-07-02 04:54:44,063 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 113.0 (TID 1159). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,063 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 113.0 (TID 1166, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,064 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 113.0 (TID 1166)\n",
      "2015-07-02 04:54:44,064 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 113.0 (TID 1159) in 59 ms on localhost (44/52)\n",
      "2015-07-02 04:54:44,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,067 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,074 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 113.0 (TID 1160). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,075 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 113.0 (TID 1167, localhost, PROCESS_LOCAL, 1045 bytes)\n",
      "2015-07-02 04:54:44,075 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 113.0 (TID 1167)\n",
      "2015-07-02 04:54:44,075 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 113.0 (TID 1160) in 62 ms on localhost (45/52)\n",
      "2015-07-02 04:54:44,077 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,077 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,080 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -198, init = 244, finish = 0\n",
      "2015-07-02 04:54:44,080 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -127, init = 169, finish = 0\n",
      "2015-07-02 04:54:44,085 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -180, init = 227, finish = 0\n",
      "2015-07-02 04:54:44,086 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -109, init = 155, finish = 0\n",
      "2015-07-02 04:54:44,090 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -89, init = 131, finish = 0\n",
      "2015-07-02 04:54:44,109 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -106, init = 148, finish = 1\n",
      "2015-07-02 04:54:44,110 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 113.0 (TID 1162). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,111 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 113.0 (TID 1162) in 78 ms on localhost (46/52)\n",
      "2015-07-02 04:54:44,118 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -108, init = 150, finish = 0\n",
      "2015-07-02 04:54:44,140 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 113.0 (TID 1161). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,141 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 113.0 (TID 1161) in 115 ms on localhost (47/52)\n",
      "2015-07-02 04:54:44,153 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 113.0 (TID 1163). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,154 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 113.0 (TID 1163) in 118 ms on localhost (48/52)\n",
      "2015-07-02 04:54:44,159 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 113.0 (TID 1164). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,160 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 113.0 (TID 1164) in 124 ms on localhost (49/52)\n",
      "2015-07-02 04:54:44,161 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 113.0 (TID 1165). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,161 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 113.0 (TID 1166). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,161 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 113.0 (TID 1165) in 116 ms on localhost (50/52)\n",
      "2015-07-02 04:54:44,162 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 113.0 (TID 1166) in 99 ms on localhost (51/52)\n",
      "2015-07-02 04:54:44,162 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 113.0 (TID 1167). 1113 bytes result sent to driver\n",
      "2015-07-02 04:54:44,163 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 113.0 (TID 1167) in 89 ms on localhost (52/52)\n",
      "2015-07-02 04:54:44,163 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:44,163 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 113 (reduceByKey at <ipython-input-3-4ba038715317>:141) finished in 0.834 s\n",
      "2015-07-02 04:54:44,163 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 04:54:44,164 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 04:54:44,164 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 114)\n",
      "2015-07-02 04:54:44,164 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 04:54:44,165 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 114: List()\n",
      "2015-07-02 04:54:44,165 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 114 (PythonRDD[352] at collect at <ipython-input-3-4ba038715317>:141), which is now runnable\n",
      "2015-07-02 04:54:44,166 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4752) called with curMem=3488430, maxMem=278302556\n",
      "2015-07-02 04:54:44,166 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_66 stored as values in memory (estimated size 4.6 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:44,167 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(2986) called with curMem=3493182, maxMem=278302556\n",
      "2015-07-02 04:54:44,167 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_66_piece0 stored as bytes in memory (estimated size 2.9 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:44,167 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_66_piece0 in memory on localhost:40918 (size: 2.9 KB, free: 264.9 MB)\n",
      "2015-07-02 04:54:44,168 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_66_piece0\n",
      "2015-07-02 04:54:44,168 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 66 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:44,169 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 114 (PythonRDD[352] at collect at <ipython-input-3-4ba038715317>:141)\n",
      "2015-07-02 04:54:44,169 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 114.0 with 52 tasks\n",
      "2015-07-02 04:54:44,170 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 114.0 (TID 1168, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,171 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 114.0 (TID 1169, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,171 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 114.0 (TID 1170, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,171 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 114.0 (TID 1171, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,171 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 114.0 (TID 1172, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,172 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 114.0 (TID 1173, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,172 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 114.0 (TID 1174, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 114.0 (TID 1170)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 114.0 (TID 1168)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 114.0 (TID 1169)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 114.0 (TID 1174)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 114.0 (TID 1172)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 114.0 (TID 1171)\n",
      "2015-07-02 04:54:44,172 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 114.0 (TID 1173)\n",
      "2015-07-02 04:54:44,175 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,175 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,177 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,178 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,178 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,178 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,182 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,183 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,183 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,215 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -203, init = 245, finish = 0\n",
      "2015-07-02 04:54:44,216 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 114.0 (TID 1168). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,216 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 114.0 (TID 1175, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,217 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 114.0 (TID 1175)\n",
      "2015-07-02 04:54:44,217 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 114.0 (TID 1168) in 47 ms on localhost (1/52)\n",
      "2015-07-02 04:54:44,218 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -200, init = 244, finish = 0\n",
      "2015-07-02 04:54:44,218 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -196, init = 239, finish = 1\n",
      "2015-07-02 04:54:44,218 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 114.0 (TID 1170). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,219 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 114.0 (TID 1169). 993 bytes result sent to driver\n",
      "2015-07-02 04:54:44,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 114.0 (TID 1176, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,220 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 114.0 (TID 1176)\n",
      "2015-07-02 04:54:44,220 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 114.0 (TID 1177, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,221 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,221 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 114.0 (TID 1177)\n",
      "2015-07-02 04:54:44,221 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,224 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -189, init = 237, finish = 0\n",
      "2015-07-02 04:54:44,225 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -140, init = 189, finish = 0\n",
      "2015-07-02 04:54:44,225 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -114, init = 164, finish = 0\n",
      "2015-07-02 04:54:44,232 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 114.0 (TID 1170) in 61 ms on localhost (2/52)\n",
      "2015-07-02 04:54:44,233 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 55, boot = -104, init = 158, finish = 1\n",
      "2015-07-02 04:54:44,233 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 114.0 (TID 1169) in 63 ms on localhost (3/52)\n",
      "2015-07-02 04:54:44,234 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 114.0 (TID 1172). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,234 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 114.0 (TID 1174). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,234 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 114.0 (TID 1178, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,235 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 114.0 (TID 1173). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,235 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 114.0 (TID 1171). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,235 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 114.0 (TID 1178)\n",
      "2015-07-02 04:54:44,236 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 114.0 (TID 1179, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,236 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 114.0 (TID 1179)\n",
      "2015-07-02 04:54:44,236 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 114.0 (TID 1180, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,237 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 114.0 (TID 1172) in 66 ms on localhost (4/52)\n",
      "2015-07-02 04:54:44,237 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 114.0 (TID 1180)\n",
      "2015-07-02 04:54:44,237 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 114.0 (TID 1181, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,238 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 114.0 (TID 1171) in 67 ms on localhost (5/52)\n",
      "2015-07-02 04:54:44,238 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,238 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,238 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 114.0 (TID 1173) in 66 ms on localhost (6/52)\n",
      "2015-07-02 04:54:44,238 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 114.0 (TID 1181)\n",
      "2015-07-02 04:54:44,239 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 114.0 (TID 1174) in 67 ms on localhost (7/52)\n",
      "2015-07-02 04:54:44,243 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,243 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,243 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,244 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,245 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,261 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -119, init = 162, finish = 0\n",
      "2015-07-02 04:54:44,262 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 114.0 (TID 1175). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,262 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 114.0 (TID 1182, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,263 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 114.0 (TID 1182)\n",
      "2015-07-02 04:54:44,263 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 114.0 (TID 1175) in 47 ms on localhost (8/52)\n",
      "2015-07-02 04:54:44,264 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,265 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,280 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 57, boot = -126, init = 183, finish = 0\n",
      "2015-07-02 04:54:44,280 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 114.0 (TID 1177). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,281 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 114.0 (TID 1183, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,281 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 114.0 (TID 1183)\n",
      "2015-07-02 04:54:44,281 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 114.0 (TID 1177) in 61 ms on localhost (9/52)\n",
      "2015-07-02 04:54:44,283 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 62, boot = -130, init = 192, finish = 0\n",
      "2015-07-02 04:54:44,283 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -122, init = 166, finish = 0\n",
      "2015-07-02 04:54:44,284 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -114, init = 158, finish = 0\n",
      "2015-07-02 04:54:44,285 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -135, init = 183, finish = 0\n",
      "2015-07-02 04:54:44,286 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -136, init = 182, finish = 0\n",
      "2015-07-02 04:54:44,288 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,288 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,289 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 114.0 (TID 1179). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,290 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 114.0 (TID 1178). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,290 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 114.0 (TID 1184, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,291 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 114.0 (TID 1181). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,291 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 114.0 (TID 1180). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,291 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 114.0 (TID 1176). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,291 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 114.0 (TID 1184)\n",
      "2015-07-02 04:54:44,291 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 114.0 (TID 1185, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,291 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 114.0 (TID 1185)\n",
      "2015-07-02 04:54:44,291 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 114.0 (TID 1179) in 56 ms on localhost (10/52)\n",
      "2015-07-02 04:54:44,292 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 114.0 (TID 1178) in 58 ms on localhost (11/52)\n",
      "2015-07-02 04:54:44,293 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 114.0 (TID 1186, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,293 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 114.0 (TID 1186)\n",
      "2015-07-02 04:54:44,293 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 114.0 (TID 1181) in 56 ms on localhost (12/52)\n",
      "2015-07-02 04:54:44,295 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,295 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,298 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 114.0 (TID 1187, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,298 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 114.0 (TID 1187)\n",
      "2015-07-02 04:54:44,299 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 114.0 (TID 1180) in 62 ms on localhost (13/52)\n",
      "2015-07-02 04:54:44,299 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,300 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,300 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,300 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,301 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 114.0 (TID 1188, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,301 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 114.0 (TID 1188)\n",
      "2015-07-02 04:54:44,302 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 114.0 (TID 1176) in 82 ms on localhost (14/52)\n",
      "2015-07-02 04:54:44,304 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,305 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,304 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -36, init = 76, finish = 0\n",
      "2015-07-02 04:54:44,305 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 114.0 (TID 1182). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,306 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 114.0 (TID 1189, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,306 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 114.0 (TID 1189)\n",
      "2015-07-02 04:54:44,306 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,306 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,307 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 114.0 (TID 1182) in 45 ms on localhost (15/52)\n",
      "2015-07-02 04:54:44,308 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,309 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,329 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -57, init = 103, finish = 0\n",
      "2015-07-02 04:54:44,330 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 114.0 (TID 1183). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,330 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 114.0 (TID 1190, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,331 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 114.0 (TID 1190)\n",
      "2015-07-02 04:54:44,331 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 114.0 (TID 1183) in 50 ms on localhost (16/52)\n",
      "2015-07-02 04:54:44,333 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,333 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,334 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -63, init = 105, finish = 0\n",
      "2015-07-02 04:54:44,335 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 114.0 (TID 1184). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,336 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 114.0 (TID 1191, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,336 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 114.0 (TID 1191)\n",
      "2015-07-02 04:54:44,336 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 114.0 (TID 1184) in 46 ms on localhost (17/52)\n",
      "2015-07-02 04:54:44,338 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,338 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -53, init = 98, finish = 0\n",
      "2015-07-02 04:54:44,338 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,339 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 114.0 (TID 1185). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,339 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -55, init = 99, finish = 0\n",
      "2015-07-02 04:54:44,339 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 114.0 (TID 1192, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,340 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 114.0 (TID 1186). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,340 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 114.0 (TID 1192)\n",
      "2015-07-02 04:54:44,340 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 114.0 (TID 1193, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,341 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 114.0 (TID 1193)\n",
      "2015-07-02 04:54:44,341 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 114.0 (TID 1185) in 50 ms on localhost (18/52)\n",
      "2015-07-02 04:54:44,341 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 114.0 (TID 1186) in 49 ms on localhost (19/52)\n",
      "2015-07-02 04:54:44,344 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -64, init = 107, finish = 0\n",
      "2015-07-02 04:54:44,344 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,345 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,346 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,347 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -68, init = 111, finish = 0\n",
      "2015-07-02 04:54:44,347 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,348 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 114.0 (TID 1188). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,349 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,349 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 114.0 (TID 1189). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,351 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 114.0 (TID 1188) in 50 ms on localhost (20/52)\n",
      "2015-07-02 04:54:44,351 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 114.0 (TID 1194, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,351 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 114.0 (TID 1187). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,352 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 114.0 (TID 1194)\n",
      "2015-07-02 04:54:44,352 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 114.0 (TID 1195, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,352 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 114.0 (TID 1195)\n",
      "2015-07-02 04:54:44,352 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 114.0 (TID 1189) in 46 ms on localhost (21/52)\n",
      "2015-07-02 04:54:44,352 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 114.0 (TID 1187) in 55 ms on localhost (22/52)\n",
      "2015-07-02 04:54:44,353 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 114.0 (TID 1196, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,353 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 114.0 (TID 1196)\n",
      "2015-07-02 04:54:44,354 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,355 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,356 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,356 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,356 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,357 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,373 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -44, init = 85, finish = 0\n",
      "2015-07-02 04:54:44,374 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 114.0 (TID 1190). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,374 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 114.0 (TID 1197, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,374 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 114.0 (TID 1197)\n",
      "2015-07-02 04:54:44,375 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 114.0 (TID 1190) in 44 ms on localhost (23/52)\n",
      "2015-07-02 04:54:44,377 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,377 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,378 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,379 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 114.0 (TID 1191). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,380 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 114.0 (TID 1198, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,380 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 114.0 (TID 1198)\n",
      "2015-07-02 04:54:44,381 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 114.0 (TID 1191) in 44 ms on localhost (24/52)\n",
      "2015-07-02 04:54:44,383 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,383 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,385 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -37, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,386 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 114.0 (TID 1192). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,387 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 114.0 (TID 1199, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,387 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -42, init = 86, finish = 0\n",
      "2015-07-02 04:54:44,387 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 114.0 (TID 1199)\n",
      "2015-07-02 04:54:44,387 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 114.0 (TID 1192) in 48 ms on localhost (25/52)\n",
      "2015-07-02 04:54:44,388 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 114.0 (TID 1193). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,388 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 114.0 (TID 1200, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,389 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 114.0 (TID 1193) in 49 ms on localhost (26/52)\n",
      "2015-07-02 04:54:44,389 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 114.0 (TID 1200)\n",
      "2015-07-02 04:54:44,390 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,390 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,391 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,391 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,394 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -50, init = 91, finish = 0\n",
      "2015-07-02 04:54:44,395 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 114.0 (TID 1194). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,396 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 114.0 (TID 1201, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,396 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -55, init = 97, finish = 0\n",
      "2015-07-02 04:54:44,396 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 114.0 (TID 1201)\n",
      "2015-07-02 04:54:44,397 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 114.0 (TID 1195). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,397 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 114.0 (TID 1202, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,398 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -43, init = 85, finish = 0\n",
      "2015-07-02 04:54:44,398 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 114.0 (TID 1202)\n",
      "2015-07-02 04:54:44,398 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 114.0 (TID 1194) in 47 ms on localhost (27/52)\n",
      "2015-07-02 04:54:44,399 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 114.0 (TID 1195) in 47 ms on localhost (28/52)\n",
      "2015-07-02 04:54:44,399 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 114.0 (TID 1196). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,400 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 114.0 (TID 1203, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,401 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 114.0 (TID 1203)\n",
      "2015-07-02 04:54:44,401 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 114.0 (TID 1196) in 47 ms on localhost (29/52)\n",
      "2015-07-02 04:54:44,402 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,402 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,402 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,403 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,405 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,406 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,417 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,418 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 114.0 (TID 1197). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,418 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 114.0 (TID 1204, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,419 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 114.0 (TID 1204)\n",
      "2015-07-02 04:54:44,419 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 114.0 (TID 1197) in 45 ms on localhost (30/52)\n",
      "2015-07-02 04:54:44,421 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,421 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,423 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -36, init = 77, finish = 0\n",
      "2015-07-02 04:54:44,424 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 114.0 (TID 1198). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,425 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 114.0 (TID 1205, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,425 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 114.0 (TID 1205)\n",
      "2015-07-02 04:54:44,425 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 114.0 (TID 1198) in 45 ms on localhost (31/52)\n",
      "2015-07-02 04:54:44,427 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,427 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,430 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -38, init = 79, finish = 0\n",
      "2015-07-02 04:54:44,431 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 114.0 (TID 1199). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,431 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 114.0 (TID 1206, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,432 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 114.0 (TID 1199) in 45 ms on localhost (32/52)\n",
      "2015-07-02 04:54:44,432 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 114.0 (TID 1206)\n",
      "2015-07-02 04:54:44,432 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,432 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 114.0 (TID 1200). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,433 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 114.0 (TID 1207, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,434 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 114.0 (TID 1207)\n",
      "2015-07-02 04:54:44,434 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 114.0 (TID 1200) in 46 ms on localhost (33/52)\n",
      "2015-07-02 04:54:44,435 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,435 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,437 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,437 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,441 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -38, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,441 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 114.0 (TID 1201). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,442 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,442 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 114.0 (TID 1208, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,442 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 114.0 (TID 1202). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,443 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 114.0 (TID 1201) in 48 ms on localhost (34/52)\n",
      "2015-07-02 04:54:44,443 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 114.0 (TID 1208)\n",
      "2015-07-02 04:54:44,444 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 114.0 (TID 1209, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,444 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 114.0 (TID 1209)\n",
      "2015-07-02 04:54:44,444 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 114.0 (TID 1202) in 47 ms on localhost (35/52)\n",
      "2015-07-02 04:54:44,446 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -46, init = 87, finish = 0\n",
      "2015-07-02 04:54:44,447 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 114.0 (TID 1203). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,447 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 114.0 (TID 1210, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,448 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,449 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:44,449 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 114.0 (TID 1203) in 49 ms on localhost (36/52)\n",
      "2015-07-02 04:54:44,449 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 114.0 (TID 1210)\n",
      "2015-07-02 04:54:44,448 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,451 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,452 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,462 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -39, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,463 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 114.0 (TID 1204). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,464 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 114.0 (TID 1211, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,464 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 114.0 (TID 1211)\n",
      "2015-07-02 04:54:44,464 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 114.0 (TID 1204) in 46 ms on localhost (37/52)\n",
      "2015-07-02 04:54:44,465 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,466 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,467 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -38, init = 79, finish = 0\n",
      "2015-07-02 04:54:44,468 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 114.0 (TID 1205). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,468 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 114.0 (TID 1212, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,469 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 114.0 (TID 1212)\n",
      "2015-07-02 04:54:44,469 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 114.0 (TID 1205) in 45 ms on localhost (38/52)\n",
      "2015-07-02 04:54:44,471 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,471 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,474 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -39, init = 79, finish = 0\n",
      "2015-07-02 04:54:44,475 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 114.0 (TID 1206). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,475 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 114.0 (TID 1213, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,476 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 114.0 (TID 1206) in 45 ms on localhost (39/52)\n",
      "2015-07-02 04:54:44,476 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 114.0 (TID 1213)\n",
      "2015-07-02 04:54:44,478 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -37, init = 79, finish = 0\n",
      "2015-07-02 04:54:44,478 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 114.0 (TID 1207). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,479 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 114.0 (TID 1214, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,479 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,480 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 114.0 (TID 1214)\n",
      "2015-07-02 04:54:44,480 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 114.0 (TID 1207) in 47 ms on localhost (40/52)\n",
      "2015-07-02 04:54:44,482 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,483 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,487 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,488 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:44,488 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 114.0 (TID 1209). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,488 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 114.0 (TID 1208). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,489 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 114.0 (TID 1215, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,489 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 114.0 (TID 1209) in 45 ms on localhost (41/52)\n",
      "2015-07-02 04:54:44,489 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 114.0 (TID 1215)\n",
      "2015-07-02 04:54:44,490 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 114.0 (TID 1216, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,490 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 114.0 (TID 1216)\n",
      "2015-07-02 04:54:44,490 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 114.0 (TID 1208) in 48 ms on localhost (42/52)\n",
      "2015-07-02 04:54:44,491 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:44,492 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,492 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,492 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 114.0 (TID 1210). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,493 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 114.0 (TID 1217, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,493 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 114.0 (TID 1210) in 46 ms on localhost (43/52)\n",
      "2015-07-02 04:54:44,494 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 114.0 (TID 1217)\n",
      "2015-07-02 04:54:44,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,497 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,497 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,505 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -41, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,506 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 114.0 (TID 1211). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,506 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 114.0 (TID 1218, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,507 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 114.0 (TID 1218)\n",
      "2015-07-02 04:54:44,507 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 114.0 (TID 1211) in 44 ms on localhost (44/52)\n",
      "2015-07-02 04:54:44,509 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,509 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,511 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,512 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 114.0 (TID 1212). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 114.0 (TID 1219, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,513 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 114.0 (TID 1219)\n",
      "2015-07-02 04:54:44,513 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 114.0 (TID 1212) in 45 ms on localhost (45/52)\n",
      "2015-07-02 04:54:44,515 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 50 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,515 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,519 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,520 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 114.0 (TID 1213). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,521 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 114.0 (TID 1213) in 45 ms on localhost (46/52)\n",
      "2015-07-02 04:54:44,522 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:44,522 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 114.0 (TID 1214). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,523 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 114.0 (TID 1214) in 44 ms on localhost (47/52)\n",
      "2015-07-02 04:54:44,531 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -42, init = 82, finish = 0\n",
      "2015-07-02 04:54:44,532 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 114.0 (TID 1216). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,533 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 114.0 (TID 1216) in 44 ms on localhost (48/52)\n",
      "2015-07-02 04:54:44,535 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -39, init = 83, finish = 0\n",
      "2015-07-02 04:54:44,536 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 114.0 (TID 1215). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,536 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,537 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 114.0 (TID 1215) in 48 ms on localhost (49/52)\n",
      "2015-07-02 04:54:44,537 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 114.0 (TID 1217). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,538 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 114.0 (TID 1217) in 46 ms on localhost (50/52)\n",
      "2015-07-02 04:54:44,549 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -37, init = 78, finish = 0\n",
      "2015-07-02 04:54:44,550 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 114.0 (TID 1218). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,550 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 114.0 (TID 1218) in 44 ms on localhost (51/52)\n",
      "2015-07-02 04:54:44,555 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,556 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 114.0 (TID 1219). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,557 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 114.0 (TID 1219) in 43 ms on localhost (52/52)\n",
      "2015-07-02 04:54:44,557 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 114.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:44,557 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 114 (collect at <ipython-input-3-4ba038715317>:141) finished in 0.387 s\n",
      "2015-07-02 04:54:44,558 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 7 finished: collect at <ipython-input-3-4ba038715317>:141, took 1.254581 s\n",
      "2015-07-02 04:54:44,570 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-3-4ba038715317>:146\n",
      "2015-07-02 04:54:44,571 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 45 is 154 bytes\n",
      "2015-07-02 04:54:44,572 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 44 is 157 bytes\n",
      "2015-07-02 04:54:44,572 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 43 is 169 bytes\n",
      "2015-07-02 04:54:44,573 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 42 is 390 bytes\n",
      "2015-07-02 04:54:44,573 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 41 is 349 bytes\n",
      "2015-07-02 04:54:44,574 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 40 is 156 bytes\n",
      "2015-07-02 04:54:44,574 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 39 is 347 bytes\n",
      "2015-07-02 04:54:44,575 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 38 is 156 bytes\n",
      "2015-07-02 04:54:44,576 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 37 is 368 bytes\n",
      "2015-07-02 04:54:44,577 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 36 is 406 bytes\n",
      "2015-07-02 04:54:44,578 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 35 is 508 bytes\n",
      "2015-07-02 04:54:44,579 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 34 is 157 bytes\n",
      "2015-07-02 04:54:44,579 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 33 is 157 bytes\n",
      "2015-07-02 04:54:44,580 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 32 is 466 bytes\n",
      "2015-07-02 04:54:44,581 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 31 is 157 bytes\n",
      "2015-07-02 04:54:44,581 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 30 is 157 bytes\n",
      "2015-07-02 04:54:44,582 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 29 is 509 bytes\n",
      "2015-07-02 04:54:44,583 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 28 is 157 bytes\n",
      "2015-07-02 04:54:44,584 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 27 is 476 bytes\n",
      "2015-07-02 04:54:44,586 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.MapOutputTrackerMaster (Logging.scala:logInfo(59)) - Size of output statuses for shuffle 26 is 482 bytes\n",
      "2015-07-02 04:54:44,587 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 8 (collect at <ipython-input-3-4ba038715317>:146) with 52 output partitions (allowLocal=false)\n",
      "2015-07-02 04:54:44,587 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 135(collect at <ipython-input-3-4ba038715317>:146)\n",
      "2015-07-02 04:54:44,587 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 134)\n",
      "2015-07-02 04:54:44,588 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
      "2015-07-02 04:54:44,589 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 135 (PythonRDD[353] at collect at <ipython-input-3-4ba038715317>:146), which has no missing parents\n",
      "2015-07-02 04:54:44,590 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(7888) called with curMem=3496168, maxMem=278302556\n",
      "2015-07-02 04:54:44,590 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_67 stored as values in memory (estimated size 7.7 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:44,591 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4949) called with curMem=3504056, maxMem=278302556\n",
      "2015-07-02 04:54:44,592 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_67_piece0 stored as bytes in memory (estimated size 4.8 KB, free 262.1 MB)\n",
      "2015-07-02 04:54:44,592 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_67_piece0 in memory on localhost:40918 (size: 4.8 KB, free: 264.9 MB)\n",
      "2015-07-02 04:54:44,592 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_67_piece0\n",
      "2015-07-02 04:54:44,593 INFO  [sparkDriver-akka.actor.default-dispatcher-14] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 67 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 04:54:44,596 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 52 missing tasks from Stage 135 (PythonRDD[353] at collect at <ipython-input-3-4ba038715317>:146)\n",
      "2015-07-02 04:54:44,596 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 135.0 with 52 tasks\n",
      "2015-07-02 04:54:44,597 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 135.0 (TID 1220, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,598 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 135.0 (TID 1221, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,598 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 2.0 in stage 135.0 (TID 1222, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,598 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 3.0 in stage 135.0 (TID 1223, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,598 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 4.0 in stage 135.0 (TID 1224, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,599 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 5.0 in stage 135.0 (TID 1225, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,599 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 6.0 in stage 135.0 (TID 1226, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,599 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 135.0 (TID 1220)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 2.0 in stage 135.0 (TID 1222)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 135.0 (TID 1221)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 4.0 in stage 135.0 (TID 1224)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 5.0 in stage 135.0 (TID 1225)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 6.0 in stage 135.0 (TID 1226)\n",
      "2015-07-02 04:54:44,600 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 3.0 in stage 135.0 (TID 1223)\n",
      "2015-07-02 04:54:44,604 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,604 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,605 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,604 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,605 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,605 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,607 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,607 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,607 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,608 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,609 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,609 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,612 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,612 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,645 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -103, init = 147, finish = 0\n",
      "2015-07-02 04:54:44,646 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 135.0 (TID 1221). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,646 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -116, init = 160, finish = 0\n",
      "2015-07-02 04:54:44,647 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 7.0 in stage 135.0 (TID 1227, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,647 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 135.0 (TID 1222). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,647 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 8.0 in stage 135.0 (TID 1228, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,645 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -117, init = 162, finish = 0\n",
      "2015-07-02 04:54:44,648 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 135.0 (TID 1220). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,648 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 135.0 (TID 1221) in 50 ms on localhost (1/52)\n",
      "2015-07-02 04:54:44,656 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 2.0 in stage 135.0 (TID 1222) in 58 ms on localhost (2/52)\n",
      "2015-07-02 04:54:44,654 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -91, init = 137, finish = 0\n",
      "2015-07-02 04:54:44,653 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 8.0 in stage 135.0 (TID 1228)\n",
      "2015-07-02 04:54:44,654 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 7.0 in stage 135.0 (TID 1227)\n",
      "2015-07-02 04:54:44,654 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -85, init = 131, finish = 1\n",
      "2015-07-02 04:54:44,653 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -103, init = 153, finish = 0\n",
      "2015-07-02 04:54:44,658 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 9.0 in stage 135.0 (TID 1229, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,658 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 51, boot = -105, init = 155, finish = 1\n",
      "2015-07-02 04:54:44,663 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 135.0 (TID 1223). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,663 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 135.0 (TID 1220) in 66 ms on localhost (3/52)\n",
      "2015-07-02 04:54:44,663 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 135.0 (TID 1226). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,664 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 9.0 in stage 135.0 (TID 1229)\n",
      "2015-07-02 04:54:44,658 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 135.0 (TID 1225). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,664 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 10.0 in stage 135.0 (TID 1230, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,669 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 10.0 in stage 135.0 (TID 1230)\n",
      "2015-07-02 04:54:44,662 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 135.0 (TID 1224). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,669 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 11.0 in stage 135.0 (TID 1231, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,671 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 11.0 in stage 135.0 (TID 1231)\n",
      "2015-07-02 04:54:44,671 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 3.0 in stage 135.0 (TID 1223) in 73 ms on localhost (4/52)\n",
      "2015-07-02 04:54:44,672 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 12.0 in stage 135.0 (TID 1232, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,673 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 12.0 in stage 135.0 (TID 1232)\n",
      "2015-07-02 04:54:44,673 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 13.0 in stage 135.0 (TID 1233, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,673 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 13.0 in stage 135.0 (TID 1233)\n",
      "2015-07-02 04:54:44,673 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 6.0 in stage 135.0 (TID 1226) in 74 ms on localhost (5/52)\n",
      "2015-07-02 04:54:44,674 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 4.0 in stage 135.0 (TID 1224) in 76 ms on localhost (6/52)\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,675 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 5.0 in stage 135.0 (TID 1225) in 76 ms on localhost (7/52)\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,676 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,676 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,675 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,676 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,677 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,677 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,677 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,680 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,680 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,715 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -137, init = 186, finish = 0\n",
      "2015-07-02 04:54:44,716 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 135.0 (TID 1228). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,716 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 14.0 in stage 135.0 (TID 1234, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,717 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -139, init = 189, finish = 0\n",
      "2015-07-02 04:54:44,717 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 135.0 (TID 1227). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,718 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 15.0 in stage 135.0 (TID 1235, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,719 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 15.0 in stage 135.0 (TID 1235)\n",
      "2015-07-02 04:54:44,722 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -119, init = 165, finish = 0\n",
      "2015-07-02 04:54:44,723 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -128, init = 172, finish = 0\n",
      "2015-07-02 04:54:44,723 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 135.0 (TID 1232). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,723 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 135.0 (TID 1230). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,724 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -131, init = 177, finish = 1\n",
      "2015-07-02 04:54:44,724 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -129, init = 175, finish = 0\n",
      "2015-07-02 04:54:44,724 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 135.0 (TID 1229). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,726 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 135.0 (TID 1231). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,724 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 16.0 in stage 135.0 (TID 1236, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,728 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -113, init = 164, finish = 1\n",
      "2015-07-02 04:54:44,729 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 135.0 (TID 1233). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,729 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 17.0 in stage 135.0 (TID 1237, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,730 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 17.0 in stage 135.0 (TID 1237)\n",
      "2015-07-02 04:54:44,730 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 16.0 in stage 135.0 (TID 1236)\n",
      "2015-07-02 04:54:44,730 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 18.0 in stage 135.0 (TID 1238, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,730 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 19.0 in stage 135.0 (TID 1239, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,730 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 19.0 in stage 135.0 (TID 1239)\n",
      "2015-07-02 04:54:44,731 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 20.0 in stage 135.0 (TID 1240, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,731 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 20.0 in stage 135.0 (TID 1240)\n",
      "2015-07-02 04:54:44,732 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 7.0 in stage 135.0 (TID 1227) in 86 ms on localhost (8/52)\n",
      "2015-07-02 04:54:44,732 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 10.0 in stage 135.0 (TID 1230) in 68 ms on localhost (9/52)\n",
      "2015-07-02 04:54:44,733 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 11.0 in stage 135.0 (TID 1231) in 63 ms on localhost (10/52)\n",
      "2015-07-02 04:54:44,733 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 14.0 in stage 135.0 (TID 1234)\n",
      "2015-07-02 04:54:44,733 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 13.0 in stage 135.0 (TID 1233) in 60 ms on localhost (11/52)\n",
      "2015-07-02 04:54:44,733 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 8.0 in stage 135.0 (TID 1228) in 86 ms on localhost (12/52)\n",
      "2015-07-02 04:54:44,734 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 12.0 in stage 135.0 (TID 1232) in 61 ms on localhost (13/52)\n",
      "2015-07-02 04:54:44,734 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 18.0 in stage 135.0 (TID 1238)\n",
      "2015-07-02 04:54:44,735 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 9.0 in stage 135.0 (TID 1229) in 77 ms on localhost (14/52)\n",
      "2015-07-02 04:54:44,738 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,738 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,739 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,739 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,743 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,742 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,745 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,746 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,746 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,746 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,778 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -77, init = 123, finish = 0\n",
      "2015-07-02 04:54:44,778 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 135.0 (TID 1237). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,779 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 21.0 in stage 135.0 (TID 1241, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,779 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 21.0 in stage 135.0 (TID 1241)\n",
      "2015-07-02 04:54:44,780 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -66, init = 113, finish = 0\n",
      "2015-07-02 04:54:44,779 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 17.0 in stage 135.0 (TID 1237) in 50 ms on localhost (15/52)\n",
      "2015-07-02 04:54:44,780 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 135.0 (TID 1240). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,781 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 22.0 in stage 135.0 (TID 1242, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,781 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 22.0 in stage 135.0 (TID 1242)\n",
      "2015-07-02 04:54:44,781 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 20.0 in stage 135.0 (TID 1240) in 50 ms on localhost (16/52)\n",
      "2015-07-02 04:54:44,783 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -72, init = 118, finish = 0\n",
      "2015-07-02 04:54:44,784 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 135.0 (TID 1234). 870 bytes result sent to driver\n",
      "2015-07-02 04:54:44,783 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -69, init = 121, finish = 0\n",
      "2015-07-02 04:54:44,785 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 53, boot = -71, init = 123, finish = 1\n",
      "2015-07-02 04:54:44,787 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 23.0 in stage 135.0 (TID 1243, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,788 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 23.0 in stage 135.0 (TID 1243)\n",
      "2015-07-02 04:54:44,789 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 14.0 in stage 135.0 (TID 1234) in 72 ms on localhost (17/52)\n",
      "2015-07-02 04:54:44,789 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,789 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,790 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -71, init = 115, finish = 0\n",
      "2015-07-02 04:54:44,791 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -70, init = 115, finish = 0\n",
      "2015-07-02 04:54:44,792 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 135.0 (TID 1238). 1509 bytes result sent to driver\n",
      "2015-07-02 04:54:44,792 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,792 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,795 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 18.0 in stage 135.0 (TID 1238) in 64 ms on localhost (18/52)\n",
      "2015-07-02 04:54:44,796 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 24.0 in stage 135.0 (TID 1244, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,798 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 24.0 in stage 135.0 (TID 1244)\n",
      "2015-07-02 04:54:44,798 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,798 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,803 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 135.0 (TID 1239). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,804 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 135.0 (TID 1235). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,805 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 135.0 (TID 1236). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,805 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 25.0 in stage 135.0 (TID 1245, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,806 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 25.0 in stage 135.0 (TID 1245)\n",
      "2015-07-02 04:54:44,807 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 26.0 in stage 135.0 (TID 1246, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,808 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,809 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,809 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 26.0 in stage 135.0 (TID 1246)\n",
      "2015-07-02 04:54:44,809 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 19.0 in stage 135.0 (TID 1239) in 78 ms on localhost (19/52)\n",
      "2015-07-02 04:54:44,810 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,810 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,811 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 27.0 in stage 135.0 (TID 1247, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,811 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 16.0 in stage 135.0 (TID 1236) in 87 ms on localhost (20/52)\n",
      "2015-07-02 04:54:44,811 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 27.0 in stage 135.0 (TID 1247)\n",
      "2015-07-02 04:54:44,812 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 15.0 in stage 135.0 (TID 1235) in 93 ms on localhost (21/52)\n",
      "2015-07-02 04:54:44,813 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,813 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,814 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,814 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,830 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -43, init = 92, finish = 0\n",
      "2015-07-02 04:54:44,831 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 135.0 (TID 1241). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,831 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 28.0 in stage 135.0 (TID 1248, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,832 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 28.0 in stage 135.0 (TID 1248)\n",
      "2015-07-02 04:54:44,832 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 21.0 in stage 135.0 (TID 1241) in 54 ms on localhost (22/52)\n",
      "2015-07-02 04:54:44,833 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -46, init = 95, finish = 1\n",
      "2015-07-02 04:54:44,834 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,834 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 135.0 (TID 1242). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,834 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,834 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 29.0 in stage 135.0 (TID 1249, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,835 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 29.0 in stage 135.0 (TID 1249)\n",
      "2015-07-02 04:54:44,836 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 22.0 in stage 135.0 (TID 1242) in 55 ms on localhost (23/52)\n",
      "2015-07-02 04:54:44,837 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,837 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,839 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -52, init = 101, finish = 1\n",
      "2015-07-02 04:54:44,840 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 135.0 (TID 1243). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,840 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 30.0 in stage 135.0 (TID 1250, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,841 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 23.0 in stage 135.0 (TID 1243) in 54 ms on localhost (24/52)\n",
      "2015-07-02 04:54:44,841 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 30.0 in stage 135.0 (TID 1250)\n",
      "2015-07-02 04:54:44,843 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,843 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,850 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -75, init = 117, finish = 0\n",
      "2015-07-02 04:54:44,851 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -72, init = 122, finish = 0\n",
      "2015-07-02 04:54:44,851 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 135.0 (TID 1245). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,851 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 135.0 (TID 1244). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,852 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 31.0 in stage 135.0 (TID 1251, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,852 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 31.0 in stage 135.0 (TID 1251)\n",
      "2015-07-02 04:54:44,852 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 32.0 in stage 135.0 (TID 1252, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,853 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 32.0 in stage 135.0 (TID 1252)\n",
      "2015-07-02 04:54:44,853 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 25.0 in stage 135.0 (TID 1245) in 48 ms on localhost (25/52)\n",
      "2015-07-02 04:54:44,853 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 24.0 in stage 135.0 (TID 1244) in 58 ms on localhost (26/52)\n",
      "2015-07-02 04:54:44,855 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,856 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,858 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -81, init = 124, finish = 1\n",
      "2015-07-02 04:54:44,858 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -71, init = 112, finish = 1\n",
      "2015-07-02 04:54:44,859 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 135.0 (TID 1246). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,860 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 33.0 in stage 135.0 (TID 1253, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,860 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 26.0 in stage 135.0 (TID 1246) in 54 ms on localhost (27/52)\n",
      "2015-07-02 04:54:44,861 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 33.0 in stage 135.0 (TID 1253)\n",
      "2015-07-02 04:54:44,861 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 135.0 (TID 1247). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,862 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 34.0 in stage 135.0 (TID 1254, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,863 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 34.0 in stage 135.0 (TID 1254)\n",
      "2015-07-02 04:54:44,862 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,864 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,864 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,864 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 2 ms\n",
      "2015-07-02 04:54:44,863 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 27.0 in stage 135.0 (TID 1247) in 53 ms on localhost (28/52)\n",
      "2015-07-02 04:54:44,865 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,865 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,874 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -43, init = 85, finish = 0\n",
      "2015-07-02 04:54:44,875 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 135.0 (TID 1248). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,876 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 35.0 in stage 135.0 (TID 1255, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,876 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 35.0 in stage 135.0 (TID 1255)\n",
      "2015-07-02 04:54:44,876 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 28.0 in stage 135.0 (TID 1248) in 45 ms on localhost (29/52)\n",
      "2015-07-02 04:54:44,878 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -30, init = 71, finish = 0\n",
      "2015-07-02 04:54:44,878 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,878 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,878 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 135.0 (TID 1249). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,879 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 36.0 in stage 135.0 (TID 1256, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,879 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 29.0 in stage 135.0 (TID 1249) in 45 ms on localhost (30/52)\n",
      "2015-07-02 04:54:44,880 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 36.0 in stage 135.0 (TID 1256)\n",
      "2015-07-02 04:54:44,882 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,883 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,884 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -32, init = 73, finish = 1\n",
      "2015-07-02 04:54:44,884 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 135.0 (TID 1250). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,885 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 37.0 in stage 135.0 (TID 1257, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,885 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 30.0 in stage 135.0 (TID 1250) in 45 ms on localhost (31/52)\n",
      "2015-07-02 04:54:44,885 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 37.0 in stage 135.0 (TID 1257)\n",
      "2015-07-02 04:54:44,888 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,888 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,897 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -44, init = 87, finish = 0\n",
      "2015-07-02 04:54:44,898 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 135.0 (TID 1251). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,899 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 38.0 in stage 135.0 (TID 1258, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,899 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 38.0 in stage 135.0 (TID 1258)\n",
      "2015-07-02 04:54:44,899 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 31.0 in stage 135.0 (TID 1251) in 48 ms on localhost (32/52)\n",
      "2015-07-02 04:54:44,901 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,902 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,904 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -46, init = 94, finish = 0\n",
      "2015-07-02 04:54:44,905 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 135.0 (TID 1252). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,905 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 39.0 in stage 135.0 (TID 1259, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,906 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 39.0 in stage 135.0 (TID 1259)\n",
      "2015-07-02 04:54:44,906 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -55, init = 97, finish = 1\n",
      "2015-07-02 04:54:44,906 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -62, init = 104, finish = 0\n",
      "2015-07-02 04:54:44,906 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 135.0 (TID 1253). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,906 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 135.0 (TID 1254). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,906 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 32.0 in stage 135.0 (TID 1252) in 54 ms on localhost (33/52)\n",
      "2015-07-02 04:54:44,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 40.0 in stage 135.0 (TID 1260, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,908 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 41.0 in stage 135.0 (TID 1261, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,909 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 41.0 in stage 135.0 (TID 1261)\n",
      "2015-07-02 04:54:44,909 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 34.0 in stage 135.0 (TID 1254) in 47 ms on localhost (34/52)\n",
      "2015-07-02 04:54:44,909 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 33.0 in stage 135.0 (TID 1253) in 50 ms on localhost (35/52)\n",
      "2015-07-02 04:54:44,910 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 40.0 in stage 135.0 (TID 1260)\n",
      "2015-07-02 04:54:44,912 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,913 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,915 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,915 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,915 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,915 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,920 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -39, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,920 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 135.0 (TID 1255). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,921 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 42.0 in stage 135.0 (TID 1262, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,921 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 35.0 in stage 135.0 (TID 1255) in 46 ms on localhost (36/52)\n",
      "2015-07-02 04:54:44,921 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 42.0 in stage 135.0 (TID 1262)\n",
      "2015-07-02 04:54:44,923 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -39, init = 80, finish = 0\n",
      "2015-07-02 04:54:44,923 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,924 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,923 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 135.0 (TID 1256). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,924 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 43.0 in stage 135.0 (TID 1263, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,925 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 43.0 in stage 135.0 (TID 1263)\n",
      "2015-07-02 04:54:44,925 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 36.0 in stage 135.0 (TID 1256) in 46 ms on localhost (37/52)\n",
      "2015-07-02 04:54:44,927 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,928 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,928 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -41, init = 82, finish = 0\n",
      "2015-07-02 04:54:44,929 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 135.0 (TID 1257). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,929 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 44.0 in stage 135.0 (TID 1264, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,930 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 44.0 in stage 135.0 (TID 1264)\n",
      "2015-07-02 04:54:44,930 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 37.0 in stage 135.0 (TID 1257) in 45 ms on localhost (38/52)\n",
      "2015-07-02 04:54:44,932 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,932 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,943 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -43, init = 85, finish = 0\n",
      "2015-07-02 04:54:44,943 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 135.0 (TID 1258). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,944 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 45.0 in stage 135.0 (TID 1265, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,944 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Running task 45.0 in stage 135.0 (TID 1265)\n",
      "2015-07-02 04:54:44,944 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 38.0 in stage 135.0 (TID 1258) in 46 ms on localhost (39/52)\n",
      "2015-07-02 04:54:44,946 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,946 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,954 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -49, init = 92, finish = 0\n",
      "2015-07-02 04:54:44,954 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 135.0 (TID 1261). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,955 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 46.0 in stage 135.0 (TID 1266, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,955 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Running task 46.0 in stage 135.0 (TID 1266)\n",
      "2015-07-02 04:54:44,955 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 41.0 in stage 135.0 (TID 1261) in 47 ms on localhost (40/52)\n",
      "2015-07-02 04:54:44,956 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -43, init = 88, finish = 1\n",
      "2015-07-02 04:54:44,956 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -43, init = 86, finish = 0\n",
      "2015-07-02 04:54:44,957 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 135.0 (TID 1260). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,957 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 47.0 in stage 135.0 (TID 1267, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,958 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Running task 47.0 in stage 135.0 (TID 1267)\n",
      "2015-07-02 04:54:44,958 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 135.0 (TID 1259). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,958 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,958 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 04:54:44,959 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 40.0 in stage 135.0 (TID 1260) in 52 ms on localhost (41/52)\n",
      "2015-07-02 04:54:44,960 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 48.0 in stage 135.0 (TID 1268, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,960 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 48.0 in stage 135.0 (TID 1268)\n",
      "2015-07-02 04:54:44,960 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 39.0 in stage 135.0 (TID 1259) in 55 ms on localhost (42/52)\n",
      "2015-07-02 04:54:44,963 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,964 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,965 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -41, init = 83, finish = 0\n",
      "2015-07-02 04:54:44,965 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 135.0 (TID 1262). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:44,965 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,966 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,966 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 49.0 in stage 135.0 (TID 1269, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,969 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -39, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,969 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Running task 49.0 in stage 135.0 (TID 1269)\n",
      "2015-07-02 04:54:44,969 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 135.0 (TID 1263). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,970 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 42.0 in stage 135.0 (TID 1262) in 50 ms on localhost (43/52)\n",
      "2015-07-02 04:54:44,970 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 50.0 in stage 135.0 (TID 1270, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,971 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Running task 50.0 in stage 135.0 (TID 1270)\n",
      "2015-07-02 04:54:44,971 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 43.0 in stage 135.0 (TID 1263) in 47 ms on localhost (44/52)\n",
      "2015-07-02 04:54:44,972 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,973 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,973 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,973 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 135.0 (TID 1264). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:44,975 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 51.0 in stage 135.0 (TID 1271, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 04:54:44,975 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 51.0 in stage 135.0 (TID 1271)\n",
      "2015-07-02 04:54:44,975 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 44.0 in stage 135.0 (TID 1264) in 46 ms on localhost (45/52)\n",
      "2015-07-02 04:54:44,976 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,977 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,978 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 38 non-empty blocks out of 52 blocks\n",
      "2015-07-02 04:54:44,979 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 04:54:44,986 INFO  [Executor task launch worker-20] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -40, init = 81, finish = 0\n",
      "2015-07-02 04:54:44,987 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 135.0 (TID 1265). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:44,987 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 45.0 in stage 135.0 (TID 1265) in 44 ms on localhost (46/52)\n",
      "2015-07-02 04:54:44,998 INFO  [Executor task launch worker-18] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -44, init = 86, finish = 0\n",
      "2015-07-02 04:54:44,999 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 135.0 (TID 1266). 1394 bytes result sent to driver\n",
      "2015-07-02 04:54:44,999 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 46.0 in stage 135.0 (TID 1266) in 45 ms on localhost (47/52)\n",
      "2015-07-02 04:54:45,005 INFO  [Executor task launch worker-10] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -42, init = 87, finish = 0\n",
      "2015-07-02 04:54:45,005 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 135.0 (TID 1267). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:45,006 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -46, init = 88, finish = 0\n",
      "2015-07-02 04:54:45,006 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 47.0 in stage 135.0 (TID 1267) in 49 ms on localhost (48/52)\n",
      "2015-07-02 04:54:45,006 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 135.0 (TID 1268). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:45,007 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 48.0 in stage 135.0 (TID 1268) in 47 ms on localhost (49/52)\n",
      "2015-07-02 04:54:45,013 INFO  [Executor task launch worker-13] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -44, init = 86, finish = 0\n",
      "2015-07-02 04:54:45,014 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 135.0 (TID 1269). 1148 bytes result sent to driver\n",
      "2015-07-02 04:54:45,014 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 49.0 in stage 135.0 (TID 1269) in 48 ms on localhost (50/52)\n",
      "2015-07-02 04:54:45,016 INFO  [Executor task launch worker-19] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -41, init = 84, finish = 0\n",
      "2015-07-02 04:54:45,017 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 135.0 (TID 1270). 1263 bytes result sent to driver\n",
      "2015-07-02 04:54:45,018 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 50.0 in stage 135.0 (TID 1270) in 48 ms on localhost (51/52)\n",
      "2015-07-02 04:54:45,019 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -42, init = 83, finish = 0\n",
      "2015-07-02 04:54:45,019 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 135.0 (TID 1271). 1014 bytes result sent to driver\n",
      "2015-07-02 04:54:45,020 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 51.0 in stage 135.0 (TID 1271) in 46 ms on localhost (52/52)\n",
      "2015-07-02 04:54:45,020 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "2015-07-02 04:54:45,021 INFO  [sparkDriver-akka.actor.default-dispatcher-14] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 135 (collect at <ipython-input-3-4ba038715317>:146) finished in 0.424 s\n",
      "2015-07-02 04:54:45,022 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 8 finished: collect at <ipython-input-3-4ba038715317>:146, took 0.451686 s\n"
     ]
    }
   ],
   "source": [
    "''' ############## Part 2 ############## '''\n",
    "\n",
    "########### initialize and get data ############\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_rdd_from_file(fn):\n",
    "    '''\n",
    "    Input:  fn- full path of the source file\n",
    "    Output: the according RDD\n",
    "    '''\n",
    "    header_tokens = {u'\"participantID.A\"', u'\"user\"', u'\"participantID\"'}\n",
    "    rdd = sc.textFile(fn)\n",
    "    lines = rdd.map(lambda line: (line.split(\",\")))\n",
    "    if 'gender' in fn: \n",
    "        lines = lines.map(lambda line: (''.join(['\"',line[0],'\"']),1 if line[1] == u'F' else 0))\n",
    "        \n",
    "    # drop first line #\n",
    "    lines = lines.filter(lambda line: line[0] not in header_tokens)\n",
    "    return lines\n",
    "\n",
    "############ average number of occurences per day ############\n",
    "def avg_num_of_occurences_per_day(rdd,id_idx,date_idx):\n",
    "    '''\n",
    "    Input:  \n",
    "        rdd- the data in RDD container\n",
    "        id_idx- the index of the ID field in rdd\n",
    "        date_idx- the index of the date field in rdd\n",
    "    Output: id-date RDD\n",
    "    '''\n",
    "    return rdd.map(lambda line: (line[id_idx],[{datetime.strptime(line[date_idx],'%Y-%m-%d %H:%M:%S').date()},1.0,1]) ) \\\n",
    "                   .reduceByKey(lambda v1,v2: [v1[0].union(v2[0]),v1[1]+v2[1],(v1[1]+v2[1])/len(v1[0].union(v2[0]))]) \\\n",
    "                    .map(lambda line: (line[0],line[1][-1]) )\n",
    "\n",
    "############ average of distinct contacts per day ############\n",
    "def avg_num_of_distinct_contacts(rdd,id_idx,date_idx,contact_idx):\n",
    "    '''\n",
    "    Input:  \n",
    "        rdd- the data in RDD container\n",
    "        id_idx- the index of the ID field in rdd\n",
    "        date_idx- the index of the date field in rdd\n",
    "        contact_idx- the index of the contact field (participantB/Mac address/hashed phone number) in rdd\n",
    "    Output: id-averaged_number_of_distict_contacts RDD\n",
    "    '''\n",
    "    return rdd.map(lambda line: (line[id_idx],\\\n",
    "                                 [{datetime.strptime(line[date_idx],'%Y-%m-%d %H:%M:%S').date()},\\\n",
    "                                  {line[contact_idx]},\\\n",
    "                                  1.0 ]   ) )\\\n",
    "                    .reduceByKey(lambda v1,v2: [v1[0].union(v2[0]),\\\n",
    "                                                v1[1].union(v2[1]),\\\n",
    "                                                1.0*len(v1[1].union(v2[1]))/len(v1[0].union(v2[0]) )]) \\\n",
    "                    .map(lambda line: (line[0],line[1][-1]) )\n",
    "\n",
    "############ portion of incoming calls ############\n",
    "def portion_of_incoming(rdd,id_idx,direction_idx):\n",
    "    '''\n",
    "    Input:  \n",
    "        rdd- the data in RDD container\n",
    "        id_idx- the index of the ID field in rdd\n",
    "        direction_idx- the index of the type field (incoming/outgoing) in rdd\n",
    "    Output: id-proportion_of_incoming_events RDD\n",
    "    '''\n",
    "    return rdd.map(lambda line: ((line[id_idx],line[direction_idx]),1) ) \\\n",
    "                    .reduceByKey(lambda v1,v2: v1+v2) \\\n",
    "                    .map(lambda line: (line[0][0],[line[1],float(line[1]) if line[0][1] in {u'\"incoming+\"',u'\"incoming\"'} else 0.0]) ) \\\n",
    "                    .reduceByKey(lambda v1,v2: [v1[0]+v2[0],(v1[1]+v2[1])/(v1[0]+v2[0])]) \\\n",
    "                    .map(lambda line: (line[0],line[1][-1]) )\n",
    "\n",
    "############ export to file ############\n",
    "def export_list_to_file(l,fn,features_names=None):\n",
    "    '''\n",
    "    Input:  \n",
    "        l- a list of lists which is required to export\n",
    "        fn- the full path of the output file\n",
    "        features_names- a list of strings represents the features' name for the header\n",
    "    Output: nothing\n",
    "    '''    \n",
    "    import csv\n",
    "    with open(fn, 'wb') as csvfile:\n",
    "        f = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        if features_names: f.writerow(['id']+features_names)\n",
    "        for i in l:\n",
    "            f.writerow(i)\n",
    "    return\n",
    "\n",
    "############ extract features from the source files and export them to a file ############   \n",
    "def feature_engineering_phase(sms_fn,calls_fn,meetings_fn,gender_fn,output_fn):\n",
    "    '''\n",
    "    Input:  \n",
    "        sms_fn- the full path of the SMSs source file\n",
    "        calls_fn- the full path of the calls source file\n",
    "        meetings_fn- the full path of the bluetooth source file\n",
    "        gender- the full path of the gender source file\n",
    "        output_fn- the full path of the output file\n",
    "    Output: a list of the features names\n",
    "    '''    \n",
    "    # loading the source files:\n",
    "    sms = create_rdd_from_file(sms_fn)\n",
    "    calls = create_rdd_from_file(calls_fn)\n",
    "    meetings = create_rdd_from_file(meetings_fn)\n",
    "    gender = create_rdd_from_file(gender_fn)\n",
    "    # extracting the average number of events per day:\n",
    "    average_number_of_sms_per_day = avg_num_of_occurences_per_day(sms,0,2)\n",
    "    average_number_of_calls_per_day = avg_num_of_occurences_per_day(calls,0,2)\n",
    "    average_number_of_meetings_per_day = avg_num_of_occurences_per_day(meetings,0,1)\n",
    "    # extracting the number of distinct contacts per day:\n",
    "    average_distinct_calls_contacts_per_day = avg_num_of_distinct_contacts(calls,0,2,5)\n",
    "    average_distinct_sms_contacts_per_day = avg_num_of_distinct_contacts(sms,0,2,4)\n",
    "    average_distinct_meetings_contacts_per_day = avg_num_of_distinct_contacts(meetings,0,1,3)\n",
    "    # extracting the proportion of incoming events:\n",
    "    portion_of_incoming_calls = portion_of_incoming(calls,0,3)\n",
    "    portion_of_incoming_sms = portion_of_incoming(sms,0,3)\n",
    "\n",
    "    ############ average duration of calls ############\n",
    "    total_average_duration = calls.filter(lambda line: line[4] != u'').map(lambda line: (1,[int(line[4]),1.0]) ) \\\n",
    "                       .reduceByKey(lambda v1,v2: [(v1[0]*v1[1]+v2[0]*v2[1])/(v1[1]+v2[1]),v1[1]+v2[1]]).collect()\n",
    "    total_average_duration = total_average_duration[0][1][0]\n",
    "\n",
    "    average_duration = calls.map(lambda line: (line[0],[int(line[4]) if line[4]!=u'' else total_average_duration,1.0]) ) \\\n",
    "                        .reduceByKey(lambda v1,v2: [(v1[0]*v1[1]+v2[0]*v2[1])/(v1[1]+v2[1]),v1[1]+v2[1]]) \\\n",
    "                        .map(lambda line: (line[0],line[1][0]) )     \n",
    "\n",
    "\n",
    "    print \"Finished creating the RDDs\"\n",
    "\n",
    "    ############ join all the features ############\n",
    "    '''\n",
    "    Features:\n",
    "    1. average_number_of_sms_per_day\n",
    "    2. average_number_of_calls_per_day\n",
    "    3. average_number_of_meetings_per_day\n",
    "    4. average_distinct_sms_contacts_per_day\n",
    "    5. average_distinct_calls_contacts_per_day\n",
    "    6. average_distinct_meetings_contacts_per_day\n",
    "    7. portion_of_incoming_calls\n",
    "    8. portion_of_incoming_sms\n",
    "    9. average_duration\n",
    "    '''\n",
    "    features_names = ['average_number_of_sms_per_day', 'average_number_of_calls_per_day', 'average_number_of_meetings_per_day',\\\n",
    "                     'average_distinct_sms_contacts_per_day', 'average_distinct_calls_contacts_per_day', 'average_distinct_meetings_contacts_per_day',\\\n",
    "                     'portion_of_incoming_calls', 'portion_of_incoming_sms',\\\n",
    "                     'average_duration','gender']\n",
    "\n",
    "    features = average_number_of_sms_per_day\n",
    "\n",
    "    for i in features_names[1:]:\n",
    "        features = features.join(eval(i))\n",
    "\n",
    "    features = features.map(lambda line: [line[0],line[1][0][0][0][0][0][0][0][0][0]\\\n",
    "                                          , line[1][0][0][0][0][0][0][0][0][1]\\\n",
    "                                          , line[1][0][0][0][0][0][0][0][1]\\\n",
    "                                          , line[1][0][0][0][0][0][0][1]\\\n",
    "                                          , line[1][0][0][0][0][0][1]\\\n",
    "                                          , line[1][0][0][0][0][1]\\\n",
    "                                          , line[1][0][0][0][1]\\\n",
    "                                          , line[1][0][0][1]\\\n",
    "                                          , line[1][0][1]\\\n",
    "                                          , line[1][1]] )\n",
    "\n",
    "    print \"Finished joinning RDDs\"\n",
    "\n",
    "    ############ normalize each feature to 0-1 scale ############\n",
    "\n",
    "    minimum = features.map(lambda line: (1, line[1:]) ) \\\n",
    "              .reduceByKey(lambda v1,v2: [min(v1[i],v2[i]) for i in range(len(v1))]).collect()[0][1]\n",
    "\n",
    "    maximum = features.map(lambda line: (1, line[1:]) ) \\\n",
    "              .reduceByKey(lambda v1,v2: [max(v1[i],v2[i]) for i in range(len(v1))]).collect()[0][1]\n",
    "\n",
    "    norm_features = features.map(lambda line: [line[0]]+[(1.0*line[i+1]-minimum[i])/(maximum[i]-minimum[i]) \\\n",
    "                                                         for i in range(len(maximum))])\n",
    "    print \"Finished normalizing\"\n",
    "\n",
    "    export_list_to_file(norm_features.collect(), output_fn, features_names)\n",
    "    return features_names\n",
    "\n",
    "# Creates features_initial.csv, which contains the extracted features of the participants and their label.\n",
    "sms_fn = '/assignment/input/SMSLog.csv'\n",
    "calls_fn = '/assignment/input/CallLog.csv'\n",
    "meetings_fn = '/assignment/input/BluetoothProximity.csv'\n",
    "gender_fn = '/assignment/input/gender_labels.csv'\n",
    "output_fn = '/assignment/features/features_initial.csv'\n",
    "features_names = feature_engineering_phase(sms_fn,calls_fn,meetings_fn,gender_fn,output_fn)\n",
    "\n",
    "\n",
    "\n",
    "############ import file and write to train and test ############   \n",
    "def split_file_to_train_test(source_fn,train_fn,test_fn,features_names=None):\n",
    "    '''\n",
    "    Input:  \n",
    "        source_fn- the full path of the source file\n",
    "        train_fn- the full path of the train output file\n",
    "        test_fn- the full path of the test output file\n",
    "        features_names- a list of strings represent the features' name, which returned from feature_engineering_phase()\n",
    "    Output: nothing\n",
    "    '''    \n",
    "    file = np.genfromtxt(source_fn, delimiter=\",\", dtype= None)\n",
    "    file = file[1:]   # remove the header\n",
    "    np.random.shuffle(file)\n",
    "    test, train = file[:(file.shape[0]/3)], file[(file.shape[0]/3):]\n",
    "    \n",
    "    export_list_to_file(test,test_fn,features_names)\n",
    "    export_list_to_file(train,train_fn,features_names)\n",
    "    return\n",
    "\n",
    "test_fn = '/assignment/features/testing_initial.csv'\n",
    "train_fn = '/assignment/features/training_initial.csv'\n",
    "\n",
    "# split the the data to train and test sets:\n",
    "split_file_to_train_test(output_fn,train_fn,test_fn,features_names)\n",
    "\n",
    "print \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015-07-02 05:21:36,080 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=3810764, maxMem=278302556\n",
      "2015-07-02 05:21:36,081 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_151 stored as values in memory (estimated size 241.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:36,098 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=4057714, maxMem=278302556\n",
      "2015-07-02 05:21:36,098 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_151_piece0 stored as bytes in memory (estimated size 19.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:36,099 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_151_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,099 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_151_piece0\n",
      "2015-07-02 05:21:36,099 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 151 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 05:21:36,109 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 05:21:36,111 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:50\n",
      "2015-07-02 05:21:36,111 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 50 (collect at <ipython-input-5-d5e94eb36cd1>:50) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,111 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 217(collect at <ipython-input-5-d5e94eb36cd1>:50)\n",
      "2015-07-02 05:21:36,111 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
      "2015-07-02 05:21:36,113 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
      "2015-07-02 05:21:36,113 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 217 (PythonRDD[601] at collect at <ipython-input-5-d5e94eb36cd1>:50), which has no missing parents\n",
      "2015-07-02 05:21:36,113 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5384) called with curMem=4077179, maxMem=278302556\n",
      "2015-07-02 05:21:36,114 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_152 stored as values in memory (estimated size 5.3 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:36,114 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3413) called with curMem=4082563, maxMem=278302556\n",
      "2015-07-02 05:21:36,115 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_152_piece0 stored as bytes in memory (estimated size 3.3 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:36,115 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_152_piece0 in memory on localhost:40918 (size: 3.3 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,115 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_152_piece0\n",
      "2015-07-02 05:21:36,116 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 152 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,116 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 217 (PythonRDD[601] at collect at <ipython-input-5-d5e94eb36cd1>:50)\n",
      "2015-07-02 05:21:36,116 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 217.0 with 2 tasks\n",
      "2015-07-02 05:21:36,117 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 217.0 (TID 1434, localhost, PROCESS_LOCAL, 1309 bytes)\n",
      "2015-07-02 05:21:36,117 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 217.0 (TID 1435, localhost, PROCESS_LOCAL, 1309 bytes)\n",
      "2015-07-02 05:21:36,118 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 217.0 (TID 1434)\n",
      "2015-07-02 05:21:36,118 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 217.0 (TID 1435)\n",
      "2015-07-02 05:21:36,125 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/testing_initial.csv:0+4034\n",
      "2015-07-02 05:21:36,128 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/testing_initial.csv:4034+4035\n",
      "2015-07-02 05:21:36,129 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 9, boot = 4, init = 4, finish = 1\n",
      "2015-07-02 05:21:36,129 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 217.0 (TID 1434). 4160 bytes result sent to driver\n",
      "2015-07-02 05:21:36,130 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 217.0 (TID 1434) in 13 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,132 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 11, boot = 6, init = 4, finish = 1\n",
      "2015-07-02 05:21:36,132 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 217.0 (TID 1435). 4160 bytes result sent to driver\n",
      "2015-07-02 05:21:36,133 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 217.0 (TID 1435) in 16 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,133 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 217.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,133 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 217 (collect at <ipython-input-5-d5e94eb36cd1>:50) finished in 0.017 s\n",
      "2015-07-02 05:21:36,134 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 50 finished: collect at <ipython-input-5-d5e94eb36cd1>:50, took 0.022837 s\n",
      "2015-07-02 05:21:36,141 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(246950) called with curMem=4085976, maxMem=278302556\n",
      "2015-07-02 05:21:36,142 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_153 stored as values in memory (estimated size 241.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:36,156 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(19465) called with curMem=4332926, maxMem=278302556\n",
      "2015-07-02 05:21:36,156 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_153_piece0 stored as bytes in memory (estimated size 19.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:36,157 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_153_piece0 in memory on localhost:40918 (size: 19.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,157 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_153_piece0\n",
      "2015-07-02 05:21:36,158 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 153 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "2015-07-02 05:21:36,169 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
      "2015-07-02 05:21:36,190 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,190 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 606 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,190 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 51 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,190 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 219(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,191 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 218)\n",
      "2015-07-02 05:21:36,191 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 218)\n",
      "2015-07-02 05:21:36,192 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 218 (PairwiseRDD[606] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,193 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4352391, maxMem=278302556\n",
      "2015-07-02 05:21:36,193 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_154 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:36,195 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4361831, maxMem=278302556\n",
      "2015-07-02 05:21:36,195 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_154_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,195 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_154_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,195 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_154_piece0\n",
      "2015-07-02 05:21:36,196 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 154 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,196 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 218 (PairwiseRDD[606] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,196 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 218.0 with 2 tasks\n",
      "2015-07-02 05:21:36,197 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 218.0 (TID 1436, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,197 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 218.0 (TID 1437, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,197 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 218.0 (TID 1437)\n",
      "2015-07-02 05:21:36,198 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 218.0 (TID 1436)\n",
      "2015-07-02 05:21:36,199 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,199 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:36,242 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -35, init = 76, finish = 3\n",
      "2015-07-02 05:21:36,243 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -34, init = 77, finish = 2\n",
      "2015-07-02 05:21:36,243 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 218.0 (TID 1437). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,244 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 218.0 (TID 1436). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,244 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 218.0 (TID 1437) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,246 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 218.0 (TID 1436) in 49 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,246 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 218.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,246 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 218 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:36,246 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:36,246 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:36,246 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 219)\n",
      "2015-07-02 05:21:36,247 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:36,248 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 219: List()\n",
      "2015-07-02 05:21:36,249 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 219 (PythonRDD[609] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:36,249 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4368001, maxMem=278302556\n",
      "2015-07-02 05:21:36,250 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_155 stored as values in memory (estimated size 5.1 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,251 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4373201, maxMem=278302556\n",
      "2015-07-02 05:21:36,251 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_155_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,252 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_155_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,252 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_155_piece0\n",
      "2015-07-02 05:21:36,252 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 155 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,253 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 219 (PythonRDD[609] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,253 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 219.0 with 2 tasks\n",
      "2015-07-02 05:21:36,254 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 219.0 (TID 1438, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,254 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 219.0 (TID 1439, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,254 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 219.0 (TID 1439)\n",
      "2015-07-02 05:21:36,254 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 219.0 (TID 1438)\n",
      "2015-07-02 05:21:36,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,297 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:36,298 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:36,298 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 219.0 (TID 1438). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:36,298 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 219.0 (TID 1439). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:36,300 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 219.0 (TID 1438) in 46 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,302 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 219.0 (TID 1439) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,303 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 219.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,306 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 219 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:36,306 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 51 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.116378 s\n",
      "2015-07-02 05:21:36,349 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,350 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 612 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,350 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 52 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,350 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 221(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,350 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 220)\n",
      "2015-07-02 05:21:36,351 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 220)\n",
      "2015-07-02 05:21:36,352 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 220 (PairwiseRDD[612] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,352 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4376482, maxMem=278302556\n",
      "2015-07-02 05:21:36,353 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_156 stored as values in memory (estimated size 9.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,353 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6174) called with curMem=4385922, maxMem=278302556\n",
      "2015-07-02 05:21:36,354 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_156_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,354 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_156_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:36,354 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_156_piece0\n",
      "2015-07-02 05:21:36,355 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 156 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,355 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 220 (PairwiseRDD[612] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,355 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 220.0 with 2 tasks\n",
      "2015-07-02 05:21:36,356 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 220.0 (TID 1440, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,356 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 220.0 (TID 1441, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,356 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 220.0 (TID 1441)\n",
      "2015-07-02 05:21:36,356 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 220.0 (TID 1440)\n",
      "2015-07-02 05:21:36,358 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,358 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:36,400 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -46, init = 87, finish = 2\n",
      "2015-07-02 05:21:36,400 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -46, init = 87, finish = 2\n",
      "2015-07-02 05:21:36,402 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 220.0 (TID 1441). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,402 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 220.0 (TID 1440). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,404 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 220.0 (TID 1441) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,404 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 220.0 (TID 1440) in 49 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,404 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 220.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,405 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 220 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:36,405 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:36,405 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:36,406 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 221)\n",
      "2015-07-02 05:21:36,406 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:36,407 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 221: List()\n",
      "2015-07-02 05:21:36,407 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 221 (PythonRDD[615] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:36,408 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4392096, maxMem=278302556\n",
      "2015-07-02 05:21:36,408 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_157 stored as values in memory (estimated size 5.1 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,409 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4397296, maxMem=278302556\n",
      "2015-07-02 05:21:36,409 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_157_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,409 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_157_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,410 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_157_piece0\n",
      "2015-07-02 05:21:36,410 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 157 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,411 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 221 (PythonRDD[615] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,411 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 221.0 with 2 tasks\n",
      "2015-07-02 05:21:36,411 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 221.0 (TID 1442, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,411 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 221.0 (TID 1443, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,412 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 221.0 (TID 1442)\n",
      "2015-07-02 05:21:36,412 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 221.0 (TID 1443)\n",
      "2015-07-02 05:21:36,414 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,414 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:36,414 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,414 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,475 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -5, init = 66, finish = 0\n",
      "2015-07-02 05:21:36,475 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 61, boot = -5, init = 66, finish = 0\n",
      "2015-07-02 05:21:36,475 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 221.0 (TID 1442). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:36,475 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 221.0 (TID 1443). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:36,476 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 221.0 (TID 1442) in 65 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,476 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 221.0 (TID 1443) in 65 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,476 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 221.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,477 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 221 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.066 s\n",
      "2015-07-02 05:21:36,478 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 52 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.128272 s\n",
      "2015-07-02 05:21:36,508 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,508 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 618 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,508 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 53 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,509 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 223(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,509 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 222)\n",
      "2015-07-02 05:21:36,509 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 222)\n",
      "2015-07-02 05:21:36,511 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 222 (PairwiseRDD[618] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,511 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4400576, maxMem=278302556\n",
      "2015-07-02 05:21:36,511 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_158 stored as values in memory (estimated size 9.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,512 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6168) called with curMem=4410016, maxMem=278302556\n",
      "2015-07-02 05:21:36,512 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_158_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_158_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,513 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_158_piece0\n",
      "2015-07-02 05:21:36,513 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 158 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,514 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 222 (PairwiseRDD[618] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,514 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 222.0 with 2 tasks\n",
      "2015-07-02 05:21:36,514 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 222.0 (TID 1444, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,515 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 222.0 (TID 1445, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,515 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 222.0 (TID 1444)\n",
      "2015-07-02 05:21:36,515 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 222.0 (TID 1445)\n",
      "2015-07-02 05:21:36,517 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,517 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:36,559 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -33, init = 74, finish = 1\n",
      "2015-07-02 05:21:36,559 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -33, init = 74, finish = 2\n",
      "2015-07-02 05:21:36,560 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 222.0 (TID 1444). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,561 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 222.0 (TID 1445). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,561 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 222.0 (TID 1444) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,562 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 222.0 (TID 1445) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,562 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 222.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,563 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 222 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:36,563 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:36,563 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:36,563 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 223)\n",
      "2015-07-02 05:21:36,563 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:36,564 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 223: List()\n",
      "2015-07-02 05:21:36,564 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 223 (PythonRDD[621] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:36,565 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4416184, maxMem=278302556\n",
      "2015-07-02 05:21:36,566 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_159 stored as values in memory (estimated size 5.1 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,567 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4421384, maxMem=278302556\n",
      "2015-07-02 05:21:36,567 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_159_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,568 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_159_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,568 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_159_piece0\n",
      "2015-07-02 05:21:36,569 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 159 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,569 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 223 (PythonRDD[621] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,569 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 223.0 with 2 tasks\n",
      "2015-07-02 05:21:36,570 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 223.0 (TID 1446, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,570 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 223.0 (TID 1447, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,570 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 223.0 (TID 1447)\n",
      "2015-07-02 05:21:36,570 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 223.0 (TID 1446)\n",
      "2015-07-02 05:21:36,572 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,572 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,572 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,573 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:36,612 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -3, init = 44, finish = 0\n",
      "2015-07-02 05:21:36,612 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -1, init = 42, finish = 0\n",
      "2015-07-02 05:21:36,612 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 223.0 (TID 1447). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:36,612 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 223.0 (TID 1446). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:36,613 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 223.0 (TID 1447) in 43 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,614 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 223.0 (TID 1446) in 44 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,614 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 223.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,614 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 223 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.045 s\n",
      "2015-07-02 05:21:36,615 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 53 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.106795 s\n",
      "2015-07-02 05:21:36,653 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,654 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 624 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,654 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 54 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,654 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 225(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,654 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 224)\n",
      "2015-07-02 05:21:36,655 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 224)\n",
      "2015-07-02 05:21:36,656 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 224 (PairwiseRDD[624] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,656 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4424665, maxMem=278302556\n",
      "2015-07-02 05:21:36,657 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_160 stored as values in memory (estimated size 9.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,657 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6164) called with curMem=4434105, maxMem=278302556\n",
      "2015-07-02 05:21:36,658 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_160_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,658 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_160_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,658 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_160_piece0\n",
      "2015-07-02 05:21:36,659 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 160 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,659 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 224 (PairwiseRDD[624] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,659 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 224.0 with 2 tasks\n",
      "2015-07-02 05:21:36,660 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 224.0 (TID 1448, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,660 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 224.0 (TID 1449, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,660 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 224.0 (TID 1448)\n",
      "2015-07-02 05:21:36,660 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 224.0 (TID 1449)\n",
      "2015-07-02 05:21:36,662 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:36,662 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,705 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -41, init = 82, finish = 2\n",
      "2015-07-02 05:21:36,706 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -41, init = 83, finish = 2\n",
      "2015-07-02 05:21:36,709 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 224.0 (TID 1449). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,709 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 224.0 (TID 1448). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,710 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 224.0 (TID 1449) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,711 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 224.0 (TID 1448) in 52 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,711 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 224.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,712 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 224 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.053 s\n",
      "2015-07-02 05:21:36,713 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:36,713 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:36,713 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 225)\n",
      "2015-07-02 05:21:36,713 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:36,714 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 225: List()\n",
      "2015-07-02 05:21:36,714 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 225 (PythonRDD[627] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:36,715 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4440269, maxMem=278302556\n",
      "2015-07-02 05:21:36,715 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_161 stored as values in memory (estimated size 5.1 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,716 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4445469, maxMem=278302556\n",
      "2015-07-02 05:21:36,716 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_161_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,717 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_161_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,717 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_161_piece0\n",
      "2015-07-02 05:21:36,717 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 161 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,718 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 225 (PythonRDD[627] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,718 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 225.0 with 2 tasks\n",
      "2015-07-02 05:21:36,718 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 225.0 (TID 1450, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,718 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 225.0 (TID 1451, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,719 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 225.0 (TID 1450)\n",
      "2015-07-02 05:21:36,719 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 225.0 (TID 1451)\n",
      "2015-07-02 05:21:36,720 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,721 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:36,721 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,721 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,761 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:36,762 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -7, init = 48, finish = 0\n",
      "2015-07-02 05:21:36,762 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 225.0 (TID 1451). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:36,762 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 225.0 (TID 1450). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:36,763 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 225.0 (TID 1451) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,763 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 225.0 (TID 1450) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,763 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 225.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,764 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 225 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:36,764 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 54 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.110747 s\n",
      "2015-07-02 05:21:36,803 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,804 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 630 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,804 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 55 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,804 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 227(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,804 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 226)\n",
      "2015-07-02 05:21:36,805 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 226)\n",
      "2015-07-02 05:21:36,806 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 226 (PairwiseRDD[630] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,807 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4448750, maxMem=278302556\n",
      "2015-07-02 05:21:36,807 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_162 stored as values in memory (estimated size 9.2 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,808 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6163) called with curMem=4458190, maxMem=278302556\n",
      "2015-07-02 05:21:36,808 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_162_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.2 MB)\n",
      "2015-07-02 05:21:36,809 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_162_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,809 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_162_piece0\n",
      "2015-07-02 05:21:36,809 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 162 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,809 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 226 (PairwiseRDD[630] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,810 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 226.0 with 2 tasks\n",
      "2015-07-02 05:21:36,810 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 226.0 (TID 1452, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,810 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 226.0 (TID 1453, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,810 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 226.0 (TID 1453)\n",
      "2015-07-02 05:21:36,811 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 226.0 (TID 1452)\n",
      "2015-07-02 05:21:36,812 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:36,814 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,856 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -41, init = 84, finish = 2\n",
      "2015-07-02 05:21:36,856 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -41, init = 84, finish = 1\n",
      "2015-07-02 05:21:36,859 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 226.0 (TID 1452). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,860 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 226.0 (TID 1453). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:36,861 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 226.0 (TID 1452) in 51 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,861 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 226.0 (TID 1453) in 51 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,861 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 226.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,862 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 226 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.052 s\n",
      "2015-07-02 05:21:36,862 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:36,863 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:36,863 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 227)\n",
      "2015-07-02 05:21:36,863 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:36,864 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 227: List()\n",
      "2015-07-02 05:21:36,865 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 227 (PythonRDD[633] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:36,865 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4464353, maxMem=278302556\n",
      "2015-07-02 05:21:36,865 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_163 stored as values in memory (estimated size 5.1 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:36,866 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3282) called with curMem=4469553, maxMem=278302556\n",
      "2015-07-02 05:21:36,867 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_163_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:36,867 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_163_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,868 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_163_piece0\n",
      "2015-07-02 05:21:36,868 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 163 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,868 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 227 (PythonRDD[633] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,868 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 227.0 with 2 tasks\n",
      "2015-07-02 05:21:36,869 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 227.0 (TID 1454, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,869 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 227.0 (TID 1455, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:36,869 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 227.0 (TID 1455)\n",
      "2015-07-02 05:21:36,869 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 227.0 (TID 1454)\n",
      "2015-07-02 05:21:36,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:36,871 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:36,913 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 48, finish = 0\n",
      "2015-07-02 05:21:36,913 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -6, init = 48, finish = 1\n",
      "2015-07-02 05:21:36,913 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 227.0 (TID 1454). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:36,913 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 227.0 (TID 1455). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:36,915 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 227.0 (TID 1454) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:36,915 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 227.0 (TID 1455) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:36,915 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 227.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:36,916 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 227 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:36,916 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 55 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.112578 s\n",
      "2015-07-02 05:21:36,953 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:36,954 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 636 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,954 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 56 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:36,954 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 229(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,954 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 228)\n",
      "2015-07-02 05:21:36,955 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 228)\n",
      "2015-07-02 05:21:36,957 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 228 (PairwiseRDD[636] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:36,957 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4472835, maxMem=278302556\n",
      "2015-07-02 05:21:36,957 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_164 stored as values in memory (estimated size 9.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:36,958 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4482275, maxMem=278302556\n",
      "2015-07-02 05:21:36,958 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_164_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:36,958 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_164_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:36,958 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_164_piece0\n",
      "2015-07-02 05:21:36,959 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 164 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:36,959 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 228 (PairwiseRDD[636] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:36,959 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 228.0 with 2 tasks\n",
      "2015-07-02 05:21:36,960 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 228.0 (TID 1456, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,960 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 228.0 (TID 1457, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:36,960 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 228.0 (TID 1456)\n",
      "2015-07-02 05:21:36,960 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 228.0 (TID 1457)\n",
      "2015-07-02 05:21:36,964 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:36,964 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,007 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -41, init = 84, finish = 3\n",
      "2015-07-02 05:21:37,007 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -41, init = 84, finish = 3\n",
      "2015-07-02 05:21:37,008 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 228.0 (TID 1457). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,009 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 228.0 (TID 1456). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,009 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 228.0 (TID 1457) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,010 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 228.0 (TID 1456) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,010 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 228.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,011 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 228 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.052 s\n",
      "2015-07-02 05:21:37,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 229)\n",
      "2015-07-02 05:21:37,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,013 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 229: List()\n",
      "2015-07-02 05:21:37,013 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 229 (PythonRDD[639] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:37,014 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4488445, maxMem=278302556\n",
      "2015-07-02 05:21:37,014 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_165 stored as values in memory (estimated size 5.1 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,015 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4493645, maxMem=278302556\n",
      "2015-07-02 05:21:37,016 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_165_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,016 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_165_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,016 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_165_piece0\n",
      "2015-07-02 05:21:37,016 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 165 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,017 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 229 (PythonRDD[639] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,017 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 229.0 with 2 tasks\n",
      "2015-07-02 05:21:37,017 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 229.0 (TID 1458, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,018 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 229.0 (TID 1459, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,018 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 229.0 (TID 1459)\n",
      "2015-07-02 05:21:37,018 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 229.0 (TID 1458)\n",
      "2015-07-02 05:21:37,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:37,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,020 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,061 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 45, finish = 1\n",
      "2015-07-02 05:21:37,061 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 45, finish = 1\n",
      "2015-07-02 05:21:37,061 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 229.0 (TID 1458). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:37,061 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 229.0 (TID 1459). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:37,062 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 229.0 (TID 1458) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,063 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 229.0 (TID 1459) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,063 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 229.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,063 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 229 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:37,064 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 56 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.110581 s\n",
      "2015-07-02 05:21:37,284 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:37,284 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 642 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,285 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 57 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:37,285 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 231(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,285 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 230)\n",
      "2015-07-02 05:21:37,286 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 230)\n",
      "2015-07-02 05:21:37,287 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 230 (PairwiseRDD[642] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:37,288 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4496926, maxMem=278302556\n",
      "2015-07-02 05:21:37,288 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_166 stored as values in memory (estimated size 9.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,289 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4506366, maxMem=278302556\n",
      "2015-07-02 05:21:37,289 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_166_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,290 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_166_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,290 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_166_piece0\n",
      "2015-07-02 05:21:37,290 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 166 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,290 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 230 (PairwiseRDD[642] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,291 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 230.0 with 2 tasks\n",
      "2015-07-02 05:21:37,291 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 230.0 (TID 1460, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,291 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 230.0 (TID 1461, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,292 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 230.0 (TID 1461)\n",
      "2015-07-02 05:21:37,292 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 230.0 (TID 1460)\n",
      "2015-07-02 05:21:37,293 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:37,294 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,336 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -224, init = 264, finish = 3\n",
      "2015-07-02 05:21:37,337 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -224, init = 265, finish = 3\n",
      "2015-07-02 05:21:37,338 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 230.0 (TID 1461). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,339 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 230.0 (TID 1460). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,339 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 230.0 (TID 1461) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,339 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 230.0 (TID 1460) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,340 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 230.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,340 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 230 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:37,340 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,340 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,340 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 231)\n",
      "2015-07-02 05:21:37,341 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,341 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 231: List()\n",
      "2015-07-02 05:21:37,342 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 231 (PythonRDD[645] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:37,342 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4512536, maxMem=278302556\n",
      "2015-07-02 05:21:37,343 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_167 stored as values in memory (estimated size 5.1 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,344 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4517736, maxMem=278302556\n",
      "2015-07-02 05:21:37,344 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_167_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,345 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_167_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,346 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_167_piece0\n",
      "2015-07-02 05:21:37,346 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 167 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,346 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 231 (PythonRDD[645] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,347 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 231.0 with 2 tasks\n",
      "2015-07-02 05:21:37,347 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 231.0 (TID 1462, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,347 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 231.0 (TID 1463, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,348 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 231.0 (TID 1462)\n",
      "2015-07-02 05:21:37,348 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 231.0 (TID 1463)\n",
      "2015-07-02 05:21:37,349 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,349 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,350 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,350 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,391 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -2, init = 45, finish = 0\n",
      "2015-07-02 05:21:37,391 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -3, init = 46, finish = 0\n",
      "2015-07-02 05:21:37,391 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 231.0 (TID 1463). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:37,391 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 231.0 (TID 1462). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:37,392 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 231.0 (TID 1463) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,392 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 231.0 (TID 1462) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,392 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 231.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,393 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 231 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:37,394 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 57 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.109789 s\n",
      "2015-07-02 05:21:37,424 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:37,425 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 648 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,425 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 58 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:37,425 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 233(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,425 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 232)\n",
      "2015-07-02 05:21:37,426 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 232)\n",
      "2015-07-02 05:21:37,427 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 232 (PairwiseRDD[648] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:37,428 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4521017, maxMem=278302556\n",
      "2015-07-02 05:21:37,428 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_168 stored as values in memory (estimated size 9.2 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,428 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4530457, maxMem=278302556\n",
      "2015-07-02 05:21:37,429 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_168_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.1 MB)\n",
      "2015-07-02 05:21:37,429 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_168_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,429 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_168_piece0\n",
      "2015-07-02 05:21:37,430 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 168 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,430 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 232 (PairwiseRDD[648] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,430 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 232.0 with 2 tasks\n",
      "2015-07-02 05:21:37,430 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 232.0 (TID 1464, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,431 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 232.0 (TID 1465, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,431 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 232.0 (TID 1464)\n",
      "2015-07-02 05:21:37,431 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 232.0 (TID 1465)\n",
      "2015-07-02 05:21:37,433 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:37,434 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,440 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 150\n",
      "2015-07-02 05:21:37,440 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_150\n",
      "2015-07-02 05:21:37,440 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_150 of size 5200 dropped from memory (free 273771129)\n",
      "2015-07-02 05:21:37,441 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_150_piece0\n",
      "2015-07-02 05:21:37,441 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_150_piece0 of size 3281 dropped from memory (free 273774410)\n",
      "2015-07-02 05:21:37,441 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_150_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,441 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_150_piece0\n",
      "2015-07-02 05:21:37,442 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 150\n",
      "2015-07-02 05:21:37,442 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 167\n",
      "2015-07-02 05:21:37,442 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_167_piece0\n",
      "2015-07-02 05:21:37,442 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_167_piece0 of size 3281 dropped from memory (free 273777691)\n",
      "2015-07-02 05:21:37,443 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_167_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,443 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_167_piece0\n",
      "2015-07-02 05:21:37,443 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_167\n",
      "2015-07-02 05:21:37,443 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_167 of size 5200 dropped from memory (free 273782891)\n",
      "2015-07-02 05:21:37,444 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 167\n",
      "2015-07-02 05:21:37,444 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 166\n",
      "2015-07-02 05:21:37,444 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_166_piece0\n",
      "2015-07-02 05:21:37,444 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_166_piece0 of size 6170 dropped from memory (free 273789061)\n",
      "2015-07-02 05:21:37,445 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_166_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,445 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_166_piece0\n",
      "2015-07-02 05:21:37,445 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_166\n",
      "2015-07-02 05:21:37,445 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_166 of size 9440 dropped from memory (free 273798501)\n",
      "2015-07-02 05:21:37,446 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 166\n",
      "2015-07-02 05:21:37,446 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 93\n",
      "2015-07-02 05:21:37,446 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 165\n",
      "2015-07-02 05:21:37,446 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_165\n",
      "2015-07-02 05:21:37,447 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_165 of size 5200 dropped from memory (free 273803701)\n",
      "2015-07-02 05:21:37,447 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_165_piece0\n",
      "2015-07-02 05:21:37,447 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_165_piece0 of size 3281 dropped from memory (free 273806982)\n",
      "2015-07-02 05:21:37,447 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_165_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,447 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_165_piece0\n",
      "2015-07-02 05:21:37,448 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 165\n",
      "2015-07-02 05:21:37,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 164\n",
      "2015-07-02 05:21:37,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_164\n",
      "2015-07-02 05:21:37,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_164 of size 9440 dropped from memory (free 273816422)\n",
      "2015-07-02 05:21:37,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_164_piece0\n",
      "2015-07-02 05:21:37,448 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_164_piece0 of size 6170 dropped from memory (free 273822592)\n",
      "2015-07-02 05:21:37,449 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_164_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,449 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_164_piece0\n",
      "2015-07-02 05:21:37,449 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 164\n",
      "2015-07-02 05:21:37,449 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 92\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 163\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_163_piece0\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_163_piece0 of size 3282 dropped from memory (free 273825874)\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_163_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_163_piece0\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_163\n",
      "2015-07-02 05:21:37,450 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_163 of size 5200 dropped from memory (free 273831074)\n",
      "2015-07-02 05:21:37,451 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 163\n",
      "2015-07-02 05:21:37,451 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 162\n",
      "2015-07-02 05:21:37,451 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_162_piece0\n",
      "2015-07-02 05:21:37,451 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_162_piece0 of size 6163 dropped from memory (free 273837237)\n",
      "2015-07-02 05:21:37,452 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_162_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,452 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_162_piece0\n",
      "2015-07-02 05:21:37,452 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_162\n",
      "2015-07-02 05:21:37,452 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_162 of size 9440 dropped from memory (free 273846677)\n",
      "2015-07-02 05:21:37,452 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 162\n",
      "2015-07-02 05:21:37,453 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 91\n",
      "2015-07-02 05:21:37,453 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 156\n",
      "2015-07-02 05:21:37,454 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_156_piece0\n",
      "2015-07-02 05:21:37,454 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_156_piece0 of size 6174 dropped from memory (free 273852851)\n",
      "2015-07-02 05:21:37,454 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_156_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,455 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_156_piece0\n",
      "2015-07-02 05:21:37,455 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_156\n",
      "2015-07-02 05:21:37,455 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_156 of size 9440 dropped from memory (free 273862291)\n",
      "2015-07-02 05:21:37,455 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 156\n",
      "2015-07-02 05:21:37,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 155\n",
      "2015-07-02 05:21:37,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_155\n",
      "2015-07-02 05:21:37,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_155 of size 5200 dropped from memory (free 273867491)\n",
      "2015-07-02 05:21:37,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_155_piece0\n",
      "2015-07-02 05:21:37,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_155_piece0 of size 3281 dropped from memory (free 273870772)\n",
      "2015-07-02 05:21:37,457 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_155_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,457 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_155_piece0\n",
      "2015-07-02 05:21:37,457 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 155\n",
      "2015-07-02 05:21:37,458 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 154\n",
      "2015-07-02 05:21:37,458 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_154_piece0\n",
      "2015-07-02 05:21:37,458 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_154_piece0 of size 6170 dropped from memory (free 273876942)\n",
      "2015-07-02 05:21:37,458 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_154_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_154_piece0\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_154\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_154 of size 9440 dropped from memory (free 273886382)\n",
      "2015-07-02 05:21:37,459 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 154\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 152\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_152\n",
      "2015-07-02 05:21:37,459 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_152 of size 5384 dropped from memory (free 273891766)\n",
      "2015-07-02 05:21:37,460 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_152_piece0\n",
      "2015-07-02 05:21:37,460 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_152_piece0 of size 3413 dropped from memory (free 273895179)\n",
      "2015-07-02 05:21:37,460 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_152_piece0 on localhost:40918 in memory (size: 3.3 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:37,460 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_152_piece0\n",
      "2015-07-02 05:21:37,461 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 152\n",
      "2015-07-02 05:21:37,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 151\n",
      "2015-07-02 05:21:37,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_151\n",
      "2015-07-02 05:21:37,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_151 of size 246950 dropped from memory (free 274142129)\n",
      "2015-07-02 05:21:37,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_151_piece0\n",
      "2015-07-02 05:21:37,461 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_151_piece0 of size 19465 dropped from memory (free 274161594)\n",
      "2015-07-02 05:21:37,462 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_151_piece0 on localhost:40918 in memory (size: 19.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,462 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_151_piece0\n",
      "2015-07-02 05:21:37,462 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 151\n",
      "2015-07-02 05:21:37,463 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 161\n",
      "2015-07-02 05:21:37,463 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_161_piece0\n",
      "2015-07-02 05:21:37,463 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_161_piece0 of size 3281 dropped from memory (free 274164875)\n",
      "2015-07-02 05:21:37,463 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_161_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,464 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_161_piece0\n",
      "2015-07-02 05:21:37,464 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_161\n",
      "2015-07-02 05:21:37,464 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_161 of size 5200 dropped from memory (free 274170075)\n",
      "2015-07-02 05:21:37,464 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 161\n",
      "2015-07-02 05:21:37,464 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 160\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_160\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_160 of size 9440 dropped from memory (free 274179515)\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_160_piece0\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_160_piece0 of size 6164 dropped from memory (free 274185679)\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_160_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,465 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_160_piece0\n",
      "2015-07-02 05:21:37,466 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 160\n",
      "2015-07-02 05:21:37,466 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 90\n",
      "2015-07-02 05:21:37,466 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 159\n",
      "2015-07-02 05:21:37,466 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_159\n",
      "2015-07-02 05:21:37,466 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_159 of size 5200 dropped from memory (free 274190879)\n",
      "2015-07-02 05:21:37,467 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_159_piece0\n",
      "2015-07-02 05:21:37,467 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_159_piece0 of size 3281 dropped from memory (free 274194160)\n",
      "2015-07-02 05:21:37,467 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_159_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,467 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_159_piece0\n",
      "2015-07-02 05:21:37,467 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 159\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 158\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_158\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_158 of size 9440 dropped from memory (free 274203600)\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_158_piece0\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_158_piece0 of size 6168 dropped from memory (free 274209768)\n",
      "2015-07-02 05:21:37,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_158_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,469 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_158_piece0\n",
      "2015-07-02 05:21:37,469 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 158\n",
      "2015-07-02 05:21:37,469 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 157\n",
      "2015-07-02 05:21:37,469 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_157_piece0\n",
      "2015-07-02 05:21:37,469 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_157_piece0 of size 3280 dropped from memory (free 274213048)\n",
      "2015-07-02 05:21:37,470 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_157_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,470 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_157_piece0\n",
      "2015-07-02 05:21:37,470 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_157\n",
      "2015-07-02 05:21:37,470 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_157 of size 5200 dropped from memory (free 274218248)\n",
      "2015-07-02 05:21:37,470 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 157\n",
      "2015-07-02 05:21:37,476 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -34, init = 76, finish = 2\n",
      "2015-07-02 05:21:37,476 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -33, init = 75, finish = 2\n",
      "2015-07-02 05:21:37,478 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 232.0 (TID 1464). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,478 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 232.0 (TID 1465). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,479 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 232.0 (TID 1464) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,480 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 232.0 (TID 1465) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,480 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 232.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,480 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 232 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:37,481 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,481 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,481 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 233)\n",
      "2015-07-02 05:21:37,481 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,482 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 233: List()\n",
      "2015-07-02 05:21:37,482 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 233 (PythonRDD[651] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:37,483 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4084308, maxMem=278302556\n",
      "2015-07-02 05:21:37,483 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_169 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4089508, maxMem=278302556\n",
      "2015-07-02 05:21:37,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_169_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,485 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_169_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,485 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_169_piece0\n",
      "2015-07-02 05:21:37,485 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 169 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,486 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 233 (PythonRDD[651] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,486 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 233.0 with 2 tasks\n",
      "2015-07-02 05:21:37,486 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 233.0 (TID 1466, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,487 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 233.0 (TID 1467, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,487 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 233.0 (TID 1467)\n",
      "2015-07-02 05:21:37,487 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 233.0 (TID 1466)\n",
      "2015-07-02 05:21:37,489 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,489 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,489 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,489 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,530 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:37,531 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:37,531 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 233.0 (TID 1466). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:37,531 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 233.0 (TID 1467). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:37,533 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 233.0 (TID 1466) in 46 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,535 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 233.0 (TID 1467) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,535 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 233.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,536 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 233 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:37,537 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 58 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.112568 s\n",
      "2015-07-02 05:21:37,583 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:37,583 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 654 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,583 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 59 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:37,584 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 235(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,584 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 234)\n",
      "2015-07-02 05:21:37,585 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 234)\n",
      "2015-07-02 05:21:37,586 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 234 (PairwiseRDD[654] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:37,588 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4092789, maxMem=278302556\n",
      "2015-07-02 05:21:37,588 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_170 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,589 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4102229, maxMem=278302556\n",
      "2015-07-02 05:21:37,589 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_170_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,590 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_170_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,590 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_170_piece0\n",
      "2015-07-02 05:21:37,590 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 170 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,591 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 234 (PairwiseRDD[654] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,591 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 234.0 with 2 tasks\n",
      "2015-07-02 05:21:37,592 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 234.0 (TID 1468, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,592 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 234.0 (TID 1469, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,593 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 234.0 (TID 1468)\n",
      "2015-07-02 05:21:37,593 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 234.0 (TID 1469)\n",
      "2015-07-02 05:21:37,595 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,596 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:37,638 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -49, init = 90, finish = 3\n",
      "2015-07-02 05:21:37,640 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 234.0 (TID 1468). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,640 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -49, init = 92, finish = 2\n",
      "2015-07-02 05:21:37,642 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 234.0 (TID 1468) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,644 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 234.0 (TID 1469). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,645 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 234.0 (TID 1469) in 53 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,646 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 234.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,646 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 234 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.054 s\n",
      "2015-07-02 05:21:37,646 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,646 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,646 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 235)\n",
      "2015-07-02 05:21:37,646 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,648 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 235: List()\n",
      "2015-07-02 05:21:37,648 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 235 (PythonRDD[657] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:37,650 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4108399, maxMem=278302556\n",
      "2015-07-02 05:21:37,651 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_171 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,651 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4113599, maxMem=278302556\n",
      "2015-07-02 05:21:37,652 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_171_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,652 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_171_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,652 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_171_piece0\n",
      "2015-07-02 05:21:37,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 171 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 235 (PythonRDD[657] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 235.0 with 2 tasks\n",
      "2015-07-02 05:21:37,654 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 235.0 (TID 1470, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,654 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 235.0 (TID 1471, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,655 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 235.0 (TID 1471)\n",
      "2015-07-02 05:21:37,655 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 235.0 (TID 1470)\n",
      "2015-07-02 05:21:37,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,658 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,659 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:37,698 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 50, finish = 0\n",
      "2015-07-02 05:21:37,699 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -7, init = 48, finish = 0\n",
      "2015-07-02 05:21:37,699 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 235.0 (TID 1470). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:37,699 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 235.0 (TID 1471). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:37,700 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 235.0 (TID 1470) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,700 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 235.0 (TID 1471) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,700 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 235.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,703 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 235 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:37,704 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 59 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.120843 s\n",
      "2015-07-02 05:21:37,741 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:37,742 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 660 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,742 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 60 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:37,742 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 237(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,742 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 236)\n",
      "2015-07-02 05:21:37,743 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 236)\n",
      "2015-07-02 05:21:37,745 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 236 (PairwiseRDD[660] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:37,745 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4116880, maxMem=278302556\n",
      "2015-07-02 05:21:37,746 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_172 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,746 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4126320, maxMem=278302556\n",
      "2015-07-02 05:21:37,747 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_172_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,747 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_172_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,747 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_172_piece0\n",
      "2015-07-02 05:21:37,748 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 172 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,748 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 236 (PairwiseRDD[660] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,748 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 236.0 with 2 tasks\n",
      "2015-07-02 05:21:37,748 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 236.0 (TID 1472, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,749 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 236.0 (TID 1473, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,749 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 236.0 (TID 1473)\n",
      "2015-07-02 05:21:37,749 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 236.0 (TID 1472)\n",
      "2015-07-02 05:21:37,751 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:37,752 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,799 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -41, init = 83, finish = 6\n",
      "2015-07-02 05:21:37,799 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -43, init = 85, finish = 6\n",
      "2015-07-02 05:21:37,803 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 236.0 (TID 1472). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,803 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 236.0 (TID 1473). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,805 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 236.0 (TID 1472) in 56 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,809 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 236.0 (TID 1473) in 60 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,809 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 236.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,810 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 236 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.062 s\n",
      "2015-07-02 05:21:37,810 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,810 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,810 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 237)\n",
      "2015-07-02 05:21:37,811 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,816 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 237: List()\n",
      "2015-07-02 05:21:37,817 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 237 (PythonRDD[663] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:37,819 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4132491, maxMem=278302556\n",
      "2015-07-02 05:21:37,819 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_173 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,821 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3282) called with curMem=4137691, maxMem=278302556\n",
      "2015-07-02 05:21:37,821 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_173_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,822 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_173_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,823 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_173_piece0\n",
      "2015-07-02 05:21:37,823 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 173 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,823 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 237 (PythonRDD[663] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,824 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 237.0 with 2 tasks\n",
      "2015-07-02 05:21:37,824 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 237.0 (TID 1474, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,824 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 237.0 (TID 1475, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:37,825 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 237.0 (TID 1475)\n",
      "2015-07-02 05:21:37,825 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 237.0 (TID 1474)\n",
      "2015-07-02 05:21:37,826 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,826 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:37,828 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:37,828 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:37,867 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 40, boot = -9, init = 49, finish = 0\n",
      "2015-07-02 05:21:37,867 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 237.0 (TID 1475). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:37,868 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 237.0 (TID 1475) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,898 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 71, boot = -9, init = 80, finish = 0\n",
      "2015-07-02 05:21:37,899 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 237.0 (TID 1474). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:37,902 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 237.0 (TID 1474) in 77 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,902 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 237.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,902 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 237 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.078 s\n",
      "2015-07-02 05:21:37,902 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 60 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.161376 s\n",
      "2015-07-02 05:21:37,941 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:37,942 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 666 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,942 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 61 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:37,942 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 239(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,942 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 238)\n",
      "2015-07-02 05:21:37,943 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 238)\n",
      "2015-07-02 05:21:37,944 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 238 (PairwiseRDD[666] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:37,944 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4140973, maxMem=278302556\n",
      "2015-07-02 05:21:37,945 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_174 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:37,946 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6162) called with curMem=4150413, maxMem=278302556\n",
      "2015-07-02 05:21:37,946 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_174_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:37,946 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_174_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:37,946 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_174_piece0\n",
      "2015-07-02 05:21:37,947 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 174 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:37,947 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 238 (PairwiseRDD[666] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:37,947 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 238.0 with 2 tasks\n",
      "2015-07-02 05:21:37,948 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 238.0 (TID 1476, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,948 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 238.0 (TID 1477, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:37,948 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 238.0 (TID 1477)\n",
      "2015-07-02 05:21:37,948 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 238.0 (TID 1476)\n",
      "2015-07-02 05:21:37,950 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:37,951 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:37,993 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -48, init = 89, finish = 2\n",
      "2015-07-02 05:21:37,994 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -43, init = 85, finish = 2\n",
      "2015-07-02 05:21:37,994 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 238.0 (TID 1476). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,995 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 238.0 (TID 1477). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:37,996 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 238.0 (TID 1476) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:37,996 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 238.0 (TID 1477) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:37,996 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 238.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:37,997 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 238 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:37,998 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:37,998 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:37,998 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 239)\n",
      "2015-07-02 05:21:37,998 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:37,999 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 239: List()\n",
      "2015-07-02 05:21:37,999 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 239 (PythonRDD[669] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,000 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4156575, maxMem=278302556\n",
      "2015-07-02 05:21:38,000 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_175 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,001 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4161775, maxMem=278302556\n",
      "2015-07-02 05:21:38,002 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_175_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,002 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_175_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:38,002 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_175_piece0\n",
      "2015-07-02 05:21:38,003 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 175 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,003 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 239 (PythonRDD[669] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,003 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 239.0 with 2 tasks\n",
      "2015-07-02 05:21:38,004 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 239.0 (TID 1478, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,004 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 239.0 (TID 1479, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,004 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 239.0 (TID 1478)\n",
      "2015-07-02 05:21:38,004 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 239.0 (TID 1479)\n",
      "2015-07-02 05:21:38,006 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,006 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,006 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,007 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:38,046 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:38,047 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 239.0 (TID 1478). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,047 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -5, init = 46, finish = 0\n",
      "2015-07-02 05:21:38,047 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 239.0 (TID 1479). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,048 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 239.0 (TID 1478) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,048 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 239.0 (TID 1479) in 44 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,048 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 239.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,049 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 239 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:38,050 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 61 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.108340 s\n",
      "2015-07-02 05:21:38,088 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:38,088 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 672 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,089 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 62 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:38,089 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 241(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,089 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 240)\n",
      "2015-07-02 05:21:38,090 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 240)\n",
      "2015-07-02 05:21:38,091 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 240 (PairwiseRDD[672] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:38,091 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4165055, maxMem=278302556\n",
      "2015-07-02 05:21:38,091 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_176 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,092 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4174495, maxMem=278302556\n",
      "2015-07-02 05:21:38,093 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_176_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,093 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_176_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:38,093 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_176_piece0\n",
      "2015-07-02 05:21:38,094 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 176 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,094 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 240 (PairwiseRDD[672] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,094 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 240.0 with 2 tasks\n",
      "2015-07-02 05:21:38,095 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 240.0 (TID 1480, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,095 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 240.0 (TID 1481, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,096 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 240.0 (TID 1481)\n",
      "2015-07-02 05:21:38,096 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 240.0 (TID 1480)\n",
      "2015-07-02 05:21:38,099 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:38,099 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:38,142 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -42, init = 85, finish = 1\n",
      "2015-07-02 05:21:38,145 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -43, init = 86, finish = 1\n",
      "2015-07-02 05:21:38,146 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 240.0 (TID 1481). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,147 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 240.0 (TID 1480). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,148 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 240.0 (TID 1481) in 52 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,149 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 240.0 (TID 1480) in 54 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,149 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 240.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,150 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 240 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.055 s\n",
      "2015-07-02 05:21:38,150 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:38,150 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:38,150 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 241)\n",
      "2015-07-02 05:21:38,150 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:38,151 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 241: List()\n",
      "2015-07-02 05:21:38,152 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 241 (PythonRDD[675] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,152 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4180666, maxMem=278302556\n",
      "2015-07-02 05:21:38,152 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_177 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,153 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4185866, maxMem=278302556\n",
      "2015-07-02 05:21:38,154 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_177_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,154 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_177_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,154 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_177_piece0\n",
      "2015-07-02 05:21:38,155 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 177 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,155 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 241 (PythonRDD[675] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,155 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 241.0 with 2 tasks\n",
      "2015-07-02 05:21:38,156 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 241.0 (TID 1482, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,156 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 241.0 (TID 1483, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,156 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 241.0 (TID 1482)\n",
      "2015-07-02 05:21:38,156 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 241.0 (TID 1483)\n",
      "2015-07-02 05:21:38,158 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,158 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,159 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,160 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:38,199 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -7, init = 49, finish = 0\n",
      "2015-07-02 05:21:38,200 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 241.0 (TID 1482). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,201 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 241.0 (TID 1482) in 46 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,201 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 48, finish = 0\n",
      "2015-07-02 05:21:38,204 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 241.0 (TID 1483). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,206 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 241.0 (TID 1483) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,206 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 241.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,208 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 241 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.053 s\n",
      "2015-07-02 05:21:38,209 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 62 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.121135 s\n",
      "2015-07-02 05:21:38,252 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:38,252 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 678 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,252 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 63 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:38,252 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 243(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,252 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 242)\n",
      "2015-07-02 05:21:38,255 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 242)\n",
      "2015-07-02 05:21:38,256 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 242 (PairwiseRDD[678] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:38,256 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4189147, maxMem=278302556\n",
      "2015-07-02 05:21:38,257 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_178 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,257 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6172) called with curMem=4198587, maxMem=278302556\n",
      "2015-07-02 05:21:38,258 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_178_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,258 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_178_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,258 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_178_piece0\n",
      "2015-07-02 05:21:38,259 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 178 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,259 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 242 (PairwiseRDD[678] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,259 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 242.0 with 2 tasks\n",
      "2015-07-02 05:21:38,260 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 242.0 (TID 1484, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,260 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 242.0 (TID 1485, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,261 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 242.0 (TID 1485)\n",
      "2015-07-02 05:21:38,261 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 242.0 (TID 1484)\n",
      "2015-07-02 05:21:38,262 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:38,263 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:38,310 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -55, init = 97, finish = 5\n",
      "2015-07-02 05:21:38,310 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -51, init = 93, finish = 6\n",
      "2015-07-02 05:21:38,313 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 242.0 (TID 1484). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,314 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 242.0 (TID 1485). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,333 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 242.0 (TID 1484) in 73 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,334 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 242.0 (TID 1485) in 73 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,334 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 242.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,335 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 242 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.075 s\n",
      "2015-07-02 05:21:38,335 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:38,335 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:38,335 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 243)\n",
      "2015-07-02 05:21:38,335 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:38,336 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 243: List()\n",
      "2015-07-02 05:21:38,337 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 243 (PythonRDD[681] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,337 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4204759, maxMem=278302556\n",
      "2015-07-02 05:21:38,338 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_179 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,339 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4209959, maxMem=278302556\n",
      "2015-07-02 05:21:38,339 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_179_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,339 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_179_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_179_piece0\n",
      "2015-07-02 05:21:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 179 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 243 (PythonRDD[681] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,340 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 243.0 with 2 tasks\n",
      "2015-07-02 05:21:38,341 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 243.0 (TID 1486, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,341 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 243.0 (TID 1487, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,342 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 243.0 (TID 1487)\n",
      "2015-07-02 05:21:38,342 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 243.0 (TID 1486)\n",
      "2015-07-02 05:21:38,344 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,344 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,345 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,345 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,385 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:38,385 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 48, finish = 0\n",
      "2015-07-02 05:21:38,386 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 243.0 (TID 1487). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,386 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 243.0 (TID 1486). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,386 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 243.0 (TID 1487) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,387 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 243.0 (TID 1486) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,387 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 243.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,388 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 243 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:38,388 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 63 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.136570 s\n",
      "2015-07-02 05:21:38,421 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:38,421 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 684 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,422 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 64 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:38,422 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 245(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,422 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 244)\n",
      "2015-07-02 05:21:38,423 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 244)\n",
      "2015-07-02 05:21:38,424 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 244 (PairwiseRDD[684] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:38,425 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4213240, maxMem=278302556\n",
      "2015-07-02 05:21:38,425 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_180 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,426 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6166) called with curMem=4222680, maxMem=278302556\n",
      "2015-07-02 05:21:38,426 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_180_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,426 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_180_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,427 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_180_piece0\n",
      "2015-07-02 05:21:38,427 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 180 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,427 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 244 (PairwiseRDD[684] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,428 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 244.0 with 2 tasks\n",
      "2015-07-02 05:21:38,428 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 244.0 (TID 1488, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,428 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 244.0 (TID 1489, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,429 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 244.0 (TID 1488)\n",
      "2015-07-02 05:21:38,429 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 244.0 (TID 1489)\n",
      "2015-07-02 05:21:38,432 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:38,432 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:38,475 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -38, init = 80, finish = 3\n",
      "2015-07-02 05:21:38,477 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 244.0 (TID 1488). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,479 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 244.0 (TID 1488) in 51 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,480 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -36, init = 81, finish = 5\n",
      "2015-07-02 05:21:38,481 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 244.0 (TID 1489). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,483 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 244.0 (TID 1489) in 55 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,483 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 244.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 244 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.056 s\n",
      "2015-07-02 05:21:38,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:38,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:38,484 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 245)\n",
      "2015-07-02 05:21:38,485 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:38,486 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 245: List()\n",
      "2015-07-02 05:21:38,486 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 245 (PythonRDD[687] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,487 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4228846, maxMem=278302556\n",
      "2015-07-02 05:21:38,487 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_181 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,488 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4234046, maxMem=278302556\n",
      "2015-07-02 05:21:38,489 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_181_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,489 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_181_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,489 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_181_piece0\n",
      "2015-07-02 05:21:38,490 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 181 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,490 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 245 (PythonRDD[687] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,490 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 245.0 with 2 tasks\n",
      "2015-07-02 05:21:38,491 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 245.0 (TID 1490, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,492 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 245.0 (TID 1491, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,492 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 245.0 (TID 1491)\n",
      "2015-07-02 05:21:38,492 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 245.0 (TID 1490)\n",
      "2015-07-02 05:21:38,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,494 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,534 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:38,535 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -9, init = 50, finish = 0\n",
      "2015-07-02 05:21:38,535 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 245.0 (TID 1490). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,535 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 245.0 (TID 1491). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,537 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 245.0 (TID 1490) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,537 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 245.0 (TID 1491) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,537 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 245.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,538 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 245 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.048 s\n",
      "2015-07-02 05:21:38,539 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 64 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.117317 s\n",
      "2015-07-02 05:21:38,573 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:38,574 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 690 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,574 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 65 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:38,574 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 247(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,574 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 246)\n",
      "2015-07-02 05:21:38,575 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 246)\n",
      "2015-07-02 05:21:38,576 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 246 (PairwiseRDD[690] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:38,577 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4237326, maxMem=278302556\n",
      "2015-07-02 05:21:38,577 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_182 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,578 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6162) called with curMem=4246766, maxMem=278302556\n",
      "2015-07-02 05:21:38,578 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_182_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:38,579 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_182_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,579 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_182_piece0\n",
      "2015-07-02 05:21:38,579 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 182 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,580 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 246 (PairwiseRDD[690] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,580 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 246.0 with 2 tasks\n",
      "2015-07-02 05:21:38,580 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 246.0 (TID 1492, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,580 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 246.0 (TID 1493, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,581 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 246.0 (TID 1492)\n",
      "2015-07-02 05:21:38,581 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 246.0 (TID 1493)\n",
      "2015-07-02 05:21:38,583 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:38,583 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:38,626 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -39, init = 80, finish = 3\n",
      "2015-07-02 05:21:38,626 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -39, init = 80, finish = 3\n",
      "2015-07-02 05:21:38,628 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 246.0 (TID 1493). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,628 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 246.0 (TID 1492). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:38,629 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 246.0 (TID 1492) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,630 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 246.0 (TID 1493) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,630 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 246.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,631 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 246 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:38,631 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:38,631 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:38,631 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 247)\n",
      "2015-07-02 05:21:38,632 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:38,634 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 247: List()\n",
      "2015-07-02 05:21:38,634 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 247 (PythonRDD[693] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,635 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4252928, maxMem=278302556\n",
      "2015-07-02 05:21:38,635 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_183 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,636 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4258128, maxMem=278302556\n",
      "2015-07-02 05:21:38,637 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_183_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,637 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_183_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,637 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_183_piece0\n",
      "2015-07-02 05:21:38,638 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 183 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,638 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 247 (PythonRDD[693] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,638 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 247.0 with 2 tasks\n",
      "2015-07-02 05:21:38,638 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 247.0 (TID 1494, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,639 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 247.0 (TID 1495, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,639 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 247.0 (TID 1494)\n",
      "2015-07-02 05:21:38,639 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 247.0 (TID 1495)\n",
      "2015-07-02 05:21:38,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,641 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,681 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:38,682 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 247.0 (TID 1494). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,682 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -7, init = 48, finish = 0\n",
      "2015-07-02 05:21:38,682 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 247.0 (TID 1495). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,683 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 247.0 (TID 1494) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,683 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 247.0 (TID 1495) in 44 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,683 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 247.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,684 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 247 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:38,685 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 65 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.111118 s\n",
      "2015-07-02 05:21:38,864 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:38,865 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 696 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,865 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 66 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:38,865 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 249(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,865 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 248)\n",
      "2015-07-02 05:21:38,866 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 248)\n",
      "2015-07-02 05:21:38,867 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 248 (PairwiseRDD[696] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:38,868 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4261408, maxMem=278302556\n",
      "2015-07-02 05:21:38,868 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_184 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,869 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4270848, maxMem=278302556\n",
      "2015-07-02 05:21:38,869 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_184_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,869 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_184_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,869 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_184_piece0\n",
      "2015-07-02 05:21:38,870 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 184 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,870 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 248 (PairwiseRDD[696] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,870 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 248.0 with 2 tasks\n",
      "2015-07-02 05:21:38,871 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 248.0 (TID 1496, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,871 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 248.0 (TID 1497, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:38,871 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 248.0 (TID 1496)\n",
      "2015-07-02 05:21:38,871 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 248.0 (TID 1497)\n",
      "2015-07-02 05:21:38,873 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:38,874 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:38,920 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -182, init = 223, finish = 6\n",
      "2015-07-02 05:21:38,921 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -183, init = 226, finish = 5\n",
      "2015-07-02 05:21:38,923 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 248.0 (TID 1496). 2121 bytes result sent to driver\n",
      "2015-07-02 05:21:38,925 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 248.0 (TID 1497). 2121 bytes result sent to driver\n",
      "2015-07-02 05:21:38,926 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 248.0 (TID 1496) in 55 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,929 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 248.0 (TID 1497) in 57 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,929 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 248.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,930 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 248 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.060 s\n",
      "2015-07-02 05:21:38,930 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:38,930 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:38,931 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 249)\n",
      "2015-07-02 05:21:38,931 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:38,933 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 249: List()\n",
      "2015-07-02 05:21:38,933 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 249 (PythonRDD[699] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:38,934 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4277018, maxMem=278302556\n",
      "2015-07-02 05:21:38,934 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_185 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,935 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4282218, maxMem=278302556\n",
      "2015-07-02 05:21:38,936 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_185_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:38,936 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_185_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:38,936 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_185_piece0\n",
      "2015-07-02 05:21:38,937 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 185 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:38,937 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 249 (PythonRDD[699] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:38,937 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 249.0 with 2 tasks\n",
      "2015-07-02 05:21:38,938 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 249.0 (TID 1498, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,938 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 249.0 (TID 1499, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:38,938 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 249.0 (TID 1499)\n",
      "2015-07-02 05:21:38,938 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 249.0 (TID 1498)\n",
      "2015-07-02 05:21:38,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:38,940 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:38,980 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:38,980 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:38,981 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 249.0 (TID 1499). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:38,981 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 249.0 (TID 1498). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:38,982 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 249.0 (TID 1499) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:38,982 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 249.0 (TID 1498) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:38,982 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 249.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:38,983 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 249 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:38,984 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 66 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.119032 s\n",
      "2015-07-02 05:21:39,015 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,015 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 702 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,016 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 67 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,016 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 251(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,016 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 250)\n",
      "2015-07-02 05:21:39,017 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 250)\n",
      "2015-07-02 05:21:39,018 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 250 (PairwiseRDD[702] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,018 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4285499, maxMem=278302556\n",
      "2015-07-02 05:21:39,019 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_186 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,019 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6167) called with curMem=4294939, maxMem=278302556\n",
      "2015-07-02 05:21:39,020 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_186_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,020 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_186_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,020 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_186_piece0\n",
      "2015-07-02 05:21:39,021 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 186 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,021 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 250 (PairwiseRDD[702] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,021 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 250.0 with 2 tasks\n",
      "2015-07-02 05:21:39,022 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 250.0 (TID 1500, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,022 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 250.0 (TID 1501, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,023 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 250.0 (TID 1501)\n",
      "2015-07-02 05:21:39,023 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 250.0 (TID 1500)\n",
      "2015-07-02 05:21:39,024 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,025 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,066 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -35, init = 75, finish = 2\n",
      "2015-07-02 05:21:39,068 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 250.0 (TID 1500). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,068 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -36, init = 77, finish = 3\n",
      "2015-07-02 05:21:39,069 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 250.0 (TID 1500) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,069 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 250.0 (TID 1501). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,070 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 250.0 (TID 1501) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,071 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 250.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,071 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 250 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:39,071 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,071 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,071 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 251)\n",
      "2015-07-02 05:21:39,071 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,072 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 251: List()\n",
      "2015-07-02 05:21:39,073 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 251 (PythonRDD[705] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,074 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4301106, maxMem=278302556\n",
      "2015-07-02 05:21:39,074 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_187 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,075 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3282) called with curMem=4306306, maxMem=278302556\n",
      "2015-07-02 05:21:39,075 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_187_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,076 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_187_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,076 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_187_piece0\n",
      "2015-07-02 05:21:39,077 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 187 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,077 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 251 (PythonRDD[705] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,077 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 251.0 with 2 tasks\n",
      "2015-07-02 05:21:39,078 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 251.0 (TID 1502, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,078 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 251.0 (TID 1503, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,078 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 251.0 (TID 1502)\n",
      "2015-07-02 05:21:39,078 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 251.0 (TID 1503)\n",
      "2015-07-02 05:21:39,080 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,080 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,081 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,081 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,122 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:39,122 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -2, init = 44, finish = 0\n",
      "2015-07-02 05:21:39,123 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 251.0 (TID 1503). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:39,123 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 251.0 (TID 1502). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:39,128 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 251.0 (TID 1502) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,129 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 251.0 (TID 1503) in 51 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,130 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 251.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,131 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 251 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.053 s\n",
      "2015-07-02 05:21:39,132 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 67 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.116632 s\n",
      "2015-07-02 05:21:39,175 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,175 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 708 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,175 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 68 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,175 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 253(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,175 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 252)\n",
      "2015-07-02 05:21:39,177 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 252)\n",
      "2015-07-02 05:21:39,178 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 252 (PairwiseRDD[708] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,178 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4309588, maxMem=278302556\n",
      "2015-07-02 05:21:39,179 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_188 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,185 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 187\n",
      "2015-07-02 05:21:39,185 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_187\n",
      "2015-07-02 05:21:39,185 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_187 of size 5200 dropped from memory (free 273988728)\n",
      "2015-07-02 05:21:39,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_187_piece0\n",
      "2015-07-02 05:21:39,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_187_piece0 of size 3282 dropped from memory (free 273992010)\n",
      "2015-07-02 05:21:39,185 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4319028, maxMem=278302556\n",
      "2015-07-02 05:21:39,186 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_187_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,186 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_188_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:39,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_187_piece0\n",
      "2015-07-02 05:21:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_188_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,187 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 187\n",
      "2015-07-02 05:21:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_188_piece0\n",
      "2015-07-02 05:21:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 186\n",
      "2015-07-02 05:21:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 188 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,188 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 252 (PairwiseRDD[708] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,188 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 252.0 with 2 tasks\n",
      "2015-07-02 05:21:39,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_186_piece0\n",
      "2015-07-02 05:21:39,188 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_186_piece0 of size 6167 dropped from memory (free 273992006)\n",
      "2015-07-02 05:21:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 252.0 (TID 1504, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_186_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 252.0 (TID 1505, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,189 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_186_piece0\n",
      "2015-07-02 05:21:39,190 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_186\n",
      "2015-07-02 05:21:39,190 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_186 of size 9440 dropped from memory (free 274001446)\n",
      "2015-07-02 05:21:39,190 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 252.0 (TID 1505)\n",
      "2015-07-02 05:21:39,190 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 186\n",
      "2015-07-02 05:21:39,190 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 252.0 (TID 1504)\n",
      "2015-07-02 05:21:39,191 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 185\n",
      "2015-07-02 05:21:39,191 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_185_piece0\n",
      "2015-07-02 05:21:39,191 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_185_piece0 of size 3281 dropped from memory (free 274004727)\n",
      "2015-07-02 05:21:39,191 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_185_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,192 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_185_piece0\n",
      "2015-07-02 05:21:39,192 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_185\n",
      "2015-07-02 05:21:39,192 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_185 of size 5200 dropped from memory (free 274009927)\n",
      "2015-07-02 05:21:39,192 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,193 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 185\n",
      "2015-07-02 05:21:39,193 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 184\n",
      "2015-07-02 05:21:39,193 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_184_piece0\n",
      "2015-07-02 05:21:39,193 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_184_piece0 of size 6170 dropped from memory (free 274016097)\n",
      "2015-07-02 05:21:39,193 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_184_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,194 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_184_piece0\n",
      "2015-07-02 05:21:39,194 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_184\n",
      "2015-07-02 05:21:39,194 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_184 of size 9440 dropped from memory (free 274025537)\n",
      "2015-07-02 05:21:39,196 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 184\n",
      "2015-07-02 05:21:39,200 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 183\n",
      "2015-07-02 05:21:39,200 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_183\n",
      "2015-07-02 05:21:39,200 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_183 of size 5200 dropped from memory (free 274030737)\n",
      "2015-07-02 05:21:39,200 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_183_piece0\n",
      "2015-07-02 05:21:39,200 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_183_piece0 of size 3280 dropped from memory (free 274034017)\n",
      "2015-07-02 05:21:39,201 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_183_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,201 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,201 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_183_piece0\n",
      "2015-07-02 05:21:39,202 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 183\n",
      "2015-07-02 05:21:39,202 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 182\n",
      "2015-07-02 05:21:39,202 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_182_piece0\n",
      "2015-07-02 05:21:39,202 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_182_piece0 of size 6162 dropped from memory (free 274040179)\n",
      "2015-07-02 05:21:39,203 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_182_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,203 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_182_piece0\n",
      "2015-07-02 05:21:39,203 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_182\n",
      "2015-07-02 05:21:39,203 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_182 of size 9440 dropped from memory (free 274049619)\n",
      "2015-07-02 05:21:39,204 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 182\n",
      "2015-07-02 05:21:39,204 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 170\n",
      "2015-07-02 05:21:39,204 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_170_piece0\n",
      "2015-07-02 05:21:39,204 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_170_piece0 of size 6170 dropped from memory (free 274055789)\n",
      "2015-07-02 05:21:39,205 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_170_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,205 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_170_piece0\n",
      "2015-07-02 05:21:39,205 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_170\n",
      "2015-07-02 05:21:39,205 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_170 of size 9440 dropped from memory (free 274065229)\n",
      "2015-07-02 05:21:39,205 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 170\n",
      "2015-07-02 05:21:39,206 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 169\n",
      "2015-07-02 05:21:39,206 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_169_piece0\n",
      "2015-07-02 05:21:39,206 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_169_piece0 of size 3281 dropped from memory (free 274068510)\n",
      "2015-07-02 05:21:39,206 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_169_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,206 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_169_piece0\n",
      "2015-07-02 05:21:39,207 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_169\n",
      "2015-07-02 05:21:39,207 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_169 of size 5200 dropped from memory (free 274073710)\n",
      "2015-07-02 05:21:39,207 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 169\n",
      "2015-07-02 05:21:39,207 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 177\n",
      "2015-07-02 05:21:39,207 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_177_piece0\n",
      "2015-07-02 05:21:39,207 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_177_piece0 of size 3281 dropped from memory (free 274076991)\n",
      "2015-07-02 05:21:39,208 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_177_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,208 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_177_piece0\n",
      "2015-07-02 05:21:39,208 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_177\n",
      "2015-07-02 05:21:39,208 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_177 of size 5200 dropped from memory (free 274082191)\n",
      "2015-07-02 05:21:39,208 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 177\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 176\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_176_piece0\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_176_piece0 of size 6171 dropped from memory (free 274088362)\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_176_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_176_piece0\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_176\n",
      "2015-07-02 05:21:39,209 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_176 of size 9440 dropped from memory (free 274097802)\n",
      "2015-07-02 05:21:39,210 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 176\n",
      "2015-07-02 05:21:39,210 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 98\n",
      "2015-07-02 05:21:39,210 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 175\n",
      "2015-07-02 05:21:39,210 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_175_piece0\n",
      "2015-07-02 05:21:39,210 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_175_piece0 of size 3280 dropped from memory (free 274101082)\n",
      "2015-07-02 05:21:39,211 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_175_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,211 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_175_piece0\n",
      "2015-07-02 05:21:39,211 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_175\n",
      "2015-07-02 05:21:39,211 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_175 of size 5200 dropped from memory (free 274106282)\n",
      "2015-07-02 05:21:39,211 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 175\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 174\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_174\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_174 of size 9440 dropped from memory (free 274115722)\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_174_piece0\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_174_piece0 of size 6162 dropped from memory (free 274121884)\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_174_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,212 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_174_piece0\n",
      "2015-07-02 05:21:39,213 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 174\n",
      "2015-07-02 05:21:39,213 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 97\n",
      "2015-07-02 05:21:39,213 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 173\n",
      "2015-07-02 05:21:39,213 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_173\n",
      "2015-07-02 05:21:39,213 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_173 of size 5200 dropped from memory (free 274127084)\n",
      "2015-07-02 05:21:39,214 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_173_piece0\n",
      "2015-07-02 05:21:39,214 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_173_piece0 of size 3282 dropped from memory (free 274130366)\n",
      "2015-07-02 05:21:39,214 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_173_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,214 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_173_piece0\n",
      "2015-07-02 05:21:39,214 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 173\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 172\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_172_piece0\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_172_piece0 of size 6171 dropped from memory (free 274136537)\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_172_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_172_piece0\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_172\n",
      "2015-07-02 05:21:39,215 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_172 of size 9440 dropped from memory (free 274145977)\n",
      "2015-07-02 05:21:39,216 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 172\n",
      "2015-07-02 05:21:39,216 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 96\n",
      "2015-07-02 05:21:39,216 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 171\n",
      "2015-07-02 05:21:39,216 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_171\n",
      "2015-07-02 05:21:39,216 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_171 of size 5200 dropped from memory (free 274151177)\n",
      "2015-07-02 05:21:39,217 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_171_piece0\n",
      "2015-07-02 05:21:39,217 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_171_piece0 of size 3281 dropped from memory (free 274154458)\n",
      "2015-07-02 05:21:39,217 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_171_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,217 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_171_piece0\n",
      "2015-07-02 05:21:39,217 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 171\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 181\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_181\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_181 of size 5200 dropped from memory (free 274159658)\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_181_piece0\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_181_piece0 of size 3280 dropped from memory (free 274162938)\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_181_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,218 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_181_piece0\n",
      "2015-07-02 05:21:39,219 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 181\n",
      "2015-07-02 05:21:39,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 180\n",
      "2015-07-02 05:21:39,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_180\n",
      "2015-07-02 05:21:39,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_180 of size 9440 dropped from memory (free 274172378)\n",
      "2015-07-02 05:21:39,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_180_piece0\n",
      "2015-07-02 05:21:39,219 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_180_piece0 of size 6166 dropped from memory (free 274178544)\n",
      "2015-07-02 05:21:39,220 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_180_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,220 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_180_piece0\n",
      "2015-07-02 05:21:39,220 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 180\n",
      "2015-07-02 05:21:39,220 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 179\n",
      "2015-07-02 05:21:39,220 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_179\n",
      "2015-07-02 05:21:39,221 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_179 of size 5200 dropped from memory (free 274183744)\n",
      "2015-07-02 05:21:39,221 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_179_piece0\n",
      "2015-07-02 05:21:39,221 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_179_piece0 of size 3281 dropped from memory (free 274187025)\n",
      "2015-07-02 05:21:39,221 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_179_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,221 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_179_piece0\n",
      "2015-07-02 05:21:39,221 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 179\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 178\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_178_piece0\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_178_piece0 of size 6172 dropped from memory (free 274193197)\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_178_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_178_piece0\n",
      "2015-07-02 05:21:39,222 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_178\n",
      "2015-07-02 05:21:39,223 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_178 of size 9440 dropped from memory (free 274202637)\n",
      "2015-07-02 05:21:39,223 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 178\n",
      "2015-07-02 05:21:39,223 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 99\n",
      "2015-07-02 05:21:39,238 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -49, init = 94, finish = 1\n",
      "2015-07-02 05:21:39,239 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 252.0 (TID 1505). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,239 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 252.0 (TID 1505) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,245 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 52, boot = -48, init = 98, finish = 2\n",
      "2015-07-02 05:21:39,247 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 252.0 (TID 1504). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,248 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 252.0 (TID 1504) in 60 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,248 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 252.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,248 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 252 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.060 s\n",
      "2015-07-02 05:21:39,248 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,249 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,249 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 253)\n",
      "2015-07-02 05:21:39,249 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,250 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 253: List()\n",
      "2015-07-02 05:21:39,250 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 253 (PythonRDD[711] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,250 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4099919, maxMem=278302556\n",
      "2015-07-02 05:21:39,251 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_189 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,252 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4105119, maxMem=278302556\n",
      "2015-07-02 05:21:39,252 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_189_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,252 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_189_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,252 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_189_piece0\n",
      "2015-07-02 05:21:39,253 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 189 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,253 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 253 (PythonRDD[711] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,253 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 253.0 with 2 tasks\n",
      "2015-07-02 05:21:39,254 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 253.0 (TID 1506, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,254 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 253.0 (TID 1507, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,254 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 253.0 (TID 1507)\n",
      "2015-07-02 05:21:39,254 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 253.0 (TID 1506)\n",
      "2015-07-02 05:21:39,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,256 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,300 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -12, init = 56, finish = 0\n",
      "2015-07-02 05:21:39,301 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 253.0 (TID 1507). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:39,301 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -3, init = 47, finish = 0\n",
      "2015-07-02 05:21:39,302 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 253.0 (TID 1506). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:39,302 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 253.0 (TID 1507) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,314 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 253.0 (TID 1506) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,314 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 253 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.061 s\n",
      "2015-07-02 05:21:39,314 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 253.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,315 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 68 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.140111 s\n",
      "2015-07-02 05:21:39,351 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,351 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 714 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,352 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 69 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,352 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 255(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,352 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 254)\n",
      "2015-07-02 05:21:39,353 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 254)\n",
      "2015-07-02 05:21:39,354 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 254 (PairwiseRDD[714] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,354 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4108400, maxMem=278302556\n",
      "2015-07-02 05:21:39,355 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_190 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,355 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4117840, maxMem=278302556\n",
      "2015-07-02 05:21:39,356 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_190_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,356 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_190_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,356 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_190_piece0\n",
      "2015-07-02 05:21:39,356 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 190 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,357 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 254 (PairwiseRDD[714] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,357 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 254.0 with 2 tasks\n",
      "2015-07-02 05:21:39,357 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 254.0 (TID 1508, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,358 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 254.0 (TID 1509, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,358 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 254.0 (TID 1509)\n",
      "2015-07-02 05:21:39,358 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 254.0 (TID 1508)\n",
      "2015-07-02 05:21:39,360 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,360 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,403 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -47, init = 88, finish = 2\n",
      "2015-07-02 05:21:39,403 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -47, init = 88, finish = 2\n",
      "2015-07-02 05:21:39,405 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 254.0 (TID 1508). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,405 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 254.0 (TID 1509). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,407 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 254.0 (TID 1508) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,407 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 254.0 (TID 1509) in 49 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,407 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 254.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,408 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 254 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:39,408 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,408 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,409 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 255)\n",
      "2015-07-02 05:21:39,409 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,410 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 255: List()\n",
      "2015-07-02 05:21:39,410 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 255 (PythonRDD[717] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,411 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4124010, maxMem=278302556\n",
      "2015-07-02 05:21:39,411 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_191 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,412 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3284) called with curMem=4129210, maxMem=278302556\n",
      "2015-07-02 05:21:39,412 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_191_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,412 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_191_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,413 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_191_piece0\n",
      "2015-07-02 05:21:39,413 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 191 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,413 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 255 (PythonRDD[717] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,413 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 255.0 with 2 tasks\n",
      "2015-07-02 05:21:39,414 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 255.0 (TID 1510, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,414 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 255.0 (TID 1511, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,414 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 255.0 (TID 1511)\n",
      "2015-07-02 05:21:39,414 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 255.0 (TID 1510)\n",
      "2015-07-02 05:21:39,416 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,416 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,416 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,417 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:39,456 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -5, init = 46, finish = 0\n",
      "2015-07-02 05:21:39,456 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 255.0 (TID 1510). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:39,458 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 255.0 (TID 1510) in 43 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,458 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:39,459 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 255.0 (TID 1511). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:39,462 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 255.0 (TID 1511) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,463 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 255.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,464 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 255 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:39,465 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 69 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.113546 s\n",
      "2015-07-02 05:21:39,503 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,503 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 720 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,503 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 70 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,503 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 257(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,503 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 256)\n",
      "2015-07-02 05:21:39,504 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 256)\n",
      "2015-07-02 05:21:39,505 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 256 (PairwiseRDD[720] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,506 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4132494, maxMem=278302556\n",
      "2015-07-02 05:21:39,506 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_192 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,507 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4141934, maxMem=278302556\n",
      "2015-07-02 05:21:39,507 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_192_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:39,507 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_192_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,507 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_192_piece0\n",
      "2015-07-02 05:21:39,508 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 192 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,508 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 256 (PairwiseRDD[720] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,508 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 256.0 with 2 tasks\n",
      "2015-07-02 05:21:39,509 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 256.0 (TID 1512, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,509 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 256.0 (TID 1513, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,509 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 256.0 (TID 1513)\n",
      "2015-07-02 05:21:39,509 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 256.0 (TID 1512)\n",
      "2015-07-02 05:21:39,511 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,512 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,687 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 177, boot = -44, init = 217, finish = 4\n",
      "2015-07-02 05:21:39,687 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 177, boot = -47, init = 221, finish = 3\n",
      "2015-07-02 05:21:39,692 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 256.0 (TID 1512). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,692 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 256.0 (TID 1513). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,692 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 256.0 (TID 1512) in 184 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,693 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 256.0 (TID 1513) in 184 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,693 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 256.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,693 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 256 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.185 s\n",
      "2015-07-02 05:21:39,694 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,694 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,694 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 257)\n",
      "2015-07-02 05:21:39,694 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,695 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 257: List()\n",
      "2015-07-02 05:21:39,695 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 257 (PythonRDD[723] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,696 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4148104, maxMem=278302556\n",
      "2015-07-02 05:21:39,697 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_193 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,698 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4153304, maxMem=278302556\n",
      "2015-07-02 05:21:39,698 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_193_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,699 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_193_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,699 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_193_piece0\n",
      "2015-07-02 05:21:39,699 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 193 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,700 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 257 (PythonRDD[723] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,700 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 257.0 with 2 tasks\n",
      "2015-07-02 05:21:39,700 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 257.0 (TID 1514, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,701 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 257.0 (TID 1515, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,701 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 257.0 (TID 1514)\n",
      "2015-07-02 05:21:39,701 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 257.0 (TID 1515)\n",
      "2015-07-02 05:21:39,703 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,703 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,703 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,703 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,704 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 2, boot = -4, init = 6, finish = 0\n",
      "2015-07-02 05:21:39,705 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 257.0 (TID 1514). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:39,705 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 2, boot = -4, init = 6, finish = 0\n",
      "2015-07-02 05:21:39,705 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 257.0 (TID 1515). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:39,706 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 257.0 (TID 1514) in 5 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,706 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 257.0 (TID 1515) in 6 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,706 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 257.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,707 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 257 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.006 s\n",
      "2015-07-02 05:21:39,707 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 70 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.204115 s\n",
      "2015-07-02 05:21:39,748 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,749 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 726 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,749 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 71 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,749 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 259(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,749 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 258)\n",
      "2015-07-02 05:21:39,750 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 258)\n",
      "2015-07-02 05:21:39,751 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 258 (PairwiseRDD[726] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,751 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4156585, maxMem=278302556\n",
      "2015-07-02 05:21:39,752 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_194 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,752 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6168) called with curMem=4166025, maxMem=278302556\n",
      "2015-07-02 05:21:39,753 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_194_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,753 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_194_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,753 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_194_piece0\n",
      "2015-07-02 05:21:39,753 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 194 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,754 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 258 (PairwiseRDD[726] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,754 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 258.0 with 2 tasks\n",
      "2015-07-02 05:21:39,754 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 258.0 (TID 1516, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,755 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 258.0 (TID 1517, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,755 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 258.0 (TID 1517)\n",
      "2015-07-02 05:21:39,755 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 258.0 (TID 1516)\n",
      "2015-07-02 05:21:39,757 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,757 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,800 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -44, init = 86, finish = 1\n",
      "2015-07-02 05:21:39,800 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -45, init = 87, finish = 2\n",
      "2015-07-02 05:21:39,803 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 258.0 (TID 1516). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,803 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 258.0 (TID 1517). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,805 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 258.0 (TID 1516) in 51 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,806 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 258.0 (TID 1517) in 52 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,807 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 258.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,808 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 258 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.054 s\n",
      "2015-07-02 05:21:39,809 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,809 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,809 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 259)\n",
      "2015-07-02 05:21:39,809 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,812 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 259: List()\n",
      "2015-07-02 05:21:39,813 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 259 (PythonRDD[729] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,814 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4172193, maxMem=278302556\n",
      "2015-07-02 05:21:39,814 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_195 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,816 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4177393, maxMem=278302556\n",
      "2015-07-02 05:21:39,816 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_195_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,817 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_195_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:39,817 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_195_piece0\n",
      "2015-07-02 05:21:39,818 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 195 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,819 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 259 (PythonRDD[729] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,819 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 259.0 with 2 tasks\n",
      "2015-07-02 05:21:39,820 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 259.0 (TID 1518, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,820 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 259.0 (TID 1519, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,821 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 259.0 (TID 1518)\n",
      "2015-07-02 05:21:39,821 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 259.0 (TID 1519)\n",
      "2015-07-02 05:21:39,824 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,825 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,825 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:39,825 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,865 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 05:21:39,865 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 05:21:39,865 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 259.0 (TID 1518). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:39,866 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 259.0 (TID 1519). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:39,866 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 259.0 (TID 1518) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,867 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 259.0 (TID 1519) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,867 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 259.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,868 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 259 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:39,868 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 71 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.119678 s\n",
      "2015-07-02 05:21:39,907 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:39,908 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 732 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,908 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 72 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:39,908 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 261(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,908 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 260)\n",
      "2015-07-02 05:21:39,909 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 260)\n",
      "2015-07-02 05:21:39,910 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 260 (PairwiseRDD[732] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:39,911 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4180674, maxMem=278302556\n",
      "2015-07-02 05:21:39,911 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_196 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,911 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4190114, maxMem=278302556\n",
      "2015-07-02 05:21:39,912 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_196_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,912 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_196_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,912 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_196_piece0\n",
      "2015-07-02 05:21:39,913 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 196 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,913 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 260 (PairwiseRDD[732] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,913 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 260.0 with 2 tasks\n",
      "2015-07-02 05:21:39,914 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 260.0 (TID 1520, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,914 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 260.0 (TID 1521, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:39,915 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 260.0 (TID 1521)\n",
      "2015-07-02 05:21:39,915 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 260.0 (TID 1520)\n",
      "2015-07-02 05:21:39,916 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:39,917 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:39,970 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -43, init = 92, finish = 5\n",
      "2015-07-02 05:21:39,971 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 54, boot = -42, init = 91, finish = 5\n",
      "2015-07-02 05:21:39,974 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 260.0 (TID 1521). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,974 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 260.0 (TID 1520). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:39,977 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 260.0 (TID 1521) in 62 ms on localhost (1/2)\n",
      "2015-07-02 05:21:39,978 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 260.0 (TID 1520) in 63 ms on localhost (2/2)\n",
      "2015-07-02 05:21:39,978 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 260.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:39,979 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 260 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.066 s\n",
      "2015-07-02 05:21:39,980 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:39,980 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:39,980 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 261)\n",
      "2015-07-02 05:21:39,981 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:39,982 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 261: List()\n",
      "2015-07-02 05:21:39,983 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 261 (PythonRDD[735] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:39,983 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4196284, maxMem=278302556\n",
      "2015-07-02 05:21:39,984 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_197 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,985 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4201484, maxMem=278302556\n",
      "2015-07-02 05:21:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_197_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:39,986 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_197_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:39,987 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_197_piece0\n",
      "2015-07-02 05:21:39,987 INFO  [sparkDriver-akka.actor.default-dispatcher-5] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 197 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:39,988 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 261 (PythonRDD[735] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:39,988 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 261.0 with 2 tasks\n",
      "2015-07-02 05:21:39,989 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 261.0 (TID 1522, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,989 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 261.0 (TID 1523, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:39,990 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 261.0 (TID 1523)\n",
      "2015-07-02 05:21:39,990 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 261.0 (TID 1522)\n",
      "2015-07-02 05:21:39,992 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,992 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:39,993 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:39,993 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,033 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:40,033 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 261.0 (TID 1523). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,033 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 47, finish = 0\n",
      "2015-07-02 05:21:40,033 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 261.0 (TID 1522). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,034 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 261.0 (TID 1523) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,035 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 261.0 (TID 1522) in 47 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,035 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 261.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,035 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 261 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:40,036 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 72 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.128459 s\n",
      "2015-07-02 05:21:40,085 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:40,085 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 738 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,086 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 73 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:40,086 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 263(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,086 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 262)\n",
      "2015-07-02 05:21:40,087 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 262)\n",
      "2015-07-02 05:21:40,088 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 262 (PairwiseRDD[738] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:40,089 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4204765, maxMem=278302556\n",
      "2015-07-02 05:21:40,090 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_198 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,091 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4214205, maxMem=278302556\n",
      "2015-07-02 05:21:40,091 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_198_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,092 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_198_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,092 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_198_piece0\n",
      "2015-07-02 05:21:40,092 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 198 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,093 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 262 (PairwiseRDD[738] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,093 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 262.0 with 2 tasks\n",
      "2015-07-02 05:21:40,094 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 262.0 (TID 1524, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,094 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 262.0 (TID 1525, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,094 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 262.0 (TID 1524)\n",
      "2015-07-02 05:21:40,094 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 262.0 (TID 1525)\n",
      "2015-07-02 05:21:40,097 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:40,097 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:40,146 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -52, init = 94, finish = 6\n",
      "2015-07-02 05:21:40,148 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -54, init = 96, finish = 8\n",
      "2015-07-02 05:21:40,153 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 262.0 (TID 1524). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,155 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 262.0 (TID 1524) in 62 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,155 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 262.0 (TID 1525). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,162 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 262.0 (TID 1525) in 68 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,162 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 262.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,163 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 262 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.070 s\n",
      "2015-07-02 05:21:40,163 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:40,163 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:40,163 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 263)\n",
      "2015-07-02 05:21:40,163 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:40,164 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 263: List()\n",
      "2015-07-02 05:21:40,165 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 263 (PythonRDD[741] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:40,165 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4220375, maxMem=278302556\n",
      "2015-07-02 05:21:40,165 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_199 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,166 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4225575, maxMem=278302556\n",
      "2015-07-02 05:21:40,166 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_199_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,167 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_199_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,167 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_199_piece0\n",
      "2015-07-02 05:21:40,167 INFO  [sparkDriver-akka.actor.default-dispatcher-19] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 199 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,167 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 263 (PythonRDD[741] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,167 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 263.0 with 2 tasks\n",
      "2015-07-02 05:21:40,168 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 263.0 (TID 1526, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,168 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 263.0 (TID 1527, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,168 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 263.0 (TID 1526)\n",
      "2015-07-02 05:21:40,168 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 263.0 (TID 1527)\n",
      "2015-07-02 05:21:40,170 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:40,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,171 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,211 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -10, init = 52, finish = 0\n",
      "2015-07-02 05:21:40,211 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -9, init = 51, finish = 0\n",
      "2015-07-02 05:21:40,212 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 263.0 (TID 1526). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,212 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 263.0 (TID 1527). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,212 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 263.0 (TID 1526) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,213 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 263.0 (TID 1527) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,213 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 263.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,219 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 263 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:40,220 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 73 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.135117 s\n",
      "2015-07-02 05:21:40,254 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:40,255 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 744 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,255 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 74 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:40,255 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 265(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,255 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 264)\n",
      "2015-07-02 05:21:40,256 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 264)\n",
      "2015-07-02 05:21:40,257 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 264 (PairwiseRDD[744] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:40,258 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4228856, maxMem=278302556\n",
      "2015-07-02 05:21:40,258 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_200 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,259 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4238296, maxMem=278302556\n",
      "2015-07-02 05:21:40,259 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_200_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,260 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_200_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,260 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_200_piece0\n",
      "2015-07-02 05:21:40,260 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 200 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,261 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 264 (PairwiseRDD[744] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,261 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 264.0 with 2 tasks\n",
      "2015-07-02 05:21:40,261 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 264.0 (TID 1528, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,261 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 264.0 (TID 1529, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,262 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 264.0 (TID 1528)\n",
      "2015-07-02 05:21:40,262 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 264.0 (TID 1529)\n",
      "2015-07-02 05:21:40,263 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:40,264 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:40,307 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -42, init = 83, finish = 3\n",
      "2015-07-02 05:21:40,307 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -45, init = 86, finish = 2\n",
      "2015-07-02 05:21:40,309 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 264.0 (TID 1529). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,309 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 264.0 (TID 1528). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,310 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 264.0 (TID 1529) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,311 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 264.0 (TID 1528) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,311 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 264.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 264 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 265)\n",
      "2015-07-02 05:21:40,312 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:40,314 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 265: List()\n",
      "2015-07-02 05:21:40,314 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 265 (PythonRDD[747] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:40,315 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4244466, maxMem=278302556\n",
      "2015-07-02 05:21:40,315 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_201 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,316 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4249666, maxMem=278302556\n",
      "2015-07-02 05:21:40,316 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_201_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:40,317 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_201_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,317 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_201_piece0\n",
      "2015-07-02 05:21:40,317 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 201 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,318 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 265 (PythonRDD[747] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,318 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 265.0 with 2 tasks\n",
      "2015-07-02 05:21:40,318 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 265.0 (TID 1530, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,318 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 265.0 (TID 1531, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,319 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 265.0 (TID 1531)\n",
      "2015-07-02 05:21:40,319 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 265.0 (TID 1530)\n",
      "2015-07-02 05:21:40,320 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,321 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:40,321 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,321 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,366 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -5, init = 51, finish = 0\n",
      "2015-07-02 05:21:40,366 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -5, init = 51, finish = 0\n",
      "2015-07-02 05:21:40,367 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 265.0 (TID 1530). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,367 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 265.0 (TID 1531). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,368 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 265.0 (TID 1530) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,368 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 265.0 (TID 1531) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,368 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 265.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,369 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 265 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:40,370 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 74 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.115361 s\n",
      "2015-07-02 05:21:40,405 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:40,406 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 750 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,406 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 75 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:40,406 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 267(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,406 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 266)\n",
      "2015-07-02 05:21:40,407 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 266)\n",
      "2015-07-02 05:21:40,408 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 266 (PairwiseRDD[750] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:40,409 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4252947, maxMem=278302556\n",
      "2015-07-02 05:21:40,409 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_202 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,410 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6168) called with curMem=4262387, maxMem=278302556\n",
      "2015-07-02 05:21:40,410 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_202_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,410 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_202_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,411 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_202_piece0\n",
      "2015-07-02 05:21:40,411 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 202 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,411 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 266 (PairwiseRDD[750] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,411 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 266.0 with 2 tasks\n",
      "2015-07-02 05:21:40,412 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 266.0 (TID 1532, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,412 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 266.0 (TID 1533, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,412 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 266.0 (TID 1532)\n",
      "2015-07-02 05:21:40,412 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 266.0 (TID 1533)\n",
      "2015-07-02 05:21:40,414 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:40,415 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:40,456 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -39, init = 80, finish = 2\n",
      "2015-07-02 05:21:40,458 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 266.0 (TID 1532). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,459 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -39, init = 82, finish = 2\n",
      "2015-07-02 05:21:40,459 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 266.0 (TID 1532) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,461 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 266.0 (TID 1533). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,462 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 266.0 (TID 1533) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,462 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 266.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,463 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 266 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.052 s\n",
      "2015-07-02 05:21:40,463 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:40,463 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:40,463 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 267)\n",
      "2015-07-02 05:21:40,463 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:40,464 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 267: List()\n",
      "2015-07-02 05:21:40,464 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 267 (PythonRDD[753] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:40,465 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4268555, maxMem=278302556\n",
      "2015-07-02 05:21:40,465 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_203 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,466 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4273755, maxMem=278302556\n",
      "2015-07-02 05:21:40,466 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_203_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,466 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_203_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_203_piece0\n",
      "2015-07-02 05:21:40,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 203 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 267 (PythonRDD[753] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 267.0 with 2 tasks\n",
      "2015-07-02 05:21:40,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 267.0 (TID 1534, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 267.0 (TID 1535, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,468 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 267.0 (TID 1535)\n",
      "2015-07-02 05:21:40,468 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 267.0 (TID 1534)\n",
      "2015-07-02 05:21:40,470 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,470 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,471 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,471 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,511 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -3, init = 45, finish = 0\n",
      "2015-07-02 05:21:40,511 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -5, init = 47, finish = 0\n",
      "2015-07-02 05:21:40,511 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 267.0 (TID 1534). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,511 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 267.0 (TID 1535). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,513 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 267.0 (TID 1534) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,513 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 267.0 (TID 1535) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,513 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 267.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,514 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 267 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:40,515 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 75 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.109320 s\n",
      "2015-07-02 05:21:40,550 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:40,550 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 756 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,550 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 76 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:40,550 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 269(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,550 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 268)\n",
      "2015-07-02 05:21:40,551 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 268)\n",
      "2015-07-02 05:21:40,552 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 268 (PairwiseRDD[756] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:40,553 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4277035, maxMem=278302556\n",
      "2015-07-02 05:21:40,553 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_204 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,554 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6167) called with curMem=4286475, maxMem=278302556\n",
      "2015-07-02 05:21:40,554 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_204_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,554 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_204_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,555 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_204_piece0\n",
      "2015-07-02 05:21:40,555 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 204 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,555 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 268 (PairwiseRDD[756] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,555 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 268.0 with 2 tasks\n",
      "2015-07-02 05:21:40,556 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 268.0 (TID 1536, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,556 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 268.0 (TID 1537, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,557 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 268.0 (TID 1536)\n",
      "2015-07-02 05:21:40,557 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 268.0 (TID 1537)\n",
      "2015-07-02 05:21:40,558 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:40,559 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:40,601 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -39, init = 80, finish = 2\n",
      "2015-07-02 05:21:40,601 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -39, init = 80, finish = 2\n",
      "2015-07-02 05:21:40,610 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 268.0 (TID 1536). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,610 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 268.0 (TID 1537). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,610 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 268.0 (TID 1536) in 54 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,611 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 268.0 (TID 1537) in 55 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,611 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 268.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,611 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 268 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.055 s\n",
      "2015-07-02 05:21:40,611 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:40,612 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:40,612 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 269)\n",
      "2015-07-02 05:21:40,612 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:40,613 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 269: List()\n",
      "2015-07-02 05:21:40,613 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 269 (PythonRDD[759] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:40,613 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4292642, maxMem=278302556\n",
      "2015-07-02 05:21:40,613 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_205 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,614 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4297842, maxMem=278302556\n",
      "2015-07-02 05:21:40,614 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_205_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,615 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_205_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,615 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_205_piece0\n",
      "2015-07-02 05:21:40,615 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 205 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,616 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 269 (PythonRDD[759] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,616 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 269.0 with 2 tasks\n",
      "2015-07-02 05:21:40,616 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 269.0 (TID 1538, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,616 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 269.0 (TID 1539, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,617 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 269.0 (TID 1539)\n",
      "2015-07-02 05:21:40,617 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 269.0 (TID 1538)\n",
      "2015-07-02 05:21:40,618 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,619 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:40,619 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,619 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,659 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -9, init = 50, finish = 0\n",
      "2015-07-02 05:21:40,659 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 269.0 (TID 1538). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,660 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 269.0 (TID 1538) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,661 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -9, init = 52, finish = 1\n",
      "2015-07-02 05:21:40,662 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 269.0 (TID 1539). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,663 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 269.0 (TID 1539) in 47 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,665 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 269.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,666 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 269 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:40,666 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 76 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.116797 s\n",
      "2015-07-02 05:21:40,718 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:40,718 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 762 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,718 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 77 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:40,718 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 271(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,718 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 270)\n",
      "2015-07-02 05:21:40,719 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 270)\n",
      "2015-07-02 05:21:40,720 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 270 (PairwiseRDD[762] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:40,720 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4301122, maxMem=278302556\n",
      "2015-07-02 05:21:40,721 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_206 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,721 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6170) called with curMem=4310562, maxMem=278302556\n",
      "2015-07-02 05:21:40,722 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_206_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,722 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_206_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,722 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_206_piece0\n",
      "2015-07-02 05:21:40,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 206 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 270 (PairwiseRDD[762] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 270.0 with 2 tasks\n",
      "2015-07-02 05:21:40,724 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 270.0 (TID 1540, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,724 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 270.0 (TID 1541, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:40,724 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 270.0 (TID 1541)\n",
      "2015-07-02 05:21:40,724 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 270.0 (TID 1540)\n",
      "2015-07-02 05:21:40,727 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:40,727 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:40,770 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -56, init = 99, finish = 2\n",
      "2015-07-02 05:21:40,771 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -59, init = 102, finish = 2\n",
      "2015-07-02 05:21:40,771 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 270.0 (TID 1540). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,772 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 270.0 (TID 1541). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:40,772 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 270.0 (TID 1540) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,773 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 270.0 (TID 1541) in 49 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,773 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 270.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,774 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 270 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:40,774 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:40,774 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:40,774 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 271)\n",
      "2015-07-02 05:21:40,774 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:40,776 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 271: List()\n",
      "2015-07-02 05:21:40,776 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 271 (PythonRDD[765] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:40,777 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4316732, maxMem=278302556\n",
      "2015-07-02 05:21:40,777 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_207 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,778 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4321932, maxMem=278302556\n",
      "2015-07-02 05:21:40,779 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_207_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:40,779 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_207_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:40,779 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_207_piece0\n",
      "2015-07-02 05:21:40,780 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 207 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:40,780 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 271 (PythonRDD[765] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:40,780 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 271.0 with 2 tasks\n",
      "2015-07-02 05:21:40,781 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 271.0 (TID 1542, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,781 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 271.0 (TID 1543, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:40,781 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 271.0 (TID 1543)\n",
      "2015-07-02 05:21:40,781 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 271.0 (TID 1542)\n",
      "2015-07-02 05:21:40,783 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,783 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,783 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:40,783 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:40,824 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -4, init = 45, finish = 0\n",
      "2015-07-02 05:21:40,824 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 45, finish = 1\n",
      "2015-07-02 05:21:40,825 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 271.0 (TID 1543). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:40,824 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 271.0 (TID 1542). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:40,827 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 271.0 (TID 1543) in 46 ms on localhost (1/2)\n",
      "2015-07-02 05:21:40,828 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 271.0 (TID 1542) in 47 ms on localhost (2/2)\n",
      "2015-07-02 05:21:40,828 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 271.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:40,838 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 271 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.058 s\n",
      "2015-07-02 05:21:40,839 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 77 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.121005 s\n",
      "2015-07-02 05:21:41,005 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,006 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 768 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,006 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 78 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,006 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 273(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,006 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 272)\n",
      "2015-07-02 05:21:41,007 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 272)\n",
      "2015-07-02 05:21:41,009 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 272 (PairwiseRDD[768] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,009 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4325213, maxMem=278302556\n",
      "2015-07-02 05:21:41,009 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_208 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6168) called with curMem=4334653, maxMem=278302556\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 207\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_208_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_207\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_207 of size 5200 dropped from memory (free 273966935)\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_207_piece0\n",
      "2015-07-02 05:21:41,014 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_207_piece0 of size 3281 dropped from memory (free 273970216)\n",
      "2015-07-02 05:21:41,015 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_208_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,015 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_208_piece0\n",
      "2015-07-02 05:21:41,015 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_207_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,015 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_207_piece0\n",
      "2015-07-02 05:21:41,015 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 208 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,015 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 207\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 272 (PairwiseRDD[768] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 272.0 with 2 tasks\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 206\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_206\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_206 of size 9440 dropped from memory (free 273979656)\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_206_piece0\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_206_piece0 of size 6170 dropped from memory (free 273985826)\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 272.0 (TID 1544, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,016 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_206_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,017 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_206_piece0\n",
      "2015-07-02 05:21:41,017 INFO  [sparkDriver-akka.actor.default-dispatcher-19] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 272.0 (TID 1545, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,017 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 206\n",
      "2015-07-02 05:21:41,017 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 272.0 (TID 1544)\n",
      "2015-07-02 05:21:41,017 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 272.0 (TID 1545)\n",
      "2015-07-02 05:21:41,017 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 113\n",
      "2015-07-02 05:21:41,018 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 205\n",
      "2015-07-02 05:21:41,018 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_205\n",
      "2015-07-02 05:21:41,018 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_205 of size 5200 dropped from memory (free 273991026)\n",
      "2015-07-02 05:21:41,018 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_205_piece0\n",
      "2015-07-02 05:21:41,018 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_205_piece0 of size 3280 dropped from memory (free 273994306)\n",
      "2015-07-02 05:21:41,019 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_205_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,019 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,019 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_205_piece0\n",
      "2015-07-02 05:21:41,019 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 205\n",
      "2015-07-02 05:21:41,019 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,020 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 204\n",
      "2015-07-02 05:21:41,020 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_204_piece0\n",
      "2015-07-02 05:21:41,020 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_204_piece0 of size 6167 dropped from memory (free 274000473)\n",
      "2015-07-02 05:21:41,021 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_204_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,021 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_204_piece0\n",
      "2015-07-02 05:21:41,021 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_204\n",
      "2015-07-02 05:21:41,021 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_204 of size 9440 dropped from memory (free 274009913)\n",
      "2015-07-02 05:21:41,021 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 204\n",
      "2015-07-02 05:21:41,022 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 112\n",
      "2015-07-02 05:21:41,022 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 203\n",
      "2015-07-02 05:21:41,022 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_203_piece0\n",
      "2015-07-02 05:21:41,022 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_203_piece0 of size 3280 dropped from memory (free 274013193)\n",
      "2015-07-02 05:21:41,022 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_203_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,023 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_203_piece0\n",
      "2015-07-02 05:21:41,023 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_203\n",
      "2015-07-02 05:21:41,023 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_203 of size 5200 dropped from memory (free 274018393)\n",
      "2015-07-02 05:21:41,023 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 203\n",
      "2015-07-02 05:21:41,024 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 202\n",
      "2015-07-02 05:21:41,024 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_202_piece0\n",
      "2015-07-02 05:21:41,024 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_202_piece0 of size 6168 dropped from memory (free 274024561)\n",
      "2015-07-02 05:21:41,024 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_202_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,025 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_202_piece0\n",
      "2015-07-02 05:21:41,025 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_202\n",
      "2015-07-02 05:21:41,025 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_202 of size 9440 dropped from memory (free 274034001)\n",
      "2015-07-02 05:21:41,025 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 202\n",
      "2015-07-02 05:21:41,025 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 111\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 201\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_201_piece0\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_201_piece0 of size 3281 dropped from memory (free 274037282)\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_201_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_201_piece0\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_201\n",
      "2015-07-02 05:21:41,026 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_201 of size 5200 dropped from memory (free 274042482)\n",
      "2015-07-02 05:21:41,027 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 201\n",
      "2015-07-02 05:21:41,027 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 192\n",
      "2015-07-02 05:21:41,027 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_192_piece0\n",
      "2015-07-02 05:21:41,027 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_192_piece0 of size 6170 dropped from memory (free 274048652)\n",
      "2015-07-02 05:21:41,027 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_192_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,028 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_192_piece0\n",
      "2015-07-02 05:21:41,028 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_192\n",
      "2015-07-02 05:21:41,028 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_192 of size 9440 dropped from memory (free 274058092)\n",
      "2015-07-02 05:21:41,028 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 192\n",
      "2015-07-02 05:21:41,028 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 106\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 191\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_191\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_191 of size 5200 dropped from memory (free 274063292)\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_191_piece0\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_191_piece0 of size 3284 dropped from memory (free 274066576)\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_191_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,029 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_191_piece0\n",
      "2015-07-02 05:21:41,030 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 191\n",
      "2015-07-02 05:21:41,030 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 190\n",
      "2015-07-02 05:21:41,030 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_190\n",
      "2015-07-02 05:21:41,030 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_190 of size 9440 dropped from memory (free 274076016)\n",
      "2015-07-02 05:21:41,030 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_190_piece0\n",
      "2015-07-02 05:21:41,030 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_190_piece0 of size 6170 dropped from memory (free 274082186)\n",
      "2015-07-02 05:21:41,031 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_190_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,031 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_190_piece0\n",
      "2015-07-02 05:21:41,031 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 190\n",
      "2015-07-02 05:21:41,031 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 105\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 189\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_189\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_189 of size 5200 dropped from memory (free 274087386)\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_189_piece0\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_189_piece0 of size 3281 dropped from memory (free 274090667)\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_189_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,032 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_189_piece0\n",
      "2015-07-02 05:21:41,033 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 189\n",
      "2015-07-02 05:21:41,033 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 188\n",
      "2015-07-02 05:21:41,033 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_188_piece0\n",
      "2015-07-02 05:21:41,033 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_188_piece0 of size 6171 dropped from memory (free 274096838)\n",
      "2015-07-02 05:21:41,033 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_188_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,033 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_188_piece0\n",
      "2015-07-02 05:21:41,034 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_188\n",
      "2015-07-02 05:21:41,034 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_188 of size 9440 dropped from memory (free 274106278)\n",
      "2015-07-02 05:21:41,034 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 188\n",
      "2015-07-02 05:21:41,034 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 108\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 195\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_195\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_195 of size 5200 dropped from memory (free 274111478)\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_195_piece0\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_195_piece0 of size 3281 dropped from memory (free 274114759)\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_195_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,035 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_195_piece0\n",
      "2015-07-02 05:21:41,036 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 195\n",
      "2015-07-02 05:21:41,036 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 194\n",
      "2015-07-02 05:21:41,036 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_194_piece0\n",
      "2015-07-02 05:21:41,036 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_194_piece0 of size 6168 dropped from memory (free 274120927)\n",
      "2015-07-02 05:21:41,037 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_194_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,037 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_194_piece0\n",
      "2015-07-02 05:21:41,037 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_194\n",
      "2015-07-02 05:21:41,037 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_194 of size 9440 dropped from memory (free 274130367)\n",
      "2015-07-02 05:21:41,037 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 194\n",
      "2015-07-02 05:21:41,038 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 107\n",
      "2015-07-02 05:21:41,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 193\n",
      "2015-07-02 05:21:41,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_193\n",
      "2015-07-02 05:21:41,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_193 of size 5200 dropped from memory (free 274135567)\n",
      "2015-07-02 05:21:41,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_193_piece0\n",
      "2015-07-02 05:21:41,038 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_193_piece0 of size 3281 dropped from memory (free 274138848)\n",
      "2015-07-02 05:21:41,039 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_193_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,039 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_193_piece0\n",
      "2015-07-02 05:21:41,039 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 193\n",
      "2015-07-02 05:21:41,039 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 200\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_200\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_200 of size 9440 dropped from memory (free 274148288)\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_200_piece0\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_200_piece0 of size 6170 dropped from memory (free 274154458)\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_200_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,040 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_200_piece0\n",
      "2015-07-02 05:21:41,041 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 200\n",
      "2015-07-02 05:21:41,041 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 110\n",
      "2015-07-02 05:21:41,041 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 199\n",
      "2015-07-02 05:21:41,041 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_199_piece0\n",
      "2015-07-02 05:21:41,041 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_199_piece0 of size 3281 dropped from memory (free 274157739)\n",
      "2015-07-02 05:21:41,041 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_199_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,042 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_199_piece0\n",
      "2015-07-02 05:21:41,042 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_199\n",
      "2015-07-02 05:21:41,042 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_199 of size 5200 dropped from memory (free 274162939)\n",
      "2015-07-02 05:21:41,042 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 199\n",
      "2015-07-02 05:21:41,042 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 198\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_198_piece0\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_198_piece0 of size 6170 dropped from memory (free 274169109)\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_198_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_198_piece0\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_198\n",
      "2015-07-02 05:21:41,043 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_198 of size 9440 dropped from memory (free 274178549)\n",
      "2015-07-02 05:21:41,043 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 198\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 197\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_197_piece0\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_197_piece0 of size 3281 dropped from memory (free 274181830)\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_197_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_197_piece0\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_197\n",
      "2015-07-02 05:21:41,044 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_197 of size 5200 dropped from memory (free 274187030)\n",
      "2015-07-02 05:21:41,045 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 197\n",
      "2015-07-02 05:21:41,045 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 196\n",
      "2015-07-02 05:21:41,045 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_196_piece0\n",
      "2015-07-02 05:21:41,045 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_196_piece0 of size 6170 dropped from memory (free 274193200)\n",
      "2015-07-02 05:21:41,046 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_196_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,046 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_196_piece0\n",
      "2015-07-02 05:21:41,046 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_196\n",
      "2015-07-02 05:21:41,046 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_196 of size 9440 dropped from memory (free 274202640)\n",
      "2015-07-02 05:21:41,046 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 196\n",
      "2015-07-02 05:21:41,062 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -182, init = 223, finish = 2\n",
      "2015-07-02 05:21:41,062 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -183, init = 223, finish = 2\n",
      "2015-07-02 05:21:41,063 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 272.0 (TID 1544). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,063 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 272.0 (TID 1545). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,064 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 272.0 (TID 1544) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,064 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 272.0 (TID 1545) in 47 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,064 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 272.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,065 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 272 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.049 s\n",
      "2015-07-02 05:21:41,065 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,065 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,065 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 273)\n",
      "2015-07-02 05:21:41,065 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,066 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 273: List()\n",
      "2015-07-02 05:21:41,066 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 273 (PythonRDD[771] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,067 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4099916, maxMem=278302556\n",
      "2015-07-02 05:21:41,067 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_209 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,068 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3282) called with curMem=4105116, maxMem=278302556\n",
      "2015-07-02 05:21:41,068 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_209_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,069 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_209_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,069 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_209_piece0\n",
      "2015-07-02 05:21:41,070 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 209 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,070 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 273 (PythonRDD[771] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,070 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 273.0 with 2 tasks\n",
      "2015-07-02 05:21:41,071 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 273.0 (TID 1546, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,071 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 273.0 (TID 1547, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,071 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 273.0 (TID 1546)\n",
      "2015-07-02 05:21:41,071 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 273.0 (TID 1547)\n",
      "2015-07-02 05:21:41,074 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,074 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,075 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,075 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,115 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -2, init = 45, finish = 0\n",
      "2015-07-02 05:21:41,115 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -2, init = 45, finish = 0\n",
      "2015-07-02 05:21:41,116 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 273.0 (TID 1546). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,116 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 273.0 (TID 1547). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,117 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 273.0 (TID 1546) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,117 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 273.0 (TID 1547) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,117 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 273.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,118 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 273 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:41,118 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 78 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.112536 s\n",
      "2015-07-02 05:21:41,154 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,155 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 774 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,155 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 79 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,155 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 275(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,155 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 274)\n",
      "2015-07-02 05:21:41,156 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 274)\n",
      "2015-07-02 05:21:41,158 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 274 (PairwiseRDD[774] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,159 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4108398, maxMem=278302556\n",
      "2015-07-02 05:21:41,159 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_210 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,160 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6168) called with curMem=4117838, maxMem=278302556\n",
      "2015-07-02 05:21:41,160 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_210_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,161 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_210_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,161 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_210_piece0\n",
      "2015-07-02 05:21:41,161 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 210 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,161 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 274 (PairwiseRDD[774] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,162 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 274.0 with 2 tasks\n",
      "2015-07-02 05:21:41,162 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 274.0 (TID 1548, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,162 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 274.0 (TID 1549, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,163 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 274.0 (TID 1548)\n",
      "2015-07-02 05:21:41,163 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 274.0 (TID 1549)\n",
      "2015-07-02 05:21:41,165 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,165 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,209 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -42, init = 84, finish = 2\n",
      "2015-07-02 05:21:41,209 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -41, init = 83, finish = 2\n",
      "2015-07-02 05:21:41,210 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 274.0 (TID 1549). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,211 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 274.0 (TID 1548). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,211 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 274.0 (TID 1549) in 49 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,212 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 274.0 (TID 1548) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,212 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 274.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,213 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 274 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.051 s\n",
      "2015-07-02 05:21:41,213 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,213 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,213 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 275)\n",
      "2015-07-02 05:21:41,213 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,214 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 275: List()\n",
      "2015-07-02 05:21:41,214 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 275 (PythonRDD[777] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,215 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4124006, maxMem=278302556\n",
      "2015-07-02 05:21:41,215 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_211 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,216 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4129206, maxMem=278302556\n",
      "2015-07-02 05:21:41,217 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_211_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,218 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_211_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,218 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_211_piece0\n",
      "2015-07-02 05:21:41,219 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 211 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,219 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 275 (PythonRDD[777] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,219 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 275.0 with 2 tasks\n",
      "2015-07-02 05:21:41,219 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 275.0 (TID 1550, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,220 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 275.0 (TID 1551, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,220 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 275.0 (TID 1551)\n",
      "2015-07-02 05:21:41,220 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 275.0 (TID 1550)\n",
      "2015-07-02 05:21:41,222 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,222 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,222 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,222 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,262 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -5, init = 46, finish = 0\n",
      "2015-07-02 05:21:41,263 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -3, init = 44, finish = 1\n",
      "2015-07-02 05:21:41,263 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 275.0 (TID 1551). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,263 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 275.0 (TID 1550). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,264 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 275.0 (TID 1551) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,264 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 275.0 (TID 1550) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,265 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 275.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,265 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 275 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:41,266 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 79 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.110986 s\n",
      "2015-07-02 05:21:41,307 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,308 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 780 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,308 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 80 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,309 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 277(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,309 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 276)\n",
      "2015-07-02 05:21:41,310 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 276)\n",
      "2015-07-02 05:21:41,311 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 276 (PairwiseRDD[780] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,312 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4132486, maxMem=278302556\n",
      "2015-07-02 05:21:41,312 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_212 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,312 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6169) called with curMem=4141926, maxMem=278302556\n",
      "2015-07-02 05:21:41,313 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_212_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:41,313 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_212_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,313 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_212_piece0\n",
      "2015-07-02 05:21:41,314 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 212 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,314 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 276 (PairwiseRDD[780] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,314 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 276.0 with 2 tasks\n",
      "2015-07-02 05:21:41,314 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 276.0 (TID 1552, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,315 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 276.0 (TID 1553, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,315 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 276.0 (TID 1553)\n",
      "2015-07-02 05:21:41,315 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 276.0 (TID 1552)\n",
      "2015-07-02 05:21:41,317 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,317 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,359 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -42, init = 83, finish = 2\n",
      "2015-07-02 05:21:41,359 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -43, init = 84, finish = 2\n",
      "2015-07-02 05:21:41,362 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 276.0 (TID 1553). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,362 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 276.0 (TID 1552). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,363 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 276.0 (TID 1553) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,364 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 276.0 (TID 1552) in 50 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,364 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 276.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,364 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 276 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:41,365 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,365 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,365 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 277)\n",
      "2015-07-02 05:21:41,365 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,366 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 277: List()\n",
      "2015-07-02 05:21:41,366 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 277 (PythonRDD[783] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,367 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4148095, maxMem=278302556\n",
      "2015-07-02 05:21:41,367 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_213 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,368 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3276) called with curMem=4153295, maxMem=278302556\n",
      "2015-07-02 05:21:41,368 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_213_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,369 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_213_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,369 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_213_piece0\n",
      "2015-07-02 05:21:41,370 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 213 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,370 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 277 (PythonRDD[783] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,370 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 277.0 with 2 tasks\n",
      "2015-07-02 05:21:41,371 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 277.0 (TID 1554, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,371 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 277.0 (TID 1555, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,371 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 277.0 (TID 1555)\n",
      "2015-07-02 05:21:41,371 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 277.0 (TID 1554)\n",
      "2015-07-02 05:21:41,373 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,373 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,373 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,373 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,415 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -2, init = 44, finish = 1\n",
      "2015-07-02 05:21:41,415 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -5, init = 47, finish = 1\n",
      "2015-07-02 05:21:41,415 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 277.0 (TID 1554). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,415 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 277.0 (TID 1555). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,416 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 277.0 (TID 1554) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,416 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 277.0 (TID 1555) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,416 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 277.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,417 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 277 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:41,417 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 80 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.109891 s\n",
      "2015-07-02 05:21:41,461 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,461 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 786 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,462 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 81 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,462 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 279(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,462 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 278)\n",
      "2015-07-02 05:21:41,464 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 278)\n",
      "2015-07-02 05:21:41,465 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 278 (PairwiseRDD[786] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,466 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4156571, maxMem=278302556\n",
      "2015-07-02 05:21:41,466 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_214 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6167) called with curMem=4166011, maxMem=278302556\n",
      "2015-07-02 05:21:41,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_214_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,467 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_214_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,467 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_214_piece0\n",
      "2015-07-02 05:21:41,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 214 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 278 (PairwiseRDD[786] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,468 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 278.0 with 2 tasks\n",
      "2015-07-02 05:21:41,469 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 278.0 (TID 1556, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,469 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 278.0 (TID 1557, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,469 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 278.0 (TID 1556)\n",
      "2015-07-02 05:21:41,469 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 278.0 (TID 1557)\n",
      "2015-07-02 05:21:41,471 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,472 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,517 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -47, init = 89, finish = 4\n",
      "2015-07-02 05:21:41,517 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -47, init = 89, finish = 4\n",
      "2015-07-02 05:21:41,519 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 278.0 (TID 1557). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,520 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 278.0 (TID 1556). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,521 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 278.0 (TID 1557) in 52 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,525 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 278.0 (TID 1556) in 55 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,525 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 278.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,526 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 278 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.058 s\n",
      "2015-07-02 05:21:41,527 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,527 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,527 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 279)\n",
      "2015-07-02 05:21:41,527 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,531 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 279: List()\n",
      "2015-07-02 05:21:41,531 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 279 (PythonRDD[789] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,532 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4172178, maxMem=278302556\n",
      "2015-07-02 05:21:41,533 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_215 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,534 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3279) called with curMem=4177378, maxMem=278302556\n",
      "2015-07-02 05:21:41,534 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_215_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,535 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_215_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:41,536 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_215_piece0\n",
      "2015-07-02 05:21:41,536 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 215 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,537 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 279 (PythonRDD[789] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,537 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 279.0 with 2 tasks\n",
      "2015-07-02 05:21:41,538 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 279.0 (TID 1558, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,539 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 279.0 (TID 1559, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,539 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 279.0 (TID 1559)\n",
      "2015-07-02 05:21:41,539 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 279.0 (TID 1558)\n",
      "2015-07-02 05:21:41,542 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,542 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,543 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,543 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,613 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 72, boot = -7, init = 79, finish = 0\n",
      "2015-07-02 05:21:41,613 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 72, boot = -12, init = 84, finish = 0\n",
      "2015-07-02 05:21:41,613 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 279.0 (TID 1558). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,614 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 279.0 (TID 1559). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,615 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 279.0 (TID 1558) in 76 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,615 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 279.0 (TID 1559) in 77 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,615 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 279.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,616 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 279 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.078 s\n",
      "2015-07-02 05:21:41,616 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 81 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.155149 s\n",
      "2015-07-02 05:21:41,652 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 792 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 82 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 281(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,653 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 280)\n",
      "2015-07-02 05:21:41,654 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 280)\n",
      "2015-07-02 05:21:41,655 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 280 (PairwiseRDD[792] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,656 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4180657, maxMem=278302556\n",
      "2015-07-02 05:21:41,656 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_216 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,657 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6169) called with curMem=4190097, maxMem=278302556\n",
      "2015-07-02 05:21:41,657 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_216_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,657 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_216_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,657 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_216_piece0\n",
      "2015-07-02 05:21:41,658 INFO  [sparkDriver-akka.actor.default-dispatcher-17] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 216 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,658 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 280 (PairwiseRDD[792] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,658 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 280.0 with 2 tasks\n",
      "2015-07-02 05:21:41,659 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 280.0 (TID 1560, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,659 INFO  [sparkDriver-akka.actor.default-dispatcher-17] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 280.0 (TID 1561, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,659 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 280.0 (TID 1560)\n",
      "2015-07-02 05:21:41,659 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 280.0 (TID 1561)\n",
      "2015-07-02 05:21:41,661 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,662 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,727 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -40, init = 105, finish = 2\n",
      "2015-07-02 05:21:41,727 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 67, boot = -40, init = 105, finish = 2\n",
      "2015-07-02 05:21:41,729 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 280.0 (TID 1561). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,729 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 280.0 (TID 1560). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,730 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 280.0 (TID 1561) in 71 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,730 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 280.0 (TID 1560) in 72 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,731 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 280.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,731 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 280 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.073 s\n",
      "2015-07-02 05:21:41,731 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,731 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,731 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 281)\n",
      "2015-07-02 05:21:41,731 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,732 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 281: List()\n",
      "2015-07-02 05:21:41,732 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 281 (PythonRDD[795] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,733 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4196266, maxMem=278302556\n",
      "2015-07-02 05:21:41,733 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_217 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,734 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3280) called with curMem=4201466, maxMem=278302556\n",
      "2015-07-02 05:21:41,734 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_217_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,734 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_217_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,735 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_217_piece0\n",
      "2015-07-02 05:21:41,735 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 217 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,736 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 281 (PythonRDD[795] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,736 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 281.0 with 2 tasks\n",
      "2015-07-02 05:21:41,737 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 281.0 (TID 1562, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,738 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 281.0 (TID 1563, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,738 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 281.0 (TID 1563)\n",
      "2015-07-02 05:21:41,738 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 281.0 (TID 1562)\n",
      "2015-07-02 05:21:41,740 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,740 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,740 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,740 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,780 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -2, init = 43, finish = 0\n",
      "2015-07-02 05:21:41,780 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -1, init = 42, finish = 0\n",
      "2015-07-02 05:21:41,781 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 281.0 (TID 1563). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,781 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 281.0 (TID 1562). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,781 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 281.0 (TID 1563) in 43 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,782 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 281.0 (TID 1562) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,782 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 281.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,782 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 281 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.045 s\n",
      "2015-07-02 05:21:41,783 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 82 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.130723 s\n",
      "2015-07-02 05:21:41,821 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:41,822 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 798 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,822 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 83 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:41,822 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 283(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,822 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 282)\n",
      "2015-07-02 05:21:41,823 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 282)\n",
      "2015-07-02 05:21:41,824 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 282 (PairwiseRDD[798] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:41,826 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4204746, maxMem=278302556\n",
      "2015-07-02 05:21:41,826 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_218 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,827 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4214186, maxMem=278302556\n",
      "2015-07-02 05:21:41,828 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_218_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,828 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_218_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,829 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_218_piece0\n",
      "2015-07-02 05:21:41,829 INFO  [sparkDriver-akka.actor.default-dispatcher-13] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 218 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,830 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 282 (PairwiseRDD[798] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,830 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 282.0 with 2 tasks\n",
      "2015-07-02 05:21:41,830 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 282.0 (TID 1564, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,830 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 282.0 (TID 1565, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:41,831 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 282.0 (TID 1564)\n",
      "2015-07-02 05:21:41,831 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 282.0 (TID 1565)\n",
      "2015-07-02 05:21:41,833 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:41,833 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:41,880 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -43, init = 85, finish = 5\n",
      "2015-07-02 05:21:41,882 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -44, init = 85, finish = 8\n",
      "2015-07-02 05:21:41,883 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 282.0 (TID 1565). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,884 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 282.0 (TID 1564). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:41,884 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 282.0 (TID 1565) in 54 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,887 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 282.0 (TID 1564) in 57 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,887 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 282.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,888 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 282 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.058 s\n",
      "2015-07-02 05:21:41,889 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:41,889 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:41,889 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 283)\n",
      "2015-07-02 05:21:41,889 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:41,891 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 283: List()\n",
      "2015-07-02 05:21:41,892 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 283 (PythonRDD[801] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:41,893 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4220357, maxMem=278302556\n",
      "2015-07-02 05:21:41,893 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_219 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,895 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4225557, maxMem=278302556\n",
      "2015-07-02 05:21:41,897 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_219_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:41,898 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_219_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:41,898 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_219_piece0\n",
      "2015-07-02 05:21:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 219 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:41,899 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 283 (PythonRDD[801] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:41,900 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 283.0 with 2 tasks\n",
      "2015-07-02 05:21:41,900 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 283.0 (TID 1566, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,901 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 283.0 (TID 1567, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:41,901 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 283.0 (TID 1567)\n",
      "2015-07-02 05:21:41,901 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 283.0 (TID 1566)\n",
      "2015-07-02 05:21:41,904 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,904 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,904 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:41,904 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:41,946 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -7, init = 49, finish = 1\n",
      "2015-07-02 05:21:41,946 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -10, init = 52, finish = 0\n",
      "2015-07-02 05:21:41,947 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 283.0 (TID 1567). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:41,947 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 283.0 (TID 1566). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:41,948 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 283.0 (TID 1567) in 47 ms on localhost (1/2)\n",
      "2015-07-02 05:21:41,951 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 283.0 (TID 1566) in 51 ms on localhost (2/2)\n",
      "2015-07-02 05:21:41,952 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 283.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:41,952 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 283 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.052 s\n",
      "2015-07-02 05:21:41,953 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 83 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.131673 s\n",
      "2015-07-02 05:21:42,011 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,011 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 804 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 84 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 285(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,012 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 284)\n",
      "2015-07-02 05:21:42,013 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 284)\n",
      "2015-07-02 05:21:42,015 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 284 (PairwiseRDD[804] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,015 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4228838, maxMem=278302556\n",
      "2015-07-02 05:21:42,015 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_220 stored as values in memory (estimated size 9.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:42,016 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6169) called with curMem=4238278, maxMem=278302556\n",
      "2015-07-02 05:21:42,017 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_220_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:42,017 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_220_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,017 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_220_piece0\n",
      "2015-07-02 05:21:42,018 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 220 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,018 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 284 (PairwiseRDD[804] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,018 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 284.0 with 2 tasks\n",
      "2015-07-02 05:21:42,019 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 284.0 (TID 1568, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,019 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 284.0 (TID 1569, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,019 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 284.0 (TID 1569)\n",
      "2015-07-02 05:21:42,019 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 284.0 (TID 1568)\n",
      "2015-07-02 05:21:42,021 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,021 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,069 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -60, init = 103, finish = 6\n",
      "2015-07-02 05:21:42,069 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -58, init = 100, finish = 7\n",
      "2015-07-02 05:21:42,072 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 284.0 (TID 1568). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,072 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 284.0 (TID 1569). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,078 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 284.0 (TID 1568) in 58 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,080 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 284.0 (TID 1569) in 61 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,080 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 284.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,083 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 284 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.065 s\n",
      "2015-07-02 05:21:42,083 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,084 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,084 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 285)\n",
      "2015-07-02 05:21:42,084 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,085 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 285: List()\n",
      "2015-07-02 05:21:42,085 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 285 (PythonRDD[807] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,086 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4244447, maxMem=278302556\n",
      "2015-07-02 05:21:42,086 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_221 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:42,087 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3278) called with curMem=4249647, maxMem=278302556\n",
      "2015-07-02 05:21:42,087 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_221_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:42,088 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_221_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_221_piece0\n",
      "2015-07-02 05:21:42,088 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 221 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,089 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 285 (PythonRDD[807] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,089 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 285.0 with 2 tasks\n",
      "2015-07-02 05:21:42,089 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 285.0 (TID 1570, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,090 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 285.0 (TID 1571, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,090 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 285.0 (TID 1570)\n",
      "2015-07-02 05:21:42,090 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 285.0 (TID 1571)\n",
      "2015-07-02 05:21:42,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,092 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,132 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:42,133 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 285.0 (TID 1570). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,133 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 285.0 (TID 1570) in 44 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,133 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:42,134 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 285.0 (TID 1571). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,135 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 285.0 (TID 1571) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,135 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 285.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,136 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 285 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:42,136 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 84 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.125190 s\n",
      "2015-07-02 05:21:42,181 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,181 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 810 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,181 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 85 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,182 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 287(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,182 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 286)\n",
      "2015-07-02 05:21:42,182 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 286)\n",
      "2015-07-02 05:21:42,184 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 286 (PairwiseRDD[810] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,184 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4252925, maxMem=278302556\n",
      "2015-07-02 05:21:42,184 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_222 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,185 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6169) called with curMem=4262365, maxMem=278302556\n",
      "2015-07-02 05:21:42,185 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_222_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,185 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_222_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_222_piece0\n",
      "2015-07-02 05:21:42,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 222 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 286 (PairwiseRDD[810] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,186 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 286.0 with 2 tasks\n",
      "2015-07-02 05:21:42,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 286.0 (TID 1572, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,187 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 286.0 (TID 1573, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,187 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 286.0 (TID 1572)\n",
      "2015-07-02 05:21:42,187 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 286.0 (TID 1573)\n",
      "2015-07-02 05:21:42,189 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,190 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,232 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -48, init = 89, finish = 3\n",
      "2015-07-02 05:21:42,233 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 286.0 (TID 1572). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,234 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -47, init = 89, finish = 3\n",
      "2015-07-02 05:21:42,237 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 286.0 (TID 1573). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,239 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 286.0 (TID 1573) in 51 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,239 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 286.0 (TID 1572) in 53 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,239 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 286.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,240 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 286 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.054 s\n",
      "2015-07-02 05:21:42,241 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,241 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,241 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 287)\n",
      "2015-07-02 05:21:42,241 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,242 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 287: List()\n",
      "2015-07-02 05:21:42,242 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 287 (PythonRDD[813] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,243 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4268534, maxMem=278302556\n",
      "2015-07-02 05:21:42,243 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_223 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,243 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4273734, maxMem=278302556\n",
      "2015-07-02 05:21:42,244 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_223_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,244 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_223_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,244 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_223_piece0\n",
      "2015-07-02 05:21:42,245 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 223 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,245 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 287 (PythonRDD[813] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,245 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 287.0 with 2 tasks\n",
      "2015-07-02 05:21:42,245 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 287.0 (TID 1574, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,246 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 287.0 (TID 1575, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,246 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 287.0 (TID 1575)\n",
      "2015-07-02 05:21:42,246 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 287.0 (TID 1574)\n",
      "2015-07-02 05:21:42,248 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,248 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:42,248 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,248 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,289 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -6, init = 48, finish = 0\n",
      "2015-07-02 05:21:42,289 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -7, init = 49, finish = 0\n",
      "2015-07-02 05:21:42,290 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 287.0 (TID 1574). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,290 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 287.0 (TID 1575). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,291 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 287.0 (TID 1574) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,291 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 287.0 (TID 1575) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,291 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 287.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,292 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 287 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:42,292 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 85 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.110980 s\n",
      "2015-07-02 05:21:42,328 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,329 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 816 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,329 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 86 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,329 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 289(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,329 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 288)\n",
      "2015-07-02 05:21:42,332 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 288)\n",
      "2015-07-02 05:21:42,333 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 288 (PairwiseRDD[816] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,333 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4277015, maxMem=278302556\n",
      "2015-07-02 05:21:42,333 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_224 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,334 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6165) called with curMem=4286455, maxMem=278302556\n",
      "2015-07-02 05:21:42,334 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_224_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,335 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_224_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,335 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_224_piece0\n",
      "2015-07-02 05:21:42,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 224 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 288 (PairwiseRDD[816] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 288.0 with 2 tasks\n",
      "2015-07-02 05:21:42,337 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 288.0 (TID 1576, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,337 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 288.0 (TID 1577, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,337 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 288.0 (TID 1577)\n",
      "2015-07-02 05:21:42,337 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 288.0 (TID 1576)\n",
      "2015-07-02 05:21:42,339 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,339 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,386 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 47, boot = -42, init = 83, finish = 6\n",
      "2015-07-02 05:21:42,387 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 48, boot = -41, init = 82, finish = 7\n",
      "2015-07-02 05:21:42,389 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 288.0 (TID 1576). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,390 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 288.0 (TID 1577). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,393 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 288.0 (TID 1576) in 55 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,394 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 288.0 (TID 1577) in 56 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,394 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 288.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,396 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 288 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.060 s\n",
      "2015-07-02 05:21:42,397 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,397 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,397 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 289)\n",
      "2015-07-02 05:21:42,397 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,401 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 289: List()\n",
      "2015-07-02 05:21:42,401 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 289 (PythonRDD[819] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,402 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4292620, maxMem=278302556\n",
      "2015-07-02 05:21:42,403 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_225 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,404 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3278) called with curMem=4297820, maxMem=278302556\n",
      "2015-07-02 05:21:42,404 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_225_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,405 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_225_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,406 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_225_piece0\n",
      "2015-07-02 05:21:42,406 INFO  [sparkDriver-akka.actor.default-dispatcher-3] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 225 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,407 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 289 (PythonRDD[819] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,407 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 289.0 with 2 tasks\n",
      "2015-07-02 05:21:42,408 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 289.0 (TID 1578, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,408 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 289.0 (TID 1579, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,409 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 289.0 (TID 1579)\n",
      "2015-07-02 05:21:42,409 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 289.0 (TID 1578)\n",
      "2015-07-02 05:21:42,412 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,412 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,413 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,413 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,453 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -12, init = 53, finish = 0\n",
      "2015-07-02 05:21:42,454 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 289.0 (TID 1578). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,455 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -10, init = 52, finish = 1\n",
      "2015-07-02 05:21:42,456 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 289.0 (TID 1578) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,456 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 289.0 (TID 1579). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,458 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 289.0 (TID 1579) in 49 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,458 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 289.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,459 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 289 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.052 s\n",
      "2015-07-02 05:21:42,459 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 86 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.131041 s\n",
      "2015-07-02 05:21:42,509 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,510 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 822 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,510 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 87 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,510 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 291(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,510 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 290)\n",
      "2015-07-02 05:21:42,511 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 290)\n",
      "2015-07-02 05:21:42,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 290 (PairwiseRDD[822] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4301098, maxMem=278302556\n",
      "2015-07-02 05:21:42,513 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_226 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,514 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4310538, maxMem=278302556\n",
      "2015-07-02 05:21:42,515 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_226_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,515 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_226_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,515 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_226_piece0\n",
      "2015-07-02 05:21:42,516 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 226 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,516 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 290 (PairwiseRDD[822] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,516 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 290.0 with 2 tasks\n",
      "2015-07-02 05:21:42,517 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 290.0 (TID 1580, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,517 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 290.0 (TID 1581, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,517 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 290.0 (TID 1580)\n",
      "2015-07-02 05:21:42,517 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 290.0 (TID 1581)\n",
      "2015-07-02 05:21:42,519 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,520 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,563 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -51, init = 92, finish = 3\n",
      "2015-07-02 05:21:42,563 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 45, boot = -50, init = 92, finish = 3\n",
      "2015-07-02 05:21:42,593 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 290.0 (TID 1581). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,594 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 290.0 (TID 1580). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,597 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 290.0 (TID 1581) in 78 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,599 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 290.0 (TID 1580) in 82 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,599 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 290.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,600 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 290 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.084 s\n",
      "2015-07-02 05:21:42,600 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,600 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,600 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 291)\n",
      "2015-07-02 05:21:42,600 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,601 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 291: List()\n",
      "2015-07-02 05:21:42,602 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 291 (PythonRDD[825] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,602 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4316709, maxMem=278302556\n",
      "2015-07-02 05:21:42,602 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_227 stored as values in memory (estimated size 5.1 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,603 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4321909, maxMem=278302556\n",
      "2015-07-02 05:21:42,603 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_227_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,604 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_227_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,604 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_227_piece0\n",
      "2015-07-02 05:21:42,604 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 227 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,605 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 291 (PythonRDD[825] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,605 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 291.0 with 2 tasks\n",
      "2015-07-02 05:21:42,605 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 291.0 (TID 1582, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,606 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 291.0 (TID 1583, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,606 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 291.0 (TID 1583)\n",
      "2015-07-02 05:21:42,606 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 291.0 (TID 1582)\n",
      "2015-07-02 05:21:42,608 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,608 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,608 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,608 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,648 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:42,649 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = -6, init = 47, finish = 0\n",
      "2015-07-02 05:21:42,649 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 291.0 (TID 1582). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,649 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 291.0 (TID 1583). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,650 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 291.0 (TID 1582) in 45 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,650 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 291.0 (TID 1583) in 45 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,650 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 291.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,651 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 291 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.046 s\n",
      "2015-07-02 05:21:42,651 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 87 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.142227 s\n",
      "2015-07-02 05:21:42,687 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,687 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 828 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,687 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 88 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,688 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 293(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,688 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 292)\n",
      "2015-07-02 05:21:42,688 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 292)\n",
      "2015-07-02 05:21:42,691 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 292 (PairwiseRDD[828] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,692 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4325190, maxMem=278302556\n",
      "2015-07-02 05:21:42,692 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_228 stored as values in memory (estimated size 9.2 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,700 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 227\n",
      "2015-07-02 05:21:42,700 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4334630, maxMem=278302556\n",
      "2015-07-02 05:21:42,700 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_227_piece0\n",
      "2015-07-02 05:21:42,700 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_227_piece0 of size 3281 dropped from memory (free 273965036)\n",
      "2015-07-02 05:21:42,700 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_228_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.3 MB)\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_227_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_227_piece0\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_227\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_227 of size 5200 dropped from memory (free 273970236)\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_228_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,701 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 227\n",
      "2015-07-02 05:21:42,701 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_228_piece0\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 226\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_226\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_226 of size 9440 dropped from memory (free 273979676)\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_226_piece0\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_226_piece0 of size 6171 dropped from memory (free 273985847)\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-4] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 228 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,702 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_226_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,703 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_226_piece0\n",
      "2015-07-02 05:21:42,703 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 292 (PairwiseRDD[828] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,703 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 292.0 with 2 tasks\n",
      "2015-07-02 05:21:42,703 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 226\n",
      "2015-07-02 05:21:42,703 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 292.0 (TID 1584, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,703 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 123\n",
      "2015-07-02 05:21:42,703 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 292.0 (TID 1585, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,704 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 225\n",
      "2015-07-02 05:21:42,704 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_225_piece0\n",
      "2015-07-02 05:21:42,704 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 292.0 (TID 1584)\n",
      "2015-07-02 05:21:42,704 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 292.0 (TID 1585)\n",
      "2015-07-02 05:21:42,704 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_225_piece0 of size 3278 dropped from memory (free 273989125)\n",
      "2015-07-02 05:21:42,705 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_225_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_225_piece0\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_225\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_225 of size 5200 dropped from memory (free 273994325)\n",
      "2015-07-02 05:21:42,706 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 225\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 224\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_224_piece0\n",
      "2015-07-02 05:21:42,706 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_224_piece0 of size 6165 dropped from memory (free 274000490)\n",
      "2015-07-02 05:21:42,707 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,707 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_224_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,708 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_224_piece0\n",
      "2015-07-02 05:21:42,708 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_224\n",
      "2015-07-02 05:21:42,708 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_224 of size 9440 dropped from memory (free 274009930)\n",
      "2015-07-02 05:21:42,708 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 224\n",
      "2015-07-02 05:21:42,709 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 122\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 223\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_223\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_223 of size 5200 dropped from memory (free 274015130)\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_223_piece0\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_223_piece0 of size 3281 dropped from memory (free 274018411)\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-14] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_223_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,710 INFO  [sparkDriver-akka.actor.default-dispatcher-4] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_223_piece0\n",
      "2015-07-02 05:21:42,711 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 223\n",
      "2015-07-02 05:21:42,711 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 222\n",
      "2015-07-02 05:21:42,711 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_222\n",
      "2015-07-02 05:21:42,711 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_222 of size 9440 dropped from memory (free 274027851)\n",
      "2015-07-02 05:21:42,711 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_222_piece0\n",
      "2015-07-02 05:21:42,711 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_222_piece0 of size 6169 dropped from memory (free 274034020)\n",
      "2015-07-02 05:21:42,712 INFO  [sparkDriver-akka.actor.default-dispatcher-17] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_222_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,712 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_222_piece0\n",
      "2015-07-02 05:21:42,712 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 222\n",
      "2015-07-02 05:21:42,712 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,713 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 121\n",
      "2015-07-02 05:21:42,714 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 221\n",
      "2015-07-02 05:21:42,714 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_221_piece0\n",
      "2015-07-02 05:21:42,714 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_221_piece0 of size 3278 dropped from memory (free 274037298)\n",
      "2015-07-02 05:21:42,714 INFO  [sparkDriver-akka.actor.default-dispatcher-21] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_221_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,715 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_221_piece0\n",
      "2015-07-02 05:21:42,715 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_221\n",
      "2015-07-02 05:21:42,715 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_221 of size 5200 dropped from memory (free 274042498)\n",
      "2015-07-02 05:21:42,715 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 221\n",
      "2015-07-02 05:21:42,715 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 208\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_208_piece0\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_208_piece0 of size 6168 dropped from memory (free 274048666)\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_208_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_208_piece0\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_208\n",
      "2015-07-02 05:21:42,716 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_208 of size 9440 dropped from memory (free 274058106)\n",
      "2015-07-02 05:21:42,716 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 208\n",
      "2015-07-02 05:21:42,717 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 210\n",
      "2015-07-02 05:21:42,717 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_210_piece0\n",
      "2015-07-02 05:21:42,717 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_210_piece0 of size 6168 dropped from memory (free 274064274)\n",
      "2015-07-02 05:21:42,717 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_210_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,717 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_210_piece0\n",
      "2015-07-02 05:21:42,718 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_210\n",
      "2015-07-02 05:21:42,718 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_210 of size 9440 dropped from memory (free 274073714)\n",
      "2015-07-02 05:21:42,718 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 210\n",
      "2015-07-02 05:21:42,718 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 115\n",
      "2015-07-02 05:21:42,718 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 209\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_209\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_209 of size 5200 dropped from memory (free 274078914)\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_209_piece0\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_209_piece0 of size 3282 dropped from memory (free 274082196)\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_209_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,719 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_209_piece0\n",
      "2015-07-02 05:21:42,720 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 209\n",
      "2015-07-02 05:21:42,720 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 217\n",
      "2015-07-02 05:21:42,720 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_217\n",
      "2015-07-02 05:21:42,720 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_217 of size 5200 dropped from memory (free 274087396)\n",
      "2015-07-02 05:21:42,720 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_217_piece0\n",
      "2015-07-02 05:21:42,721 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_217_piece0 of size 3280 dropped from memory (free 274090676)\n",
      "2015-07-02 05:21:42,721 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_217_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,721 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_217_piece0\n",
      "2015-07-02 05:21:42,721 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 217\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 216\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_216\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_216 of size 9440 dropped from memory (free 274100116)\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_216_piece0\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_216_piece0 of size 6169 dropped from memory (free 274106285)\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_216_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,722 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_216_piece0\n",
      "2015-07-02 05:21:42,723 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 216\n",
      "2015-07-02 05:21:42,723 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 118\n",
      "2015-07-02 05:21:42,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 215\n",
      "2015-07-02 05:21:42,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_215_piece0\n",
      "2015-07-02 05:21:42,723 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_215_piece0 of size 3279 dropped from memory (free 274109564)\n",
      "2015-07-02 05:21:42,724 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_215_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.8 MB)\n",
      "2015-07-02 05:21:42,724 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_215_piece0\n",
      "2015-07-02 05:21:42,724 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_215\n",
      "2015-07-02 05:21:42,724 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_215 of size 5200 dropped from memory (free 274114764)\n",
      "2015-07-02 05:21:42,724 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 215\n",
      "2015-07-02 05:21:42,725 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 214\n",
      "2015-07-02 05:21:42,725 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_214_piece0\n",
      "2015-07-02 05:21:42,725 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_214_piece0 of size 6167 dropped from memory (free 274120931)\n",
      "2015-07-02 05:21:42,725 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_214_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,726 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_214_piece0\n",
      "2015-07-02 05:21:42,726 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_214\n",
      "2015-07-02 05:21:42,726 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_214 of size 9440 dropped from memory (free 274130371)\n",
      "2015-07-02 05:21:42,726 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 214\n",
      "2015-07-02 05:21:42,726 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 117\n",
      "2015-07-02 05:21:42,726 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 213\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_213\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_213 of size 5200 dropped from memory (free 274135571)\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_213_piece0\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_213_piece0 of size 3276 dropped from memory (free 274138847)\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_213_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,727 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_213_piece0\n",
      "2015-07-02 05:21:42,727 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 213\n",
      "2015-07-02 05:21:42,728 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 212\n",
      "2015-07-02 05:21:42,728 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_212\n",
      "2015-07-02 05:21:42,728 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_212 of size 9440 dropped from memory (free 274148287)\n",
      "2015-07-02 05:21:42,728 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_212_piece0\n",
      "2015-07-02 05:21:42,728 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_212_piece0 of size 6169 dropped from memory (free 274154456)\n",
      "2015-07-02 05:21:42,729 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_212_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,729 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_212_piece0\n",
      "2015-07-02 05:21:42,729 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 212\n",
      "2015-07-02 05:21:42,729 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 116\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 211\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_211_piece0\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_211_piece0 of size 3280 dropped from memory (free 274157736)\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_211_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_211_piece0\n",
      "2015-07-02 05:21:42,730 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_211\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_211 of size 5200 dropped from memory (free 274162936)\n",
      "2015-07-02 05:21:42,731 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 211\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 220\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_220\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_220 of size 9440 dropped from memory (free 274172376)\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_220_piece0\n",
      "2015-07-02 05:21:42,731 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_220_piece0 of size 6169 dropped from memory (free 274178545)\n",
      "2015-07-02 05:21:42,732 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_220_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,732 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_220_piece0\n",
      "2015-07-02 05:21:42,732 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 220\n",
      "2015-07-02 05:21:42,733 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 120\n",
      "2015-07-02 05:21:42,733 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 219\n",
      "2015-07-02 05:21:42,733 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_219_piece0\n",
      "2015-07-02 05:21:42,733 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_219_piece0 of size 3281 dropped from memory (free 274181826)\n",
      "2015-07-02 05:21:42,733 INFO  [sparkDriver-akka.actor.default-dispatcher-21] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_219_piece0 on localhost:40918 in memory (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,734 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_219_piece0\n",
      "2015-07-02 05:21:42,734 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_219\n",
      "2015-07-02 05:21:42,734 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_219 of size 5200 dropped from memory (free 274187026)\n",
      "2015-07-02 05:21:42,734 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 219\n",
      "2015-07-02 05:21:42,734 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 218\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_218\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_218 of size 9440 dropped from memory (free 274196466)\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_218_piece0\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_218_piece0 of size 6171 dropped from memory (free 274202637)\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Removed broadcast_218_piece0 on localhost:40918 in memory (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,735 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_218_piece0\n",
      "2015-07-02 05:21:42,736 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 218\n",
      "2015-07-02 05:21:42,736 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned shuffle 119\n",
      "2015-07-02 05:21:42,751 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 46, boot = -49, init = 92, finish = 3\n",
      "2015-07-02 05:21:42,752 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 292.0 (TID 1585). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,753 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 292.0 (TID 1585) in 50 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,755 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 50, boot = -47, init = 95, finish = 2\n",
      "2015-07-02 05:21:42,757 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 292.0 (TID 1584). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,758 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 292.0 (TID 1584) in 55 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,758 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 292.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 292 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.056 s\n",
      "2015-07-02 05:21:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 293)\n",
      "2015-07-02 05:21:42,759 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,760 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 293: List()\n",
      "2015-07-02 05:21:42,760 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 293 (PythonRDD[831] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,761 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4099919, maxMem=278302556\n",
      "2015-07-02 05:21:42,761 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_229 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,762 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3276) called with curMem=4105119, maxMem=278302556\n",
      "2015-07-02 05:21:42,762 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_229_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,763 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_229_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,763 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_229_piece0\n",
      "2015-07-02 05:21:42,763 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 229 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,764 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 293 (PythonRDD[831] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,764 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 293.0 with 2 tasks\n",
      "2015-07-02 05:21:42,764 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 293.0 (TID 1586, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,764 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 293.0 (TID 1587, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,765 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 293.0 (TID 1587)\n",
      "2015-07-02 05:21:42,765 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 293.0 (TID 1586)\n",
      "2015-07-02 05:21:42,766 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,767 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 1 ms\n",
      "2015-07-02 05:21:42,767 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,767 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,808 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -4, init = 46, finish = 0\n",
      "2015-07-02 05:21:42,808 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -8, init = 50, finish = 0\n",
      "2015-07-02 05:21:42,809 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 293.0 (TID 1586). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,809 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 293.0 (TID 1587). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,810 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 293.0 (TID 1586) in 46 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,810 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 293.0 (TID 1587) in 46 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,811 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 293.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,811 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 293 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.047 s\n",
      "2015-07-02 05:21:42,812 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 88 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.124506 s\n",
      "2015-07-02 05:21:42,844 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:42,845 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 834 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,845 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 89 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:42,845 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 295(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,845 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 294)\n",
      "2015-07-02 05:21:42,847 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 294)\n",
      "2015-07-02 05:21:42,848 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 294 (PairwiseRDD[834] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:42,849 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4108395, maxMem=278302556\n",
      "2015-07-02 05:21:42,849 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_230 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,850 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4117835, maxMem=278302556\n",
      "2015-07-02 05:21:42,850 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_230_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,851 INFO  [sparkDriver-akka.actor.default-dispatcher-19] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_230_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,851 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_230_piece0\n",
      "2015-07-02 05:21:42,851 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 230 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,852 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 294 (PairwiseRDD[834] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,852 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 294.0 with 2 tasks\n",
      "2015-07-02 05:21:42,853 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 294.0 (TID 1588, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,853 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 294.0 (TID 1589, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:42,854 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 294.0 (TID 1588)\n",
      "2015-07-02 05:21:42,854 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 294.0 (TID 1589)\n",
      "2015-07-02 05:21:42,856 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:42,857 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:42,900 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -39, init = 80, finish = 3\n",
      "2015-07-02 05:21:42,906 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 294.0 (TID 1588). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,907 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 49, boot = -39, init = 83, finish = 5\n",
      "2015-07-02 05:21:42,914 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 294.0 (TID 1588) in 62 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,914 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 294.0 (TID 1589). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:42,915 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 294.0 (TID 1589) in 62 ms on localhost (2/2)\n",
      "2015-07-02 05:21:42,916 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 294.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:42,916 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 294 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.064 s\n",
      "2015-07-02 05:21:42,916 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:42,917 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:42,917 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 295)\n",
      "2015-07-02 05:21:42,917 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:42,918 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 295: List()\n",
      "2015-07-02 05:21:42,918 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 295 (PythonRDD[837] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:42,918 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4124006, maxMem=278302556\n",
      "2015-07-02 05:21:42,919 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_231 stored as values in memory (estimated size 5.1 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,919 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4129206, maxMem=278302556\n",
      "2015-07-02 05:21:42,920 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_231_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:42,920 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_231_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:42,920 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_231_piece0\n",
      "2015-07-02 05:21:42,921 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 231 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:42,922 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 295 (PythonRDD[837] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:42,922 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 295.0 with 2 tasks\n",
      "2015-07-02 05:21:42,922 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 295.0 (TID 1590, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,922 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 295.0 (TID 1591, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:42,923 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 295.0 (TID 1591)\n",
      "2015-07-02 05:21:42,923 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 295.0 (TID 1590)\n",
      "2015-07-02 05:21:42,926 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,926 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,928 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:42,928 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:42,967 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -13, init = 55, finish = 1\n",
      "2015-07-02 05:21:42,967 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 295.0 (TID 1591). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:42,975 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 43, boot = -10, init = 53, finish = 0\n",
      "2015-07-02 05:21:42,975 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 295.0 (TID 1590). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:42,980 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 295.0 (TID 1590) in 56 ms on localhost (1/2)\n",
      "2015-07-02 05:21:42,982 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 295.0 (TID 1591) in 60 ms on localhost (2/2)\n",
      "2015-07-02 05:21:43,015 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 295.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:43,019 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 295 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.096 s\n",
      "2015-07-02 05:21:43,019 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 89 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.174632 s\n",
      "2015-07-02 05:21:43,050 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-5-d5e94eb36cd1>:31\n",
      "2015-07-02 05:21:43,050 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 840 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:43,050 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 90 (collect at <ipython-input-5-d5e94eb36cd1>:31) with 2 output partitions (allowLocal=false)\n",
      "2015-07-02 05:21:43,050 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 297(collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:43,050 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 296)\n",
      "2015-07-02 05:21:43,051 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 296)\n",
      "2015-07-02 05:21:43,052 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 296 (PairwiseRDD[840] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31), which has no missing parents\n",
      "2015-07-02 05:21:43,053 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(9440) called with curMem=4132487, maxMem=278302556\n",
      "2015-07-02 05:21:43,053 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_232 stored as values in memory (estimated size 9.2 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:43,054 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(6171) called with curMem=4141927, maxMem=278302556\n",
      "2015-07-02 05:21:43,054 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_232_piece0 stored as bytes in memory (estimated size 6.0 KB, free 261.5 MB)\n",
      "2015-07-02 05:21:43,055 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_232_piece0 in memory on localhost:40918 (size: 6.0 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:43,055 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_232_piece0\n",
      "2015-07-02 05:21:43,056 INFO  [sparkDriver-akka.actor.default-dispatcher-15] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 232 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:43,056 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 296 (PairwiseRDD[840] at reduceByKey at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:43,056 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 296.0 with 2 tasks\n",
      "2015-07-02 05:21:43,057 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 296.0 (TID 1592, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:43,057 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 296.0 (TID 1593, localhost, PROCESS_LOCAL, 1299 bytes)\n",
      "2015-07-02 05:21:43,058 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 296.0 (TID 1593)\n",
      "2015-07-02 05:21:43,058 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 296.0 (TID 1592)\n",
      "2015-07-02 05:21:43,059 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:8145+8146\n",
      "2015-07-02 05:21:43,060 INFO  [stdout writer for python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/assignment/features/training_initial.csv:0+8145\n",
      "2015-07-02 05:21:43,104 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -82, init = 124, finish = 2\n",
      "2015-07-02 05:21:43,104 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -84, init = 126, finish = 2\n",
      "2015-07-02 05:21:43,108 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 296.0 (TID 1593). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:43,109 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 296.0 (TID 1592). 1958 bytes result sent to driver\n",
      "2015-07-02 05:21:43,111 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 296.0 (TID 1592) in 54 ms on localhost (1/2)\n",
      "2015-07-02 05:21:43,112 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 296.0 (TID 1593) in 55 ms on localhost (2/2)\n",
      "2015-07-02 05:21:43,112 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 296.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:43,114 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 296 (reduceByKey at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.057 s\n",
      "2015-07-02 05:21:43,114 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
      "2015-07-02 05:21:43,114 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
      "2015-07-02 05:21:43,115 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 297)\n",
      "2015-07-02 05:21:43,115 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
      "2015-07-02 05:21:43,120 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 297: List()\n",
      "2015-07-02 05:21:43,120 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 297 (PythonRDD[843] at collect at <ipython-input-5-d5e94eb36cd1>:31), which is now runnable\n",
      "2015-07-02 05:21:43,121 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5200) called with curMem=4148098, maxMem=278302556\n",
      "2015-07-02 05:21:43,122 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_233 stored as values in memory (estimated size 5.1 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:43,123 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3281) called with curMem=4153298, maxMem=278302556\n",
      "2015-07-02 05:21:43,123 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_233_piece0 stored as bytes in memory (estimated size 3.2 KB, free 261.4 MB)\n",
      "2015-07-02 05:21:43,124 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.BlockManagerInfo (Logging.scala:logInfo(59)) - Added broadcast_233_piece0 in memory on localhost:40918 (size: 3.2 KB, free: 264.9 MB)\n",
      "2015-07-02 05:21:43,124 INFO  [sparkDriver-akka.actor.default-dispatcher-20] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Updated info of block broadcast_233_piece0\n",
      "2015-07-02 05:21:43,125 INFO  [sparkDriver-akka.actor.default-dispatcher-20] spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 233 from broadcast at DAGScheduler.scala:838\n",
      "2015-07-02 05:21:43,125 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 297 (PythonRDD[843] at collect at <ipython-input-5-d5e94eb36cd1>:31)\n",
      "2015-07-02 05:21:43,126 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 297.0 with 2 tasks\n",
      "2015-07-02 05:21:43,126 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 297.0 (TID 1594, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:43,127 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 297.0 (TID 1595, localhost, PROCESS_LOCAL, 1056 bytes)\n",
      "2015-07-02 05:21:43,128 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 297.0 (TID 1595)\n",
      "2015-07-02 05:21:43,128 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 297.0 (TID 1594)\n",
      "2015-07-02 05:21:43,131 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:43,131 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:43,132 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
      "2015-07-02 05:21:43,132 INFO  [stdout writer for python] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
      "2015-07-02 05:21:43,173 INFO  [Executor task launch worker-24] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 42, boot = -11, init = 53, finish = 0\n",
      "2015-07-02 05:21:43,173 INFO  [Executor task launch worker-23] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 44, boot = -11, init = 55, finish = 0\n",
      "2015-07-02 05:21:43,173 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 297.0 (TID 1594). 870 bytes result sent to driver\n",
      "2015-07-02 05:21:43,173 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 297.0 (TID 1595). 1201 bytes result sent to driver\n",
      "2015-07-02 05:21:43,174 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 297.0 (TID 1594) in 48 ms on localhost (1/2)\n",
      "2015-07-02 05:21:43,175 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 297.0 (TID 1595) in 48 ms on localhost (2/2)\n",
      "2015-07-02 05:21:43,175 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 297.0, whose tasks have all completed, from pool \n",
      "2015-07-02 05:21:43,176 INFO  [sparkDriver-akka.actor.default-dispatcher-20] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 297 (collect at <ipython-input-5-d5e94eb36cd1>:31) finished in 0.050 s\n",
      "2015-07-02 05:21:43,177 INFO  [Thread-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 90 finished: collect at <ipython-input-5-d5e94eb36cd1>:31, took 0.127033 s\n"
     ]
    }
   ],
   "source": [
    "''' ############## Parts 3,4 and 5 ############## '''\n",
    "\n",
    "############ Weighted KNN ############  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def knn_reducer(v1,v2,k):\n",
    "    '''\n",
    "    Input:  \n",
    "        v1- list of tuples of Euclidean distance and label\n",
    "        v2- list of tuples of Euclidean distance and label\n",
    "        k- the required number of nearest neighbors\n",
    "    Output: list of sorted tuples of Euclidean distance and label (maximal length of k)\n",
    "    '''    \n",
    "    joined = v1+v2\n",
    "    joined = sorted(joined)\n",
    "    if len(joined) <= k:\n",
    "        return joined\n",
    "    else:\n",
    "        return joined[:k]\n",
    "    \n",
    "\n",
    "def knn(train_rdd,pred_line,k):\n",
    "    '''\n",
    "    Input:  \n",
    "        train_rdd- the RDD of the training set\n",
    "        pred_line- the record we wish to predict its label\n",
    "        k- the required number of nearest neighbors\n",
    "    Output: the predicted label of pred_line\n",
    "    '''    \n",
    "    nn = train_rdd.map(lambda line: (1,[(np.linalg.norm(np.asarray(line[1:-1])-np.asarray(pred_line[1:])),line[-1])]) ) \\\n",
    "                    .reduceByKey(lambda v1,v2: knn_reducer(v1,v2,k)).collect()\n",
    "    \n",
    "    decision = 0\n",
    "    sum_w = 0\n",
    "    weights = nn[0][1]\n",
    "\n",
    "    for w in weights:\n",
    "        sum_w += np.exp(-w[0])\n",
    "    \n",
    "    for w,l in weights:\n",
    "        decision += (np.exp(-w)/sum_w)*l\n",
    "\n",
    "    if decision > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "############ Train and test the model ############   \n",
    "\n",
    "test = sc.textFile(test_fn).map(lambda line: (line.split(\",\")))\\\n",
    "                            .filter(lambda line: line[0] != 'id')\\\n",
    "                            .map(lambda line: [line[0]]+[float(i) for i in line[1:]]).collect()\n",
    "train = sc.textFile(train_fn).map(lambda line: (line.split(\",\")))\\\n",
    "                                .filter(lambda line: line[0] != 'id')\\\n",
    "                                .map(lambda line: [line[0]]+[float(i) for i in line[1:]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hit = 0.0\n",
    "for idx,t in enumerate(test):\n",
    "    test[idx] = [t[0]]+[t[-1]]+[knn(train,t[:-1],7)]\n",
    "    if test[idx][-2] == test[idx][-1]:\n",
    "        hit += 1\n",
    "accuracy =  hit/len(test)\n",
    "\n",
    "tested_init_fn = '/assignment/output/tested_initial.csv'\n",
    "export_list_to_file(test,tested_init_fn,['label','prediction'])\n",
    "\n",
    "accuracy_init_fn = '/assignment/output/accuracy_initial.txt'\n",
    "export_list_to_file([['Model Accuracy'],['The weighted KNN model\\'s accuracy is:'],[str(accuracy)]],accuracy_init_fn)\n",
    "print 'END'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
